[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Criação de pipelines de análise reprodutíveis em R",
    "section": "",
    "text": "Bem-vindo!",
    "crumbs": [
      "Bem-vindo!"
    ]
  },
  {
    "objectID": "index.html#como-ajudar-cientistas-de-dados-analistas-e-investigadores-a-escrever-código-fiável-com-a-ajuda-de-algumas-ideias-da-engenharia-de-software.",
    "href": "index.html#como-ajudar-cientistas-de-dados-analistas-e-investigadores-a-escrever-código-fiável-com-a-ajuda-de-algumas-ideias-da-engenharia-de-software.",
    "title": "Criação de pipelines de análise reprodutíveis em R",
    "section": "Como ajudar cientistas de dados, analistas e investigadores a escrever código fiável com a ajuda de algumas ideias da engenharia de software.",
    "text": "Como ajudar cientistas de dados, analistas e investigadores a escrever código fiável com a ajuda de algumas ideias da engenharia de software.\n\nCientistas de dados, estatísticos, analistas, investigadores e profissionais similares escrevem muito código.\nNão só escrevem muito código como têm também de ler e rever código. Ou trabalham em equipas e precisam de rever código uns dos outros ou então precisam de reproduzir resultados de projectos antigos, seja no contexto de revisão por pares, seja em auditorias. No entanto, nunca, ou muito raramente, aprenderam técnicas e ferramentas que facilitam o processo de escrita, colaboração, revisão e reprodução de projectos.\nO que é de lamentar, pois os engenheiros de software enfrentam o mesmo tipo de desafios e já foram capazes de resolver este problema há várias décadas.\nO objectivo deste livro é o de mostrar algumas das melhores práticas da engenharia de software e DevOps para tornar os nosso projectos mais robustos, fiáveis e reprodutíveis. Não importa se trabalhamos sozinhos, em equipas pequenas ou grandes. Não importa se o nosso trabalho vai ser revisto por pares ou auditado: as técnicas que aqui serão apresentadas tornarão os nossos projectos mais fiáveis e poupar-nos-ão a muitas frustações.\nEnquanto analistas de dados, somos tentados a pensar que não somos desenvolvedores de código. Os desenvolvedores são aqueles génios que escrevem código de alta qualidade e criam aqueles pacotes de grande utilidade. Mas na verdade, nós também fazemos desenvolvimento de código. A diferença é que o nosso focus reside em escrever código para as nossas análises funcionarem e não para outras pessoas. Pelo menos, esta é a nossa ideia. Nas outras pessoas devemos incluir os nossos colegas de equipa, revisores, auditores, chefias… Todas as pessoas que possam vir a ler o nosso código devem ser incluídas. No mínimo, nós próprios voltaremos a ler o nosso código no futuro.\nAo aprendermos a desenhar projectos e a escrever código de uma forma que futuros usuários possam entender o que foi feito e não nos queiram assassinar, estamos a melhorar a qualidade do nosso trafbalho duma forma natural.\nA abertura de issues, PRs ou outras questões podem ser feitas no repositório deste livro: Github repository1.",
    "crumbs": [
      "Bem-vindo!"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Criação de pipelines de análise reprodutíveis em R",
    "section": "",
    "text": "https://github.com/balima78/rap_pt↩︎",
    "crumbs": [
      "Bem-vindo!"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Prefácio",
    "section": "",
    "text": "Este livro é uma tradução livre de Building reproducible analytical pipelines with R. O autor do livro original, Bruno Rodrigues, tem uma vasto trabalho desenvolvido na área da programação estatística e para nosso deleite tem o bom hábito de partilhar algum do seu trabalho, quer seja através do seu blog, do seu X, do seu canal de youtube ou do seu Github. Depois de nos disponibilizar o ebook Modern R with the tidyverse, fomos agora presenteados com Building reproducible analytical pipelines with R que mostra uma abordagem pragmática de bem programar em estatística.\nO Bruno Rodrigues apresenta-se como lusoburguês, ou seja, é um luxemburguês descendente de portugueses. Também por isto, espero que esta tradução possa também ser vista como uma singela homenagem a tudo o que o Bruno Rodrigues nos tem oferecido.\nDe um ponto de vista mais pessoal, trabalho em estatísca há quase de 25 anos e com R há cerca de uma dúzia de anos, ainda assim encontrei neste livro Building reproducible analytical pipelines with R o meu guia de trabalho e o meu manual de boas práticas. E que melhor forma de estudar este livro do que fazer uma tradução para português?\nMuitos de nós que trabalhmos com dados não temos bases de engenharia de software. A nossa form ação assenta na teoria de probabilidade, algebra linear, algoritmos, alguma programação mas não em engenharia de software. Embora os estatísticos, cientistas de dados (ou como nos queiram chamar atualmente) não sejam engenheiros de software têm de escrever muito código e código muito relevante. No entanto, muitos de nós escrevemos sem regras.\nPor exemplo, quanto do nosso código que produz resultados altamente sensíveis, seja na ciência ou na industria, é devidamente testado? Quanto do código que usamos depende duma só pessoa aparecer no trabalho e que usa um conhecimento secreto que não está documentado? Quanto do código que corremos ninguém se atreve a mexer porque a pessoa que o escreveu originalmente arranjou melhores coisas que fazer?\nEm colaborações com colegas de equipa para produzir um relatório ou um artigo científico consideramos os riscos potencias?\nSomos capazes de dizer exactamente, como um número de um relatório usado por um superior foi produzido? E se houver uma auditoria externa? Serão os aujditores capazes de executar o nosso código sem a nossa intervenção?\nQuando trabalhamos em ciência o nosso trabalho é auditado, ou pelo menos, em teoria, deveria sê-lo. As ideias de open science, open data e reproducibilidade são geralmente bem aceites na comunidade científica, no entanto, na prática, quantos artigos são de facto reprodutíveis? Quantos resultados científicos são auditáveis e rastreáveis?\nO conceito de Pipelines de Análise Reprodutíveis (originalmente, Reproducible Analytical Pipelines) foi desenvolvido pelo Office for National Statistics (ONS). Em 2019, a equipa do ONS responsável pela divulgação deste conceito publicou um ebook gratuito. Deve-se também salientar o trabalho de Software Carpentery, onde encontramos muitas bases que podem ser aplicadas na programação estística.\nResumidamente, aqui podemos encontrar algumas ideias, que embora não sejam novas podem ser do interesse de quem usa o ambiente de programação em R.\nEste livro divide-se em duas partes. A primeira apresenta o conhecimento básico que devemos adquirir para criar pipelines verdadeiramente reprodutíveis, nomeadamente:\n\nControlo de versões com o Git e gestão de projectos com o Github;\nProgramação funcional;\nProgramação letrada.\n\nO principal conceito da Parte 1 é “não te repitas” (“don’t repeat yourself”). O Git e o Github podem-nos ajudar a não perdermos código, nem a perdermos o controlo de quem deve fazer o quê num projecto (mesmo se trabalharmos sozinhos, a utilização do Git e do Github podem-nos econonmizar muitas horas e muitas dores de cabeça). A programação funcional e a programação letrada podem ajudar-nos a melhorar o nosso código evitando duas fontes de erros muito habituais: resultados de computação que dependem do estado do noso programa (e também do estado de todo o hardware que estamos a usar) e os erros de copiar e colar.\nNa segunda parte deste livro utilizaremos os conhecimentos anteriores na utilização de várias ferramentas qua nos ajudarão a ir para lá do controlo de versões e da programa ção funcional e letrada:\n\nGestão de dependencias com {renv};\nDesenvolvimento de pacotes com {fusen};\nTestes unitários e de integração;\nCriação de automação com {targets};\nAmbientes reprodutíveis com Docker;\nIntegração contínua e entrega.\n\nEmbora este não seja um livro para iniciantes (precisamos estar familiarizados com o ambiente de programação R), não assumiremos que temos um conhecimento prévio das ferramentas apresentadas na Parte 2. Na verdade, mesmo que já estejamos familiarizados com Git, Github, programação funcional e letrada, podemos sempre apren der mais alguma coisa com na Parte 1. Mas fica já o aviso, se queremos tirar proveito deste livro vamos ter de escrever muito código.\nMais uma vez quero reforçar que esta é uma tradução livre do exelente trabalho que foi desenvolvido pelo Bruno Rodrigues e não pretende ser uma alternativa ao original.\n\n\n\n\n\nCapa do livro original\n\n\n\n\nMuito pelo contrário, espero que este livro, pelo menos, sirva de incentivo para desbravar Building reproducible analytical pipelines with R.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução",
    "section": "",
    "text": "1.1 Para quem é este livro?\nEste livro destina-se a qualquer pessoa que use dados brutos para criar qualquer tipo de resultado com base nesses dados brutos. Pode ser um simples relatório trimestral, por exemplo, no qual os dados são usados para tabelas e gráficos, ou um artigo científico para uma revista com revisão por pares ou até mesmo uma aplicação interativa Web. Não importa, pois o processo é, essencialmente, muito semelhante:\nAssumimos alguma familiaridade com a linguagem de programação R, os exemplos apresentados e as ferramentas usadas são específicas para R. No entanto os conceitos aqui apresentados podem ser aplicados a outras linguagens.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#para-quem-é-este-livro",
    "href": "intro.html#para-quem-é-este-livro",
    "title": "1  Introdução",
    "section": "",
    "text": "Obter os dados;\nLimpar os dados;\nEscrever código para analisar os dados;\nApresentar os resultados num produto final.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#qual-o-objectivo-deste-livro",
    "href": "intro.html#qual-o-objectivo-deste-livro",
    "title": "1  Introdução",
    "section": "1.2 Qual o objectivo deste livro",
    "text": "1.2 Qual o objectivo deste livro\nO objetivo deste livro é tornar o processo de análise de dados o mais fiável, rastreável e reproduzível possível, e fazendo isto por desenho. Ou seja, quando concluirmos a nossa análise, está tudo pronto.\nNão queremos gastar tempo, que geralmente não temos, para reescrever ou formatar uma análise para a tornar reprodutível depois de concluída. Já sabemso que não o vamos fazer! Depois de concluirmos uma análise temos de iniciar um novo projecto. Se precisarmos de executar de novo uma análise mais antiga (por exemplo, porque os dados foram atualizados), na altura preocupar-nos-emos com isso, certo?\nEsperemos que no futuro, nos lembremos de cada peculariedade do nosso código e saibamos qual o script a executar em cada ponto do processso, quais os comentários que estão desatualizados e podem ser ignorados com segurança, que recursos dos dados precisam ser verificados (e quando precisam ser verificados) e assim por diante… É melhor esperar que o futuro seja um trabalhador mais diligente do que nós somos agora!\nDaqui em diante, referir-nos-emos a um projecto reprodutível como um pipeline de análise reprodutível, ou RAP (nas suas siglas em inglês “Reproducible Analytival Pipeline”), para abreviar. Há duas formas de criar esse RAP: ou tems a sorte de contar com algujém cujo trabalho é o de transformar o nosso código num RFAP, ou então temos de ser nós mesmos a fazê-lo. E esta segunda opção é geralmente a mais provável. O problema é que geralmente não o fazemos. Estamos sempre com pressa para chegar aos resultados e não pensamos em tornar o processo reproduzível. Isso ocorre porque pensamos que tornar o processo reproduzível leva tempo e que esse tempo é melhor gasto trabalhando na análise em si. Mas essa é uma concepção errada, por duas ordens de razão.\nPrimeiro a utilização das técnicas discutidas neste livro não con somem muito tempo. Podermos ver, na verdade não são coisas que acrescentamos à análise, mas fazem parte da própria análise e também ajnudam na gestgão do projecto. E algumas dessas técnicas nos ajudarão a economizar tempo (especialmente nos testes) e dores de cabeça.\nDepois a análise nunca é feita de uma só vez. Apenas coisas mais simples serão feitas apenas uma única vez, como extrair um va lor de uma base de dados. Ainda assim, é provável que depois de darmos esse número, sejamos solicitados para obtermos a variação desse número (por exemplo desagregando por uma ou várias variáveis). Ou talvez nos peçam para atualizarmos aquele número passado meio ano. Assim, procuramos gauardar a consulta SQL em algum script a que possamos recorrer para darmos uma resposta consistente. Mas e no caso de análises mais complexas? manter o mesmo script pode ser um bom princípio (quando temos esse script) mas não é suficiente.\nNão raras vezes, estamos na situação em que, temos de atualizar um relatório, passado um ano, em que estão envolvidas várias pessoas no processo e só a parte da recolha dos novos dados já é uma complicação. Porque uns têm os dados duma maneira e outros doutra… Bom, a primeira lição (e se calhar a mais importante) deste livro é: ao iniciarmos a construção de um RAP, devemos garantir que falamos com todas as pessoas que vão estar envolvidas no processo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#pré-requisitos",
    "href": "intro.html#pré-requisitos",
    "title": "1  Introdução",
    "section": "1.3 Pré-requisitos",
    "text": "1.3 Pré-requisitos\nDevemos estar familiarizados com o ambiente de programação em R. Ao longo deste livro, vamos assumir que usamos o R em vários projectos e que queremos melhorar não só o uso da linguagem em si mas principalmente como devemos gerir projectos complexos. Sabemos o que são pacotes e como instalá-los, já escrevemos algumas funções em R, sabe mos o que são loops, e sabemos quais são as estruturas de dados que podemos usar (como por exemplo as listas). Também estamos familiarizados com a visualização, processamento e análise de dados, embora estes não sejam temas do livro.\nNão usaremos qualquer tipo de Graphical User Interface (GUI) mas sim um IDE como o RStudio. Isto porque a interação com um GUI não é reprodutível. O nosso objectivo é o de escever código que possa ser executado não-interactivamente por uma máquina. Isto porque uma condição necessária para que um fluxo de trabalho seja reprodutível e designado como RAP é que o fluxo de trabalho possa ser executado por uma máquina, automaticamente, sem nenhuma intervenção humana. Esta é a segunda lição para a criação de RAPs: não deve haver nenhuma intervenção humana necessária para obter os resultados depois que o RAP for iniciado. Se conseguirmos isto, o nosso fluxo de trabalho provavelmente é reprodutível ou, pelo menos, poderá ser reproduzido mais facilmente do que se exigir alguma manipulação por um ser humano em algum momento do ciclo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#reproducibilidade",
    "href": "intro.html#reproducibilidade",
    "title": "1  Introdução",
    "section": "1.4 Reproducibilidade",
    "text": "1.4 Reproducibilidade\nUm projecto reprodutível é aquele que pode ser executado repetidamente por qualquer pessoa e sem esforço (ou com um esforço mínimo). Porém, há diferentes níveis de reproducibilidade, como veremos nesta secção. Vejamos alguns requisitos que um projectodeve ter para ser um RAP.\n\n1.4.1 Usar ferramentas open-source\nO código-fonte aberto é um requisito rígido para a reproducibilidade.\nNão há “se” nem “mas”. Não nos referimos apenas ao código que escrevemos para o nosso artigo de pesquisa/relatório/análise. Todo o ecossistema que usamos para digitar o nosso código e criar o fluxo de trabalho tem de ser open-source.\nSe é código escrito num programa proprietário, como STATA, SAS ou SPSS, então não é um RAP. Não importa se o código está bem documentado, escrito e disponível num sistema de controlo de versões (internamente na nossa empresa ou aberto ao público). Esse projeto simplesmente não é reproduzível. Por quê? Porque, num horizonte de tempo longo o suficiente, não há como reexecutar o nosso código com a mesma versão exata da linguagem de programação proprietária e na mesma versão exata do sistema operacional que foi usado no momento em que o projeto foi desenvolvido.\n\n\n1.4.2 Dependências ocultas que poem em causa a reproducibilidade\nMas há outro problema: vamos supor que temos escrito um bom fluxo de trabalho, testado exaustivamente, bem documentado, disponibilizado no Github (e vamos supor que os dados estão também disponíve is para download gratuito e ainda que o artigo é de acesso aberto). Vamos supor ainda que o código foi escrito em R ou Python. Podemos então dizer que estga análise ou estudo é reprodutível? Bem, se a análise foi executada num sistema operacional proprietário, então não é reprodutível. Isto porque o sistema operacional em que o código é executado também pode influenciar os resultados que o pipeline gera. Há algumas particularidades nos sistemas operacionais que podem fazer com que certas coisas funcionem de forma diferente. É certo que, na prática, isso raramente é um problema, mas acontece1, especialmente se estivermos a trabalhar com aritmética de ponto flutuante de alta precisão, como acontece no setor financeiro, por exemplo.\nFelizmente, não há necessidade de mudarmos de sistema operacional para lidarmos com este problema. Podemos usar o Docker como salvaguarda.\n\n\n1.4.3 Requisitos para um RAP\nResumidamente, para termos algo verdadeiramente reprodutível, tem de respeitar os eguintes pontos:\n\nObviamente, o código-fonte deve estar disponível e ser exaustivamente testado e documentado (e é por isso que usaremos o Git e o Github);\nTodas as dependências devem ser fáceis de encontrar e instalar (vamos lidar com isso usando ferramentas de gerenciamento de dependências);\nSer escrito com uma linguagem de programação de código aberto (ferramentas nocode como o Excel são, por padrão, não reprodutíveis porque não podem ser usadas de forma não interativa, e é por isso que usaremos a linguagem de programação R);\nO projeto precisa ser executado em um sistema operacional de código aberto (felizmente, podemos lidar com isso sem ter de instalar e aprender a usar um novo sistema operacional, graças ao Docker);\nObviamente, os dados e o documento/relatório também precisam estar acessíveis, se não publicamente, como é o caso duma investigação, então dentro da nossa empresa. Isso significa que o conceito de “scripts e/ou dados disponíveis mediante solicitação” deve ir para o lixo.\n\n\n\n\n\n\nUma frase real de um artigo real publicado no THE LANCET Regional Health. E se disponibilizares os dados e eu não risco o teu carro, que tal isso como uma solicitação razoável?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#há-diferentes-tipos-de-reproducibilidade",
    "href": "intro.html#há-diferentes-tipos-de-reproducibilidade",
    "title": "1  Introdução",
    "section": "1.5 Há diferentes tipos de reproducibilidade?",
    "text": "1.5 Há diferentes tipos de reproducibilidade?\nVamos dar um passo atrás: vivemos no mundo real e, no mundo real, há algumas restrições que estão fora do nosso controle. Essas restrições podem impossibilitar a criação de um RAP verdadeiro, portanto, às vezes, precisamos nos contentar com algo que pode não ser um RAP verdadeiro, mas uma segunda ou até terceira melhor opção.\nNo que se segue, vamos supor o seguinte: na discussão em baixo, o código é testado e documentado, portanto, vamos discutir apenas o código que executa o pipeline em si.\nO pior pipeline reprodutível seria algo que funcionasse, mas somente na nossa máquina. Isso pode ser simplesmente devido ao fato de termos codificado caminhos que só existem no nosso laptop. Qualquer pessoa que queira executar novamente o pipeline precisará alterar os caminhos. Isto é algo que precisa ser documentado em um README, o que presumimos ser o caso, então é isso. Mas talvez esse pipeline só seja executado no nosso laptop porque o ambiente computacional que estamos a u sar é difícil de reproduzir. Talvez estamos a usar um software, mesmo que seja um software de código aberto, que não seja fácil de instalar (qualquer pessoa que tenha tentado instalar pacotes R no Linux que dependem do pacote {rJava} percebe este exemplo).\nPortanto, um pipeline menos mau seria aquele que pudesse ser executado mais facilmente em qualquer máquina semelhante à nossa. Isso poderia ser feito não usando caminhos absolutos codificados e fornecendo instruções para configurar o ambiente. Por exemplo, no caso do R, isso poderia ser tão simples quanto fornecer um script chamado install_deps.R que seria uma chamada para install.packages(). Poderia ser assim:\n\ninstall.packages(c(\"package1\",\n                   \"package2\",\n                   etc))\n\nNeste caso, o problema é que precisamos de nos certificar de que as versões corretas dos pacotes sejam instaladas. Se o nosso script usa {ggplot2} versão 2.2.1, os usuários devem instalar essa versão também e, ao executar o script acima, a versão mais recente de {ggplot2} será instalada. Talvez isto não seja um problema, mas pode ser que o nosso script use uma função da versão 2.2.1 que já não esteja disponível na versão mais recente (ou talvez o nome tenha sido alterado, ou talvez tenha sido modificado de alguma forma e não forneça exatamente o mesmo resultado). Quanto mais pacotes o script usar (e quanto mais antigo elee forem), maior será a probabilidade de que alguma versão do pacote não seja compatível. Há também a questão da própria versão do R. De modo geral, as versões recentes do R parecem não ser tão más a executar códigos mais antigos escritos em R.\nMas importar sublinhar que quando corremos um código antigo e não nos dá erro, não significa que os resultados sejam exactamente iguais. Pode haver casos que a mesma função devolva resultados diferentes em diferentes versões do R. Ou seja, não basta contar com a estabilidade da própria linguagem de programaçãoao longo do tempocomo con dição suficiente para a reproducibilidade.\nIsto significa que a reproducibilidade é um continuum e , dependendo das restrições que tenhamos, o nosso projecto pode ser ‘não muito reprodutível’ ou ‘totalmente reprodutível’. Assim, o grau de reproducibilidade do nosso projecto pode ser influenciado por:\n\nVersão da linguagem de programação usada;\nVersões dos pacotes da referida linguagem de programação utilizada;\nSistema operacional e sua versão;\nVersões das bibliotecas do sistema subjacente (que geralmente andam de mãos dadas com a versão do sistema operacional, mas não necessariamente).\nE até mesmo a arquitetura de hardware na qual executamos todo o software.\n\nPortanto, quando dizemos que a “reproducibilidade é um continuum”, significa que podemos implementar o nosso projecto de modo a que nenhum, um, dois, três ou todos os items anteriores são considerados, para que seja reprodutível.\nEsta não é uma ideia nova ou inédita. Peng (2011) já discutiu este conceito a que chamou de “espectro de reproducibilidade”. Na segunda parte deste livro, voltaremos a esta ideia que designaremos como “iceberg de reproducibilidade”.\n\n\n\n\n\nO espectro de reproducibilidade do artigo de 2011 de Peng.\n\n\n\n\nMesmo quando falamos de arquitectura de hardware, esta também não é imutável. A Apple mudou a arquitectura dos seus compuradores recentemente, e no futuro a Microsoft também pode pressionar os fabricantes OEM a criarem mais computadores baseados em ARM.\nMas então como podemos gerir todos estes riscos e equilibrar a necessidade imediata de resultados com a necessidade futura de executar novamente um projeto antigo? E se a retoma desse projeto antigo nunca for necessária no futuro?\nÉ aqui que este livro nos pode ajudar, ao usarmos as técnicas discutidas aqui, tornaremos fácil e rápida a configuração dum projecto desde o início para que seja realmente reprodutível. Ao criarmos o projecto desta forma também garantirá que evitaremos erros e resultados errados. Será mais fácil e rápido iterar e melhorar o nosso código, colaborar com terceiros e confiar nos resultados do nosso pipeline. Mesmo que ninguém volte a executar o nosso código, poderemos beneficiar das boas práticas aqui recomendadas.\n\n\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introdução",
    "section": "",
    "text": "https://github.com/numpy/numpy/issues/9187↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "part1_intro.html",
    "href": "part1_intro.html",
    "title": "Parte 1: Don’t Repeat Yourself",
    "section": "",
    "text": "Introdução\nNesta primeira parte, vamo-nos focar nos ingredientes principais da reproducibilidade. E por ingredientes principais referimo-nos às ferramentas indispensáveis na nossa caixa de ferramentas que devemos de ter antes de tentarmos sequer fazer um projecto reprodutível. Estas ferramentas são tão importantes que lhes didicaremos uma grande parte deste livro:\nMesmo que já estejamos familiarizados com estes tópicos e mesmo que os utilizamos no nosso dia-a-dia, vale a pena espreitarmos para esta Parte 1 do livro antes de abordarmos a Parte 2. Na segunda parte o focus recairá noutro conjunto de ferramentas que permitem criar os RAP.\nOu seja, nesta primeira parte não vamos a aprender a construir RAPs mas n ão podemos saltar para a criação de RAPs antes de comprendermos os conceitos supracitados.\nVamos começar por analisar alguns con juntos de dados. Fazer o download, limpar os dados e fazer alguns gráficos. Isto será feito em dois scripts, escritos duma forma comum sem “engenharia de software”, para assim replicarmos a forma como analistas de dados, cientistas de dados ou investigadores, que não têm qualquer tipo de form ação em ciência de computação, fazem este tipo de análises. Ou seja, estes programadores têm como objectivo principal a entraga rápida de resultados, sem olhar a meios.\nPoderemos então mostrar que adotando algumas ideias simples da engebnharia de software, podemos obter resultados tão rápidos quanto antes mas duma forma mais con sistente e robusta.",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself"
    ]
  },
  {
    "objectID": "part1_intro.html#introdução",
    "href": "part1_intro.html#introdução",
    "title": "Parte 1: Don’t Repeat Yourself",
    "section": "",
    "text": "Controlo de versões;\nProgramação funcional;\nProgramação letrada.",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself"
    ]
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "2  Antes de começarmos",
    "section": "",
    "text": "2.1 Conhecimentos essenciais\nÉ importante conhecermos as partes que constituem o R. Vamos deixar uma coisa clara: o R não é o RStudio (ou outro qualquer interface que possamos usar para aceder ao R). O R é um interpretador de linguagem de programação específico de um domínio. O R é específico de um domínio porque é principalmente usado para fazer estatística. É um interpretador porque obtemos resultados imediatamente assim que executamos um script na consola. Ou seja, quando escrevemos 1+1 na consola, obtemos de imediato 2. Há linguagens de programação designadas como compiladoras que precisam que o código seja compilado em binários antes da execução. O C é uma dessas linguagens. Pelo facto do R ser um interpretador permite que facilmente possamos fazer análise exploratória de dados de forma interactiva mas também apresenta alguns aspetos negativos de que falaremos ao longo do livro. A consola do R é um exemplo de um ambiente REPL (Read-Eval-Print-Loop). O código é lido, avaliado, impresso e o estado lido é devolvido iniciando de novo o loop.\nPara que o nosso trabalho em R seja facilitado, não devemos escrever o código na consola mas sim num ficheiro de texto. Podemos guardar estes ficheiros de texto, atualizá-los e partilhá-los com outros colaboradores. Chamamos os ficheiros de texto como scripts. Embora possamos escrever estes scripts em qualquer editor de texto básico (como o Notepad, por exemplo), devemos usar editores de texto desenvolvidos especificamente para facilitar a programação. As escolhas mais populares são o RStudio ou o Visual Studio Code mas também hã alternativas como o Emacs. Seja qual for o editor de texto que escolhamos, devemos gastar o nosso tempo a configurá-lo e a aprender como usá-lo. Vamos passar muito tempo e mujitas horas nesse editor de texto. O código que escevermos nesse editor será o nosso ‘ganha pão’. Devemos aprender os principais atalhos de teclado e as características avançadas do nosso editor. Este investimento inicial dar-nos-á o retorno devido. Além disso, devemos perceber o que é um ficheiro de texto. Um documento Word (fickheiro com extensão .docx) não é um ficheiro de texto. Pode parecer um ficheiro de texto mas não é. Um ficheiro .docx é um formato muito mais complexo com várias camadas de abstração. Um ficheiro de texto verdadeiro pode ser aberto pelo editor de texto mais simples de um qualquer sistema operativo.\nComo referimos, o R é uma linguagem de programação específica de um domínio principalmente usada para fazer estatística ou ciência de dados (se preferirmos esta designação). As suas capacidades básicas podem ser extendidas com a instalação de pacotes. Por exemplo, a instalação base do R tem funções úteis como mean() ou sd(), para calcularmos a média e desvio padrão dum vector de números, ou a função rnorm() para calcular valores aleatórios duma distribuição Gaussiana (Normal). No entanto não tem nenhuma função que permita treinar um modelo de random forest. Para isto precisamos de instalar um pacote com o comando install.packages(\"randomForest\"). Este copmando instala o pacote {randomForest} (ao longo do livro os pacotes estarão sempre ente chavetas). Um conjunto de pacotes instalados, designamos como “livraria”. Geralmente os pacotes são baixados do CRAN (Comprehensive R Archive Network) embora também possamos aceder a eles a partir de outros repositórios. Possivelmente uma das razões pelo que o R se tornou tão popular é devido à facilidade com que podemos criar pacotes, como veremos mais à frente. Alguns pacotes são escritos noutras linguagens de programação como o Fortran ou C++. O código destes pacotes é compilado e pode ser executado pelo R usando uma função disponibilizada ao utilizador. Por exemplo, se explorarmos o código fonte do pacote {randomForest} vamos encontrar código C e Fortran. É importante termos isto presente porque às vezes os pacotes têm de ser compilados por install.packages() e às vezes esta compilação falha.\nQuando usamos o R, lemos consjuntos de dados, fazemos gráficos, treinamos modelos, etc. Esses dados, gráficos e modelos são objectos gravados no ambeinte global. Para vermos um lista dos objectos que temos no ambiente global, podemos escrever ls() na consola do R. Quando saímos do R aparece a pergunta se queremos gravas o workspace: isto gravará o atual estado do ambiente global que será carregado na próxima vez que iniciarmos o R. É recolmendável que nunca gravemos o workspace ao sairmos do R. No caso de estarmos a usar o RStudio, podemos configurar este comportamento nas opções globais (em workspace e passar Save workspace to .RData para Never). Gravas e ler o workspace impossibilita iniciarmos uma nova sessão de R do zero o que nos pode trazer problemas difíceis de contornar.\nDevemos também estar confortáveis com os caminhos do sistema de ficheiros do nosso computador. Ou seja, sermos capazes de encontrar para onde são feitos os downloads dos ficheiros, ou sermos capazer de navegar para qualquer pasta quer seja através de um GUI ou através de um terminal. è também recomendável que usemos caminhos relativos nos nossos scripts em vez de caminhos absolutos. Isto é, não devemos começar os nossos scripts com uma linha como:\nEm vez disto, devemos usar “Projects” quando estamos no RStudio. Assim podemos usar caminhos relativos o que facilita o trabalho colaborativo. Usando “Projects” no RStudio, para lermos dadops apenas temos de escrever:\ne não temos de definir directorias de trabalho com setwd() que depois não vão existir no computador de um colaborador. O pacote {here} é um facilitador para usar caminhos relativos tal como descrito neste post1.\nDevemos estar familiarizados com a escrita de funçõe. Neste livro temos um capítulo dedicado à programação funcional e vamos a escrever algumas funções.\nFinalmente, devemos saber como pedir ajuda. se precisarmos de ajuda com este livro podemos abrir um issue no repositório Github aqui2. Tal como para este livro, se tivermos alguma dúvida com algum pacote devemos procurar o seu repositório (geralmente no Github, mas nem sempre), ou podemos tentar contactar directamente o autor do pacote, ou então recorrer ao Stackoverflow. Seja qual for a nossa opção devemos fazer o trabalho de casa primeiro:\nComo diz o provérbio popular:",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Antes de começarmos</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#conhecimentos-essenciais",
    "href": "prerequisites.html#conhecimentos-essenciais",
    "title": "2  Antes de começarmos",
    "section": "",
    "text": "setwd(\"H:/Username/Projects/housing_regression/\")\n\ndataset &lt;- read.csv(\"data.csv\")\n\n\n\n\nLer a documentação. Podemos a estar a ferramenta de forma errada.\nTomar nota da mensagem de erro. Embora as mensagens de erros possam parecer cripticas mas com a experiência aprendemos a descodificá-las;\nEscrever o script mais simples possível que reproduz o problema que temos. Este MRE (Minimal Reproducible Example) é a melhor forma para transmitirmos o nosso problema e facilitarmos a vida de quem nos queira ajudar. Regras gerais para escrever um MRE podem ser encontradas neste clássico post3.\n\n\n\nO material tem sempre razão!",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Antes de começarmos</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#footnotes",
    "href": "prerequisites.html#footnotes",
    "title": "2  Antes de começarmos",
    "section": "",
    "text": "https://github.com/jennybc/here_here↩︎\nhttps://github.com/balima78/rap_pt↩︎\nhttps://jonskeet.uk/csharp/complete.html↩︎",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Antes de começarmos</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "4  Controlo de versões com o Git",
    "section": "",
    "text": "4.1 Instalar e abrir uma conta no Github\nPara instalar o Git num macOS ou num Windows basta seguir as instrições de Git Book2. É tão fácil como instalar outro qualquer programa.\nDependendo do sistema operativo que estamos a usar, pode também ser instalado, com o Git, um interface gráfico com o utilizador, possibilitando o seu uso fora da linha de comandos. Tmabémé possível usar o Git através do RStudio ou de muitos outros editores. Aqui não vamos usar nenhum interface gráfico com o utilizador uma vez que não há um interfce que possamos dmizer que seja universal. O único universal é a linha de comandos. Além disso, se aprendermos a usar o Git através da linha de comandos teremos a vida facilitada quando precisarmos de o usar num servidor.\nUsar a linha de comandos não é assim tão dificil como pode parecer.\nSe ainda não tivermos uma conta de Github então é uma boa altura para a criarmos. Basta visitarmos a página https://github.com/, seguir as instruções e selecionar o nível gratuito para abrirmos a nossa conta.\nE é o nosso dashboard Github.\nNa próxima secção vmos a aprender alguns comandos básicos do Git para versionarmos os dois scripts que já escrevemos.",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Controlo de versões com o Git</span>"
    ]
  },
  {
    "objectID": "git.html#git-básico",
    "href": "git.html#git-básico",
    "title": "4  Controlo de versões com o Git",
    "section": "4.2 Git básico",
    "text": "4.2 Git básico\nVamos usar os dois scripts que escrevemos num capítulo anterior. Podemos começar por criar uma pasta chamada housing e colocar lá os dois scripts que escrevemos anteriormente:\n\nsave_data.R: https://is.gd/7PhUjd\nanalysis.R: https://is.gd/qCJEbi\n\nVamos abrir a pasta que tem os dois ficheiros através do explorador de ficheiros. No windows, com o botão direiro do rato, em qualquer espaço branco dentro da pasta com os ficheiros, selecionamos a opção “Open Git Bash here”. A outra opção, e que funciona tanto em Linux, como em macOS e Windows, é abrirmos um terminale navegarmos para a nossa pasta usando cd (change directory).\ncd /home/user/housing/\nVamos garantir que estamos na pasta certa, listando o seu conteúdo:\nls\nA partir de agora temos de garantir que escrevemos os comandos no terminal, em esp+ecífcio no terminal Git BAsh no Windows. Para destinguirmos este terminal do prompt da linha de comandos do R, o prompt do terminal Git Bash começará por owner@localhost. O owner é o nome do utilizador do gestor do projecto nos nossos exemplos a partir daqui, o owner do computador usado por este gestor de projecto é designado localhost (este prompt pode ser diferente de computador parfa computador, às vezes aparece o caminho completo para a directoria do projecto). Isto é o que acontecequando o owner corre ls no directório raíz do projecto:\nowner@localhost ➤ ls\nanalysis.R save_data.R\n(também há a opção do comando ls -la que também lista ficheiros ocultos)\nDevemos garantir que vemos os ficheiros quando executamos ls. Se não os virmos é porque não estamos na directoria correcta.\nPodemos agora começar a rastrear estes ficheiros com o Git. No mesmo terminal onde fizemos ls, vamos executar o seguinte comando git:\nowner@localhost ➤ git init\nhint: Using 'master' as the name for the initial branch.\nhint: This default branch name is subject to change. \nhint: To configure the initial branch name to use in all of your\nhint: new repositories, which will suppress this warning, call:\nhint: \nhint:   git config --global init.defaultBranch &lt;name&gt;\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', \nhint: 'trunk' and 'development'. The just-created branch can be \nhint: renamed via this command:\nhint: \nhint:   git branch -m &lt;name&gt;\nInitialized empty Git repository in /home/user/housing/.git/\nvamos gastar algum tempo a ler estes hint. Muitos comandos git dão-nos pistas e é sempre uma boa ideia lê-los. Este hint diz-nos que o nome do branch por defeito é “master” mas que isto é alterável. Um branch é como uma versão do nosso código. O branch “master” terá a versão por defeito do nosso código. Mas podemos criar um branch designado “dev” que terá uma versão do nosso código com funcionalidades ainda em desenvolvimento. Não há nada de especial o branch por defeito chamado “master” e podeia ter sido designado doutra forma qualquer. Por exemplo, se criamos um repositório primeito no Github em vez de o criarmos no nosso computador, o nome por defeito do branch será “main”. Temos de ter atenção a estas coisas pois quando começarmos a interagir com o nosso repositório no Github, temos de garantir que temos no nome certo do branch presente. Além disso, por o branch “master” ser o mais importante, ás vezes é chamado de “trunk”, principalmente por equipas que fazem desenvolvimento base-trunk.\nVamos agora executar o comando git:\nowner@localhost ➤ git status\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        analysis.R\n        save_data.R\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nO Git diz-nos que vê dois ficheiros mas que não estãop a ser rastreados. Logo se os modificarmos o Git não fará registo das alterações. Então é uma boa ideia fazermos o que o Git nos está a dizer para fazermos: vamos adicioná-los para que o Git os possa rastrear:\nowner@localhost ➤ git add\nNothing specified, nothing added.\nhint: Maybe you wanted to say 'git add .'?\nhint: Turn this message off by running\nhint: \"git config advice.addEmptyPathspec false\"\nSe fizermos apenas git add não acontece nada. precisamos de especificar que ficheiros queremos adicionar. Podemos nomeá-los um a um, por exemplo git add file1.R file2.tx, mas podemos simplesmente adicionar todos os ficcheiros da pasta simplesmente usando . e ver o que se passa.\nowner@localhost ➤ git add .\nDesta vez não temos nenhuma mensagem, será uma coisa boa? Vamos executar git status e ver o que aconteceu:\nowner@localhost ➤ git status\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   analysis.R\n        new file:   save_data.R\nÓptimo! os nossos ficheiros já estão a ser rastreados e podemos fazer o commit. Ou seja, fazemos commit quando estamos satisfeitos com o nosso trabalho e queremos guardar um instantâneo. Estes instantâneos podem então ser subidos para o Github (com um push). Desta maneira, as alterações ficam dispioníveis para outros colaboradores baixarem (pull). Voltaremos a estes conceitos mais adiante. Devemos também saber que existe um ficheiro especial chamado .gitignore que nos permite listar ficheiros e pastas que queremos que sejam ignmoradas pelo Git. Isto pode ser útil caso estejamos a trabalhar com dados sensíveis que não queremos subir para o github. Para já não usaremos o .gitignore, voltaremos a ela na parte 2 do livro.\nEstamos agora prontos para fazer o commit dos nossos ficheiros. Cada commit deve ter uma mensagem de commit e podemos escrever esta mensagem como uma opção do comando git commit:\nowner@localhost ➤ git commit -m \"Project start\"\nA opção -m serve para especificarmos a mensagem do commit. Voltemos a executar git status:\nowner@localhost ➤ git status\nOn branch master\nnothing to commit, working tree clean\nIsto sgnifica quie todas as alterações estão registadas num commit. Ou seja depois de fazermos o push, poderiamos queimar o nosso computador pois todas as alterações já estariam no Github.com. Podemos também optar por não fazer o push ainda e continuarmos a trabalhar e a fazer commit. Por exemplo, poderiamos fazer 5 commit e só depois fazer o push e assim os 5 commit subiriam para o github.com.\nVamos agora alterar um dos ficheiros. Abrimos analysis.R num editore simplesmente alteramos o início do ficheiro adicionando uma linha. Ou seja, de:\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\npara:\n# This script analyses housing data for Luxembourg\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(tidyr)\ne voltamos a fazer git status:\nowner@localhost ➤ git status\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nUma vez que o ficheiro está rastreado, o Git pode dizer-nos que alguma coisa foi alterada mas que não está num commit. Ou seja, se o nosso computador entrasse em auto-combustão estas alterações estriam perdidas para sempre. É melhor fazermos o commit e depois o push para o Github, o mas depressa possível.\nlembremo-nos que primeniro temos de adicionar as alterações a um commit com git add .:\nowner@localhost ➤ git add .\ne depois podemos fazer o commit das alterações com uma mensagem de commit:\nowner@localhost ➤ git commit -m \"Added a comment to analysis.R\"\nAs mensagens de commit devem ser o mais curtas e explicitas possível. Embora possa não ser fácil, compebnsa ter mensagens curtas e claras. De forma ideal, devemos procurar fazer commits pequenos, se possível um commit para cada alteração. Por exemplo se estivermos a adicionar ou corrigir comentários no nosso script, quan do terminarmos esta tarefa devemos fazer um commit. Depois, talvez fazermos um pouco de limpeza de código. Este é um commit separado. Assim podemos rever ou voltar a alterações anteriores duma forma mais simples. E isto será fundamental quando tivermols uma abordagem trunk-based paar colaboramos com outros colegas num projecto. Geralmente não é boa ideia passarmos o dia a fzer código e no fim do dia fazermos um commit enorme, mas muitas vezes é o que acontece.\nMesmo que as nossas alterações ainda não estejam no Github, já podemos voltar a um commit prévio. Suponhmos que apagamos acidentalmente um ficheiro com:\nowner@localhost ➤ rm analysis.R\nvamso executar git status e ver as alterações (é a linha que começa por delete):\nOn branch master\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nPois, ficamos sem analysis.R. E quando apagamos na consola significa que ficamos sem ele para sempre. Bem, tecnicamente não, há sempre maneiras de recuperar ficheiros apagados usando as ferrementas certas, mas como estamos a usar o Git, podemos usá-lo para recuperar o ficheiro. Uma vez que ainda não fizemos o commit podemos apenas dizer ao Git para ignorar as alterações. A maneira mais simples é fazer o stash das alterações e depois fazer o drop do stash:\nowner@localhost ➤ git stash\nSaved working directory and index state WIP on master: \\\n  ab43b4b Added a comment to analysis.R\nA eliminação foi mandada fora (se a quisermos de volta, podemos tê-la com git stash pop ) e o nosso projecto voltou ao momento do último commit. Vejamos os ficheiros:\nowner@localhost ➤ ls\nanalysis.R save_data.R\nAí está ele. Podemos agora livrar-nos do stash com git stash drop. Mas e se tivéssemos apagado o ficheiro e feito o commit? Nesse cenário não poderiamos usar o git stash, mas poderiamos reverter o commit. Vamos experimentar, primeiro eliminado o ficheiro:\nowner@localhost ➤ rm analysis.R\nvejamos o status atual, com git status:\nOn branch master\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    analysis.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nVamos adicionar as alterações e fazer o commit:\nowner@localhost ➤ git add .\nowner@localhost ➤ git commit -m \"Removed analysis.R\"\n[master 8e51867] Removed analysis.R\n 1 file changed, 131 deletions(-)\n delete mode 100644 analysis.R\nVoltamos a ver o status:\nowner@localhost ➤ git status\nOn branch master\nnothing to commit, working tree clean\nFeito! O git stash não nos pode ajudar. Para recuperarmos o ficheiro precisamos de saber qual é o commit para o qual queremos voltar. Cada commit para além duma mensagem tyem um identificador único qua podemos aceder através de git log:\nowner@localhost ➤ git log\ncommit 8e51867dc5ae89e5f2ab2798be8920e703f73455 (HEAD -&gt; master)\nAuthor: User &lt;owner@mailbox.com&gt;\nDate:   Sun Feb 5 17:54:30 2023 +0100\n\n    Removed analysis.R\n\ncommit ab43b4b1069cd987685253632827f19d7a402b27\nAuthor: User &lt;owner@mailbox.com&gt;\nDate:   Sun Feb 5 17:41:52 2023 +0100\n\n    Added a comment to analysis.R\n\ncommit df2beecba0101304f1b56e300a3cd713ce7366e5\nAuthor: User &lt;owner@mailbox.com&gt;\nDate:   Sun Feb 5 17:32:26 2023 +0100\n\n    Project start\nO primeiro, a partir de cima, é o último commit que fizemos. Queremos voltar para o que tem a mensagem “Added a comment to analysis.R”. O identificador único do commit é dado pela sequência longa de caracteres e designa-se por hash. Precisamos de o copiar (bastam os primeiros 10 caracteres). Agora podemos reverter para o commit correcto com o comando:\nowner@localhost ➤ git revert ab43b4b1069cd98768..HEAD\ne está feito! Podemos verificar que está tudo certo, executando ls e vendo que temos o ficheiro de novo, fazemos git log para lermos o log do que fizemos:\nowner@localhost ➤ git log\ncommit b7f82ee119df52550e9ca1a8da2d81281e6aac58 (HEAD -&gt; master)\nAuthor: User &lt;owner@mailbox.com&gt;\nDate:   Sun Feb 5 18:03:37 2023 +0100\n\n    Revert \"Removed analysis.R\"\n\n    This reverts commit 8e51867dc5ae89e5f2ab2798be8920e703f73455.\n\ncommit 8e51867dc5ae89e5f2ab2798be8920e703f73455 (HEAD -&gt; master)\nAuthor: User &lt;owner@mailbox.com&gt;\nDate:   Sun Feb 5 17:54:30 2023 +0100\n\n    Removed analysis.R\n\ncommit ab43b4b1069cd987685253632827f19d7a402b27\nAuthor: User &lt;owner@mailbox.com&gt;\nDate:   Sun Feb 5 17:41:52 2023 +0100\n\n    Added a comment to analysis.R\n\ncommit df2beecba0101304f1b56e300a3cd713ce7366e5\nAuthor: User &lt;owner@mailbox.com&gt;\nDate:   Sun Feb 5 17:32:26 2023 +0100\n\n    Project start\nSe usarmos um intervalo de commits em git revert, todos os commits são revertidos desde o inicial (não incluído) ao último. Neste exemplo, como apenas o commit iniciado por 8e51867dc5 está incluído, só este foi revertido. Isto é, teriamos obtido o mesmo resultado, se tivessemos executado git revert 8e51867dc5.\nEste pequeno exemplo mostra como é útil o Git, mesmo sem o Github e mesmo que trabalhando sozinhos num projecto. No mínimo oferece-nos a possibilidade de reverter alterações e uma linha do tempodo nosso projecto. Talvez isto não nos impressiono uma vez que vivemos num mundo em que serviços de cloud como o Dropbox tornam estas coisas bem acessíveis. Mas é no trabalho colaborativo que o Git (junto com serviços como o Github) brilha e se destaca. O Gite e os serviços de hospedagem de códgo como o Github permitem colaborações a grande escala: milhares de desenvolvedores contribuem para o kernel Linux, possivelmemte o mais bem sucedido projecto open-source, alimentando muitos dos atuais smartphones, super computadores e computadores incorporados3, e também podemos usar estas ferramentas para colaborações a uma escala pequena de forma muito eficiente.",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Controlo de versões com o Git</span>"
    ]
  },
  {
    "objectID": "git.html#git-e-github",
    "href": "git.html#git-e-github",
    "title": "4  Controlo de versões com o Git",
    "section": "4.3 Git e Github",
    "text": "4.3 Git e Github\nTemos já algum trabalho feito na nossa máquina e já fizamos alguns commits. Já estamos prontos para fazermos o push desses commits para o Github. Fazer o push significa subir as alterações para o Github. Com isto podemos disponibilizar as alterações para colegas de trabalho se estivermos a usar um repositório privado ou então disponibilizá-las para o mundo se estivermos a usar um repositório público.\nAntes de subirmos o que quer que seja para o Github, temos de criar um novo repositório. Este repositório terá o código do nosso projecto, bem como todas as alterações que temos estado a rastrear na nossa máquina, com o Git. No caso de adicionarmos um novo membro à equipa ele(a) poderá clonar o o repositório para o seu computador e ter acesso a todas as alterações, todos as mensagens de commit e toda a história do project. Se for público qualquer pessoa poderá clonar o projecto e contribuir com código. Vamos então ver alguns exemplos de como podemos colaborar com o Git usando o Github.\nVamos começar por ir para (https://github.com/)[https://github.com/] e criar um repositório:\n\n\n\nCriamos um novo repositório a partir do nosso dashboard.\n\n\nSeremos direcionados para a página:\n\n\n\nnomeamos o repositório e definimos se deve ser um repositório público ou privado.\n\n\nNomeamos o nosso repositório (1), e defenimos se deve ficar aberto para o mundo ou se qpenas deve ficar disponível para os nossos colegas de trabalho (2). Vamos fazer com que seja um repositório público.\nClicamos em Create repository e seremos encaminhados para a página:\n\n\n\nAlgumas instruções para arrancarmos.\n\n\nPodemos ver algunas instruções para começarmos o nosso projecto. A primeira coisa que devemos fazeré clicar em “SSH”:\n\n\n\nAsseguramo-nos de que selecionamos ‘SSH’.\n\n\nCom isto alteramos o link das instruções de https para ssh. Veremos mais à frente porque é que isto é importante. Para já, vamos ler as instruções. Uma vez que já comaçamos a trabalhar devemos seguir as instruções com o título “… or push an existing repository from the command line”“. Vamos rever estes comandos. O Github sugere-nos:\ngit remote add origin git@github.com:rap4all/housing.git\ngit branch -M main\ngit push -u origin main\nO mais importante são a primeira e a terceira linhas. A promeira adiciona o remoto (referido aqui como origin) que direciona para o nosso repositório. O link tem o nome de utilizador e o nome do projecto. Assim sempre que fazemos push, as nossas alterações serão subidas para o Github. A segunda linha altera o nome do branch master para main mas isto é meramente opcional pelo que vamos manter o nome master para o nosso branch. E por fim, o último comando faz o push das nossas alterações (aqui devemos trocar “main” por “master”, uma vez que não executámos a segunda linha).\nvamos então executar os comandos:\nowner@localhost ➤ git remote add origin git@github.com:rap4all/housing.git\nEste comando não devolve nenhum output e podemos fazer o push:\nowner@localhost ➤ git push -u origin master\ne falhou:\nERROR: Permission to rap4all/housing.git denied to b-rodrigues.\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\nA razão porque falhou é simples: o Github não faz ideia de quem somos. Lembremo-nos que se o repositório for público, qualquer pessoa pode cloná-lo. MAs isto não quer dizer que qualquer pessoa pode fazer push de código para o repositório. Ou seja, percisamos de uma maneira de dizer ao Github que somos os donos do repositório. Para isto, precisamos de duam forma segura de nos logarmos e vamos fazê-lo utilizando uma chave emparelhada de encriptação RSA público/privado. A ideia é simple. vamos gerar dois ficheiros no nosso computador. Estes dois ficheiros forma uma chave emparelhada público/privado. Subimos a chave pública para o Github; e sempre que quisermos interagir com o Github, este verificará a chave pública com a chave privada que temos na nossa máquina (nunca devemos enviar a nossa chave privada para ninguém). Se houver emparelhamento, o Github sabe que somos quem dizemos ser e deixanos fazer o push para o repositório. Por esta razão alteramos de https para ssh aqui a atrasado.\nVamos então gerar a chave emparelhada RSA público/privado. Num terminal Git Bash do Windows, executamos o comando:\nowner@localhost ➤ ssh-keygen\nPoderemos ver as seguintes linhas:\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa): \nSe não escrevermos nada e apenasfizermos “Enter”, veremos a mensagem:\nEnter passphrase (empty for no passphrase): \nPodemos também apenas fazer “Enter”. Não precisamos de definir uma palavra passe uma vez que a própria chave emparelhada ssh tratará de fazer o login. em alguns casos a palavra passe pode ser útil se tivemos receio de que alguém posa ter acesso fício à nossa máquina e que possa fazer push de código fazendo-se passar por nós. Mas se tivermos este tipo de receios então se calhar nem deveriamos usar o Github.\nAo fazermos “Enter”, é-nos pedido para comfirmarmos a palavra passe:\nEnter same passphrase again: \nE qaui volamos a fazer “Enter” sem escrevermos nada. Agora deverá aparecer a mensagem\nYour identification has been saved in /home/user/.ssh/id_rsa\nYour public key has been saved in /home/user/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:tPZnR7qdN06mV53Mc36F3mASIyD55ktQJFBAVqJXNQw owner@localhost\nThe key's randomart image is:\n+---[RSA 3072]----+\n|   .*=E*=.       |\n|   o o.oo.. .    |\n|  . .  o.  o o   |\n|   .  ..o.  . o  |\n|       +S    o.+.|\n|       .o.   o.o*|\n|       . o. + +=*|\n|        .  o ++*=|\n|            ..=oo|\n+----[SHA256]-----+\nSe formos para o caminho especificado na primeira linha (no nosso caso será /home/user/.ssh/) deveremos ver dois ficheiros, id_rsa e id_rsa.pub, as chaves privada e pública erspectivamente. Está quase: apenas temos de copiar o conteúdo do ficheiro id_rsa.pub para o Github.\nEm “Settings”:\n\n\n\nClicamos na nossa imagem de utilizador no canto superior direito.\n\n\ne depois clicamos em “SSH and GPG keys”:\n\n\n\nNas definições do utilizador, escolhemos ‘SSH and GPG keys’.\n\n\nclicamos em “New SSH key”. Damos um nome a esta chave que sirva para reconhecermos a máquina que gerou a chave, colamos o conteúdo de id_rsa.pub na caixa de texto e clicamos em “add SSH key”:\n\n\n\nCopiamos o conteúdo da chave pública aqui.\n\n\nPodemos voltar para o nosso terminal e tentar de novo fazer o push:\nowner@localhost ➤ git push -u origin master\nAparece a mensagem:\nThe authenticity of host 'github.com (140.82.121.3)' can't be established.\nED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nescrevemos yes e devemos ver a seguinte informação:\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (9/9), done.\nWriting objects: 100% (10/10), 2.77 KiB | 2.77 MiB/s, done.\nTotal 10 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), done.\nTo github.com:rap4all/housing.git\n * [new branch]      master -&gt; master\nBranch 'master' set up to track remote branch 'master' from 'origin'.\nE está feito! Os nossos commits estão agora guardados de forma segura no Github. Se formos paera a página prncipal do nosso repositório, deveremos ver:\n\n\n\nFinalmente!",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Controlo de versões com o Git</span>"
    ]
  },
  {
    "objectID": "git.html#conhecendo-o-github",
    "href": "git.html#conhecendo-o-github",
    "title": "4  Controlo de versões com o Git",
    "section": "4.4 Conhecendo o Github",
    "text": "4.4 Conhecendo o Github\nInstalamos com sucesso o Git e pussemo-lo a trabalhar com a nossa conta do Github. Se usarmos outra máquina para fazermos desenvolvimento, precisaremos de gerar outra chave emparelhada RSA nessa máquina e adicionar a chave pública ao Github. Se usarmos outra plataforma de hospedagem de código podemos utilizr a mesma chave par RSA, mas teremos de adcionar a chave pública á essa outra plataforma de hospedagem.\nAntes de continuarmos, vamos fazer um tour no Github.\n\n\n\nPágina inicial do nosso repositório.\n\n\nNa página do repositório vemos os mesmos ficheiros e pastas que estão na directoria raíz do projecto no nosso computador. Neste caso vemos dois ficheiros. O github sugere que adicionemos um ficheiro README; para já vamos ignorar. Atentemos ao menu em cima, debaixo do nome do repositório:\n\n\n\nVárias opções para escolha.\n\n\nAs opções mais relevantes para nós serão: “Issues”, “Pull requests”, “Actions”, e “Settings”.\nVamos começar por “Settings”:\n\n\n\nEscolhemos a opção ‘Settings’.\n\n\nHá várias opções de escolha mas a mais importante para o nosso propósito é a opção “Collaborators”. è aqui que podemos convidar outras opessoas para contribuirem para o repositório. As pessoas convidadas a partir daqui podem fazer push directamente para o repositório. Vamos convidar ol autor deste livro.\n\n\n\nSeguir para adicionarmos um colaborador.\n\n\nEscrevemos o nome de utilizador Github da pessoa. Também podemos convidar colaboradores, dando o seu endereço de e-mail.\n\n\n\nProcuramos os nossos colaboradores.\n\n\nClicamos no perfil do utilzador e ele(a) deverá receber um e-mail com o convite.\nEste é o aspeto na perspetiva da conta do Bruno:\n\n\n\nO Bruno pode fazer push como se fosse dono do repositório.\n\n\nÉ importante compreender a distinção entre convidar alguém a contribuir para o repositório e ter alguém de fora do projeto a contribuir. Vamos explorar estes dois cenários na próxima secção, mas antes disso, vamos ver em que consiste o separador “Issues”.\nSe o repositório for público, qualquer pessoa pode abrir um issue para reportar um bug ou para sugerir algumas ideias, se o repositório for privado apenas os colaboradores podem fazer isto.\nVamos abrir um issue para ilustrar como funciona:\n\n\n\nClicamos em ‘New issue’ no separdor ‘Issues’ do nosso projecto.\n\n\nSeremos reencaminhados para o interface:\n\n\n\nAqui escrevemos qual é o issue.\n\n\nDamos um bom título ao issue (1), adicionamos uma descrição pormenorizada (2), (opcionalmente) atribuimo-lo a alguém (3) e (opcionalmente) adicionamos-lhe uma etiqueta (4), finalmente clicamos em “Submit new issue” (5) para fazermos a submissão:\n\n\n\nDevemos tentar dar o maior detalhe possível.\n\n\nMuitas vezes os issues não têm de ser longos e funcionam principalmente como lembretes. Por exemplo, o dono do repositório não teve tempo de adcionar um ficheiro README, mas não se quer esquecer de o fazer mais tarde. O autor atribui o issue ao Bruno. logo será responsabilidade do Bruno adicionar o README. A gestão de projectos orientada para issues é uma estratégia muito válida quando se trabalha de forma assíncrona e descentralizada.\nSe encontrarmos um bug e quisermos abrir um issue, é muito importante demos um exemplo mínimo e reprodutível (MRE - minimal reproducible exemple). MREs são chuncks de código que podem ser executados facilmente por alguém para além de nós mesmos e que produzem o bug de forma confiável. Curiosamente, se compreendermos o que torna um MRE mínimo e reprodutível, compreendemos também o que tornará as nossas condutas reprodutíveis. Então, o que é importante para uma MRE?\nEm primeiro lugar, o código tem de ser autónomo. Por exemplo, se forem necessários alguns dados, é necessário fornecer os dados. Se os dados forem sensíveis, é necessário pensar no erro com mais pormenor: o erro deve-se à estrutura dos dados ou manifesta-se em qualquer tipo de dados? Se for esse o caso, podemos usar alguns dos conjuntos de dados incorporados no R (iris, mtcars, etc) para o nosso MRE.\nO nosso MRE precisa de pacotes extra para funcionar? Então devemos ser o mais claros possível, e não podemos dar apenas os nomes dos pacotes, mas temos de dar também suas versões (uma boa prática é copiar e colar o output de sessionInfo() no final do issue).\nPor fim, o nosso exemplo depende de algum objeto definido no ambiente global? Se sim, também precisamos de dar o código para criar esse objeto.\nA fasquia que precisamos de definir para um MRE é a seguinte: salvo dependências de pacotes que podem ser necessário instalar de antemão, as pessoas que nos tentam ajudar devem ser capazes de executar o nosso script simplesmente copiando-o e colando-o numa consola de R. Qualquer outra manipulação que lhes seja pedida não é aceitável: lembremo-nos que no desenvolvimento de código aberto, os programadores trabalham muitas vezes durante o seu tempo livre e não nos devem apoio técnico! E mesmo que o fizessem, é sempre uma boa ideia tornar-lhes a tarefa o mais fácil possível para nos ajudarem, pois isso simplesmente aumenta a probabilidade de que eles realmente nos ajudem.\nAlém disso, escrever uma MRE normalmente leva-nos a depurar o código. Tal como na depuração para patinhos de borracha4, com simples facto de tentarmos explicar o problema podemos descobrir onde está o erro. Mas ao escrevermos um MRE, estamos também a reduzir o problema às suas partes mais básicas, e a remover tudo o que é desnecessário. Ao fazê-lo, podemos aperceber-nos que o que pensavamos ser um bug dum pacote talvez fosse antes um problema na “interface entre o teclado e a cadeira”.\nPor isso, não devemos subestimar a utilidade de criar MREs de alta qualidade para os nossos problemas! Um pacote que o pode ajudar nesta tarefa é o {reprex} (ver aqui5).",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Controlo de versões com o Git</span>"
    ]
  },
  {
    "objectID": "git.html#conclusão",
    "href": "git.html#conclusão",
    "title": "4  Controlo de versões com o Git",
    "section": "4.5 Conclusão",
    "text": "4.5 Conclusão\nAgora já temos o nosso primeiro repositório e já sabemos o básico sobre a utilização do Git e do Github.com. POdemos sempre rever os comandos em cima. Talvez adicionar mais alguns ficheiros ao nosso repositório, removê-los, tentar reverter certos commits, etc. Criar um novo repositório e tentar enviar alguns arquivos ou scripts para lá. Devemos dedicar realmente algum tempo para entendermos o que fizemos e como devemos usar estas ferramentas, pois são essenciais para a reprodutibilidade.",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Controlo de versões com o Git</span>"
    ]
  },
  {
    "objectID": "git.html#footnotes",
    "href": "git.html#footnotes",
    "title": "4  Controlo de versões com o Git",
    "section": "",
    "text": "https://is.gd/rQgCj8↩︎\nhttps://is.gd/9HZqW4↩︎\nhttps://www.zdnet.com/article/who-writes-linux-almost-10000-developers/↩︎\nhttps://en.wikipedia.org/wiki/Rubber_duck_debugging↩︎\nhttps://reprex.tidyverse.org/↩︎",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Controlo de versões com o Git</span>"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "5  Colaboração com desenvolvimento Trunk-based",
    "section": "",
    "text": "5.1 Colaborar como equipa",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colaboração com desenvolvimento *Trunk-based*</span>"
    ]
  },
  {
    "objectID": "github.html#colaborar-como-equipa",
    "href": "github.html#colaborar-como-equipa",
    "title": "5  Colaboração com desenvolvimento Trunk-based",
    "section": "",
    "text": "5.1.1 TBD básico\nLembremo-nos do issue que abrimos para o Bruno. O Bruno resolverá o issue adicionando um ficheiro README. Isto vai dar-nos a oportunidade de introduzir o desenvolvimento trunk-based (TBD). A ideia do desenvolvimento trunk-based é simples; os membros da equipa devem trabalhar em branches separados para adicionar funcionalidades ou corrigir bugs, e depois mergir o seu branch ao tronco (“trunk” que no nosso caso é o branch master) para adicionar as suas alterações ao código base principal. Este processo deve ser rápido, idealmente diário, ou assim que o novo código estiver pronto. Quando há muito trabalho que se acumula num branch durante vários dias ou semanas, o processo de mergir com o branch master pode ser doloroso. Se trabalharmos com branches de curta duração, quando surgirem conflitos, estes podem ser resolvidos rapidamente. Com isto facilitamos também a revisão do código, uma vez que assim o revisor apenas precisa de rever pequenas porções de código de cada vez. Se, ao contrário, com branches de longa duração com alterações em muito código para mergir, na revisão de todas as alterações e na resolução de conflitos que possam surgir, teremos muito mais trabalho. Para evitarmos este problema, o melhor é mergir diariamente ou sempre que adicionarmos uma parte de código, e muito importante, este código não pode quebrar todo o projecto (mais à frente usaremos testes unitários como garantia).\nResumindo, para evitarmos demasiado trabalho ao mergir branches que se afastaram demasiado do trunk, criaremos branches, adicionaremos o nosso código e mergimos com o trunk assim que possível. O mais cedo possível pode significar várias coisas, mas normalmente significa assim que uma funcionalidade foi adicionada, um bug foi corrigido, ou assim que adicionámos algum código que não quebra todo o projeto, mesmo que a funcionalidade que queríamos adicionar ainda não esteja concluída. A ideia é que se o merge falhar, deve falhar o mais cedo possível. As falhas precoces são mais fáceis de resolver.\nO nosso objetivo deve ser fornecer um projeto funcional para qualquer pessoa que clone o branch master em qualquer momento e ainda oferecer uma maneira simples de instalar uma versão pontual do projeto).\nEntão, de volta ao nosso problema. Primeiro, o Bruno precisa clonar o repositório:\nbruno@computer ➤ git clone git@github.com:rap4all/housing.git\nPara adicionar uma funcionalidade, o Bruno precisa de criar um novo branch com o comando git checkout com a flag -b:\nbruno@computer ➤ git checkout -b \"add_readme\"\nO projecto muda automaticamente para o novo branch:\nSwitched to a new branch 'add_readme'\nPodemos executar git status para verificarmos:\nbruno@computer ➤ git status\nOn branch add_readme\nnothing to commit, working tree clean\nO Bruno adiciona um novo ficheiro README.md com o seguinte texto:\n# Housing data for Luxembourg\n\nThese scripts for the R programming language download nominal\nhousing prices from the *Observatoire de l'Habitat* and\ntidy them up into a flat data frame.\n\n- save_data.R: downloads, cleans, and creates data frames from the data\n- analysis.R: creates plots of the data\nVamos gravar e voltar a executar git status, para vermos o que aconteceu:\nbruno@computer ➤ git status\nO Git diz ao Bruno que o ficheiro README.md não está a ser rastreado:\nOn branch add_readme\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        README.md\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nO Bruno vai então rastrear e fazer o push das alterações. Além disso, o Bruno vai usar um truque, quando fizer o push: como o Bruno está a trabalhar para resolver num issue, seria optimo se também o fechasse quando fizer o push. O que é possível referenciando o número do issue na mensagem de commit:\nbruno@computer ➤ git add .\nbruno@computer ➤ git commit -m \"fixed #1\"\n#1 é relativo ao número do issue (é o primeiro issue aberto no repositório). Assim, referenciando este issue com o seu número na mensagem de commit e fazendo o push, o issue fica fechado, automaticamente:\nbruno@computer ➤ git push origin add_readme\nComo podemos ver, o Bruno faz o push do branch add_readme (aberto para resolver o issue) e não do master. Se tentasse fazer o push do master seria impressa uma mensagem a dizer que o master está up-to-date. Vejamos o output depois do push do add_readme:\nEnumerating objects: 4, done.\nCounting objects: 100% (4/4), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 501 bytes | 501.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: Create a pull request for 'add_readme' on GitHub by visiting:\nremote:     https://github.com/rap4all/housing/ pull/new/add_readme\nremote:\nTo github.com:rap4all/housing.git\n * [new branch]      add_readme -&gt; add_readme\nO Git diz-nos que o Bruno deve criar um pull request. Mas o que é isto? Bem, se quisermos mergir o nosso branch para o trunk, devemos fazê-lo através de um pull request. Vejanos o que o Bruno vê no Github:\n\n\n\n\n\nO Bruno vê que o branch ‘add_readme’ foi recentemente updated.\n\n\n\n\nO Bruno pode agora decidir continuar a trabalhar neste branch ou, uma vez que o objetivo deste branch era apenas adicionar o ficheiro README.md, decidir fazer um pull request.\nAo clicar no botão “Compare & pull request”, o Bruno vê agora isto:\n\n\n\n\n\nNeste ecra podemos ver facilmente as alterações.\n\n\n\n\nO Bruno pode deixar um comentário, ver o que mudou (neste caso, foi adicionado um único ficheiro) e, mais importante, adicionar um revisor, se necessário:\n\n\n\n\n\nDeixemos o chefe decidir se isto está bem.\n\n\n\n\nIsto é o que o Bruno vê agora:\n\n\n\n\n\nO Github diz-nos que o branch pode ser mergido em segurança.\n\n\n\n\nO Bruno pediu a revisão, mas o Github diz-nos que o branch pode ser mergido. Isto porque adicionámos um ficheiro e não tocámos em mais nada, e mais ninguém trabalhou no projeto enquanto o Bruno estava a trabalhar. Portanto, não há riscos de surgirem conflitos.\nVejamos o que o dono do projecto vê agora. O dono do projeto deveria ter recebido uma notificação para revisar o pull request:\n\n\n\n\n\nO dono foi notificado para rever o pull request.\n\n\n\n\nAo clicar na notificação, o dono do projecto é levado para o ecrã:\n\n\n\n\n\nÉ tempo de rever o pull request.\n\n\n\n\nAqui, o revisor pode verificar o commit, os ficheiros que foram alterados e ver se existem conflitos entre este código e o código base no branch master (ou trunk). O Github também nos diz duas coisas interessantes: o dono pode adicionar uma regra que diz que qualquer pull request deve ser aprovado, e também que a integração contínua não foi configurada (veremos o que isso significa na segunda parte deste livro).\nVamos adicionar uma regra que força cada pull request a ser aprovado. Ao clicar em “Add rule”, aparece o seguinte ecrã:\n\n\n\n\n\nEscolhe como proteger o branch maste.\n\n\n\n\nClicando na primeira opção, outras opções aparecem:\n\n\n\n\n\nSão agora requeridas revisões.\n\n\n\n\nAo escolher estas opções, o dono está a forçar o desenvolvimento trunk-based (bem, os colaboradores ainda têm de submeter pull request frequentemente, caso contrário, podemos deparar-nos com situações em que o merge pode ser muito difícil).\nVamos selecionar uma última opção: se descermos, podemos selcionar a opção “Do not allow bypassing the above settings”. Com isto garantimos que nem mesmo os administradores (os donos do projecto) podem contornar estas regras.\nVoltemos ao pull request. Podemos ver que agora é requerida uma revisão:\n\n\n\n\n\nAltura de rever.\n\n\n\n\nE o dono tem de ver os ficheiros que foram alterados:\n\n\n\n\n\nVerifica o código e adiciona comentários, se forem necessários.\n\n\n\n\nÉ possível adicionar comentários a linhas, individualmente:\n\n\n\n\n\nÉ possível adicionar comentários a linhas.\n\n\n\n\nAo clicarmos no sinal mais, aparece uma caixa onde podemos deixar um comentário. Neste caso está tudo bem pelo que o dono apenas tem de clicar no butão “Viewed”:\n\n\n\n\n\nBom trabalho!\n\n\n\n\nAo clicarmos em “Review changes”, podemos adicionar um comentário geral, aprovar o pull request, ou pedir alterações que devem ser efectuadas antes do merge. Vamos aprovar:\n\n\n\n\n\nSem razões de queixa.\n\n\n\n\nAo submeter a revisão, o revisor volta para o issue:\n\n\n\n\n\nEstá feito! Podemos fazer o merge do pull request.\n\n\n\n\nO revisor pode fazer o merge do pull request clicando no butão “Merge pull request”. O Github até sugere que apeguemos o branch que já cumpriu o seu propósito:\n\n\n\n\n\nLivremo-nos deste branch.\n\n\n\n\nVamos apagá-lo (podemos sempre restaurá-lo).\n\n\n5.1.2 Tratando conflitos\nCom referimos no capítulo anterior, o Git facilita o tratamento de conflitos. Sejamos claros; mesmo com o Git, às vezes pode ser complicado resolver conflitos. Mas devemos saber que se é complicado resolver conflitos com o Git, então é provavelmente impossível fazê-lo de outra forma qualquer, e que inevitavelmente teria de haver de alguém a corrigir os ficheiros manualmente. O que facilita a resolução de conflitos com o Git é o facto do Git nos dizer onde podemos encontrar os conflitos por linha. Por exemplo, se um colaborador alterar as primeiras 10 linhas dum ficheiro e outro alterar as 10 seguintes, então não haverá qualquer tipo de conflito e o Git pode fazer o merge automático de ambas as colaborações num único ficheiro. Outras ferramentas como o Dropbox, falhariam numa situação destas, uma vez que apenas conseguem lidar com conflitos por ficheiro. O mesmo ficcheiro foi alteardo por duas pessoas diferentes? Independentemente de onde aconteceram essas mudanças, temos um conflito para tratar, e pior nem sequer sabemos onde é que esses conflitos estão no ficheiro. Teriamos de verificar as duas cópias do ficheiro manualmente. o Git no caso em que as mesmas linhas são alteradas, destaca-as claramente, para que as possamos encontrar rapidamente e resolver os problemas.\nVeremos tudo isto nas secções seguintes.\nMas como é que acontecem os conflitos. Imaginemos a seguinte situção. Tanto o Bruno como o dono do projecto criam os seus branches e editam o mesmo ficheiro. Talvez tenham falado por telefone e decidido acrescentar uma funcionalidade ou corrigir um bug. Talvez tenham decidido que não valia a pena abrir um issue e atribuir a tarefa a uma pessoa. A questão foi discutida pelo telefone e decidiram que o Bruo deveria fazê-lo. Ou seria o dono do projecto que ficou de fazê-lo? Já nenhum deles se lembra. De qualquer forma ambos decidem fazer a alteração no mesmo ficheiro pelo que surge um conflito.\nPrimeiro o Bruno precisa de voltar para o branch master no seu computador:\nbruno@computer ➤ git checkout master\nSwitched to branch 'master'\nYour branch is behind 'origin/master' by 2 commits, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\nO Git diz-nos que devemos atualizar o código no nosso computador executando git pull. Usamos o git push para subir código para o Github, e usamos o Git pull para baixarmos código do Github. Vamos executar e ver o que acontece:\nbruno@computer ➤ git pull\nUpdating b7f82ee..c774ebf\nFast-forward\n README.md | 7 +++++++\n 1 file changed, 7 insertions(+)\n create mode 100644 README.md\nForam atualizados ficheiros no computador do Bruno. O dono do projecto (que se chama owner) pode fazer o mesmo e verá o mesmo. Agora o Bruno cria um novo branch para trabalhar na nova funcionalidade:\nbruno@computer ➤ git checkout -b add_cool_feature\nE o dono do projecto também cria um novo branch:\nowner@localhost ➤ git checkout -b add_sweet_feature\nAgora ambos editam o mesmo ficheiro, analysis.R. O Bruno adicionou esta função:\n\nmake_plot &lt;- function(country_level_data,\n                      commune_level_data,\n                      commune){\n\n  filtered_data &lt;- commune_level_data %&gt;%\n    filter(locality == commune)\n\n  data_to_plot &lt;- bind_rows(\n    country_level_data,\n    filtered_data\n  )\n\n  ggplot(data_to_plot) +\n    geom_line(aes(y = pl_m2,\n                  x = year,\n                  group = locality,\n                  colour = locality))\n}\n\nDesta forma o Bruno pode apagar o código repetido e criar gráficos assim:\n\nlux_plot &lt;- make_plot(country_level_data,\n                      commune_level_data,\n                      communes[1])\n\n\n# Esch sur Alzette\n\nesch_plot &lt;- make_plot(country_level_data,\n                       commune_level_data,\n                       communes[2])\n\n# and so on...\n\nO resultado final é o mesmo, mas usando esta função, o código é mais curto e mais claro. Além disso se alguém quizer alterar o tema do gráfico apenas o tem de fazer num sítio e não para cada comunidade. Mas o que é que o dono do projecto alterou? O dono começou por eliminar a linha que carrega o package {purrr}, uma vez que nenhuma função do package estava a ser usada no script, e também alterou %&gt;% para |&gt;. Parece que foi esquecido mais do que apenas saber quem deveria fazer as alterações… De qualquer forma, ambos sobem as suas alterações dos seus respectivos branches. Aqui é a parte do Bruno:\nbruno@computer ➤ git add .\nbruno@computer ➤ git commit -m \"make_plot() for plotting\"\nbruno@computer ➤ git push origin add_cool_feature\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 647 bytes | 647.00 KiB/s, done.\nTotal 3 (delta 1), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote:\nremote: Create a pull request for 'add_cool_feature' on GitHub by visiting:\nremote:      https://github.com/rap4all/housing/pull/new/add_cool_feature\nremote:\nTo github.com:rap4all/housing.git\n * [new branch]      add_cool_feature -&gt; add_cool_feature\ne esta é a parte do dono:\nowner@localhost ➤ git add .\nowner@localhost ➤ git commit -m \"cleanup\"\nowner@localhost ➤ git push origin add_sweet_feature\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 449 bytes | 449.00 KiB/s, done.\nTotal 3 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote:\nremote: Create a pull request for 'add_sweet_feature' on GitHub by visiting:\nremote:      https://github.com/rap4all/housing/pull/new/add_sweet_feature\nremote:\nTo github.com:rap4all/housing.git\n * [new branch]      add_sweet_feature -&gt; add_sweet_feature\nVejamos o que aconteceu: dois desenvolvedores alteraram o mesmo ficheiro, analysis.R, em dois branches diferentes. Ambos os branches precisam de ser mergidos no trunk.\nLogo o Bruno faz um pull request:\n\n\n\n\n\nO Bruno abre um pull request depois de terminar as alterações.\n\n\n\n\nPrimeiro seleciona o branch com a funcionalidade (1), depois clica em “Contribute” (2) e em “Open pull request” (3). O Bruno passa para o ecrã:\n\n\n\n\n\nsem conflitos, para já…”\n\n\n\n\nAgora o Bruno pode clicar em “Create pull request” mas como são requeridas revisões não está disponível o merge automático.\nVejamos o que acontece do lado do dono do projecto. Antes de mais, há uma notificação de que está pendente uma revisão:\n\n\n\n\n\nNova revisão pendente.\n\n\n\n\nAo clicar na notificação, o dono do projecto pode rever o pull request e decidir o que fazer com ele. Neste ponto, o dono ainda não abriu um pull request para a alteração que fez. E pode ser que seja uma coisa boa, pois pode ver que as alterações do Bruno entram em conflito com as alterações que ele fez.\nComo podemos avançar? Simples. o dono do projecto pode decidir aprovar o pull request e fazer o merge das alterações do Bruno no branch master (ou trunk). E em vez de abrir um pull request para fazer o merge das suas próprias alterações no trunk, o causaria um conflito, pode fazer merge das alterações do trunk para o seu branch (em que fez as suas alterações). Isto também criará um conflito, mas agora o dono pode lidar com ele mais facilmente na sua máquina e depois só tem de fazer o push dum novo commit com ambas as alterações integradas. A imagem seguinte descreve este workflow:\n\n\n\nConflict solving with trunk-based development.\n\n\nPrimeiro passo, o dono revê e aprova o pull request do Bruno:\n\n\n\n\n\nPrimeiro, aprovemos as alteraçõs.\n\n\n\n\nÉ feito o merge do pull request e o branch é apagado. Não faz sentido que o dono do projecto faça um pull request com as suas alterações pois criariam um conflito que teria de resolver. Assim o dono do projecto volta para o seu computador e faz um update do código no seu branch em que está a fazer alterações, ao fazer o merge do master no seu branch.\nO dono do projecto confirma que está a trabalhar no seu branch de alterações:\nowner@localhost ➤ git status\nOn branch add_sweet_feature\nnothing to commit, working tree clean\nAgora pode fazer o update com o código do master, executando git pull:\nowner@localhost ➤ git pull origin master\nO dono vê a seguinte mensagem.\nremote: Enumerating objects: 6, done.\nremote: Counting objects: 100% (6/6), done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 4 (delta 1), reused 3 (delta 1), pack-reused 0\nUnpacking objects: 100% (4/4), 1.23 KiB | 418.00 KiB/s, done.\nFrom github.com:rap4all/housing\n * branch            master     -&gt; FETCH_HEAD\n   c774ebf..a43c68f  master     -&gt; origin/master\nAuto-merging analysis.R\nCONFLICT (content): Merge conflict in analysis.R\nAutomatic merge failed; fix conflicts and then commit the result.\nO Git detecta que há alguns conflitos e avisa que têm de ser resolvidos para depois se fazer o commit. Vamos abrir o ficheiro analysis.R e ver com está (link1). Primeiro, podemos ver que o Git lida com os conflitos linha a linha. Logo, cada linha alterada pelo dono que não está em conflito com as alterações do Bruno é automaticamente atualizada, refletindo as alterações do dono. Por exemplo, a linha que que carrega o package {purrr} foi apagada e todos os %&gt;% passaram a |&gt;. Estas alterações foram feitas sem problemas.\nAgora temos de perceber o que acontece quando são detectados conflitos em algumas linhas. Por exemplo, o primeiro conflito que vemos:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nfiltered_data &lt;- commune_level_data |&gt;\n  filter(locality == communes[1])\n=======\n  filtered_data &lt;- commune_level_data %&gt;%\n    filter(locality == commune)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; a43c68f5596563ffca33b3729451bffc762782c3\nVemos o aspecto das linhas no computados do dono e o seu aspecto no branch master (ou trunk). As linhas entre &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD e ======= são as que pertencem ao branch das alterações do dono. As linhas entre ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt; a43c68f5596563ffca33b3729451bffc762782c3 são as do branch master. A cadeia longa de caracteres que começa com a43c68f é o hash do commit de onde vêm aquelas linhas.\nIsto facilita muito as coisas; apenas precisamos de eliminar o código que não está atualizado fazer o commit e o push do ficheiro corrigido. Neste caso o dono do projecto apenas tem de eliminar &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD e ======= e tudo o que está entre estas linhas, bem como as linhas com o hash. O dono do projecto agora apenas tem de fazer commit, push das alterações, abrir um pull request, pedir ao Bruno para rever as alterações e fazer o merge de tudo no master.\n\n\n\n\n\nO conflito foi resolvido com elegância.\n\n\n\n\nEm (1) vemos o commit que lida com os conflitos, em (2) o dono pede a revisão do Bruno e em (3) vemos que o Bruno fez a revisão e aprovou. Finalmente, é feito o merge do pull request e o branch é apagado.\n\n\n5.1.3 Culpar a pessoa certa\nSe há muitas pessoas a colaborar num projecto, pode ser difícil sabermos quem alterou o quê e quando o fez. É aqui que o comando git blame é útil. Se quisermos saber quem alterou o ficheiro analysis.R, apenas temos de executar:\nowner@localhost ➤ git blame analysis.R\ne veremos o histórico detalhado, linha a linha com o nome dos colaboradores e a data:\nb7f82ee1 (Bruno 2023-02-05 18:03:37 +0100 24) #Let’s also compute it...\nb7f82ee1 (Bruno 2023-02-05 18:03:37 +0100 25) \n55804ccb (Owner 2023-02-11 22:33:20 +0000 26) country_level_data &lt;- ...\n55804ccb (Owner 2023-02-11 22:33:20 +0000 27)   mutate(p0 = ifelse(y...\nPodemos ver que o Bruno alterou as linhas 24 e 25 no dia 5 de fevereiro como parte do commit com o hash b7f82ee1, enquanto que o dono do repositório alterou as linhas 26 e 27 no dia 11 de fevereiro como parte do commit com o hash 55804ccb.\nPodemos aproveitar o git blame para termos uma visão das alterações em cada ficheiro.\n\n\n5.1.4 TBD simplificado\nO workflow que foi aqui apresentado pode parecer um pouco rigido para equipas pequenas (menos de 4 ou 5 colaboradores). Podemos sempre adoptar uma versão simplificada do desenvolvimento trunk-based (TBD) em que os colaboradores não precisam de abrir pull requests para fazer o merge dos seus branchs com novas funcionalidades no trunk, e em que não precisamos de revisores. Nestes casos o Git força-nos a fazer pull das alterações caso já tenha havido alguém a mergir o seu branch antes de nós. Assim, ao fazermos o pull podem surgir conflitos e é responsabilidade nossa resolver esses eventuais conflitos, fazer o commit e depois o push com os conflitos resolvidos. Outro colaborador que queira fazer o merge do seu branch no trunk terá de fazer novo pull e garantir que resolve os conflitos (caso existam) antes de fazer esse merge. Se não surgirem conflitos (por exemplo, no caso de dois colaboradores estarem a trabalhar em ficheiros diferentes, ou linhas diferentes do mesmo ficheiro) então não é necessário resolve-los e ambos podem fazer merge dos respectivos branches no master.\n\n\n5.1.5 Conclusão\nOs principais conceitos do desenvolvimento trunk-based são:\n\nCada colaborador abre o seu novo branch para desenvolver uma funcionalidade ou corrigir um bug, e trabalha sozinho no seu branch;\nNo final do dia (ou após um determinado periodo de tempo previamente acordado), é necessário fazer o merge de todos os branches;\nNesta altura é necessário resolver todos os conflitos;\nSe uma determinada funcionalidade pode demorar mais do que um dia então deve ser dividida em tarefas mais pequenas que possam ser mergidas diariamente. No início estas contribuições podem ser simples marcadores de posição que são gradualmente preenchidos com código até que seja possível implementar a funcionalidade. Esta estratégia é chamada de branching by abstraction;\nO branch master (ou trunk) contém sempre código funcional, de nível de produção;\nPara impor alguma disciplina, pode valer a pena obrigar a abrir pull request para fazer merge no trunk e também requerer a revisão.",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colaboração com desenvolvimento *Trunk-based*</span>"
    ]
  },
  {
    "objectID": "github.html#contribuir-com-repositórios-públicos",
    "href": "github.html#contribuir-com-repositórios-públicos",
    "title": "5  Colaboração com desenvolvimento Trunk-based",
    "section": "5.2 Contribuir com repositórios públicos",
    "text": "5.2 Contribuir com repositórios públicos\nNesta última secção, vamos ver como podemos contribuircom um projecto quando não fazemos parte da equipa desse projecto. Por exemplo, se formos utilizadores dum package R, encontrarmos um bug e quisermos propor uma solução. Ou se apenas detectamos um erro ortográfico num README desse package e quisermos propor uma correção. Seja qual for a situação, se o repositório for público qualquer pessoa pode sugerir uma rectificação. Vejamos como exemplo o repositório:\n\n\n\nUm repositório público.\n\n\nEste repositório tem código escrito por um individuo chamado “rap4all” e o Bruno usa este código diariamente. No entanto o Bruno detectou um erro ortográfico no README e quer propor uma correção.\nPrimeiro o Bruno vai visitar o repositório (que é público e pode ser visto online) e cria um fork:\n\n\n\nO Bruno precisa de criar um fork do repositório.\n\n\nO fork cria uma cópia do repositório na conta do Bruno:\n\n\n\nO Bruno faz o fork.\n\n\nO Bruno pode ver também o fork na sua conta:\n\n\n\nO fork do Bruno.\n\n\nAgora o Bruno pode clonar o repositório e trabalhar nele, uma vez que está a trabalhar numa cópia do repositório que é dele. Tudo o que o Bruno fizer nesta cópia não vai afectar o repositório original:\nbruno@computer ➤ git clone git@github.com:b-rodrigues/my_cool_project.git \n\n\n\nO Bruno corrigiu o erro neste fork.\n\n\nComo podemos ver, o fork do Bruno adianta o repositório original num commit. Ao clicar em “Contribute”, o Bruno pode abrir um pull request para propor a correção ao repositório original.\nEste pull request será aberto no repositório original:\n\n\n\nO Bruno abre um pull request para contribuir com a sua correção.\n\n\nMas o que é que o dono do repositório original, “rap4all”, vê? O pull request que o Bruno abriu aparece agora no menu “Pull request” do repositório original e o dono pode ver nqual é a contribuição, se quebra o código ou não, etc. Este workflow é essencialmente o mesmo apresentado em cima no desenvolvimento trunk-based com pull requests e revisões antes do merge (à excepção do fork do repositório).\n\n\n\nO dono do repositório original pode aceitar a correção do Bruno.\n\n\nAo fazer o merge da correção, o dono pode agora beneficiar dum ficheiro README gramaticamente corrigido:\n\n\n\nA beleza do open source.",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colaboração com desenvolvimento *Trunk-based*</span>"
    ]
  },
  {
    "objectID": "github.html#leitura-recomendada",
    "href": "github.html#leitura-recomendada",
    "title": "5  Colaboração com desenvolvimento Trunk-based",
    "section": "5.3 Leitura recomendada",
    "text": "5.3 Leitura recomendada\nPara sabermos tudo sobre o desenvolvimento trunk-based, devemos consultar o manual do Hammant (2020). Uma versão gratuita e online deste livro está disponível em https://trunkbaseddevelopment.com/.",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colaboração com desenvolvimento *Trunk-based*</span>"
    ]
  },
  {
    "objectID": "github.html#footnotes",
    "href": "github.html#footnotes",
    "title": "5  Colaboração com desenvolvimento Trunk-based",
    "section": "",
    "text": "https://is.gd/ktWtjr↩︎",
    "crumbs": [
      "Parte 1: Don't Repeat Yourself",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Colaboração com desenvolvimento *Trunk-based*</span>"
    ]
  },
  {
    "objectID": "part2_intro.html",
    "href": "part2_intro.html",
    "title": "Parte 2: Write IT Down",
    "section": "",
    "text": "O iceberg da reproducibilidade\nvamos refletir um pouco sobre a primeira parte do livro e porque é que ainda não falamos de nada directamente relacionado com a reproducibilidade.\nNa introdução falamos sobre o continuum ou o espectro da reproducibilidade e agora vamos detalhar este conceito. Chamemos-lhe o iceberg da reproducibilidade:\nO iceberg da reproducibilidade.\nPorquê iceberg? Porque as partes do iceberg que vemos, são óbvias, são como executarmos as nossas análises num ambiente baseado em cliques, como o Excel. Isto é, não é necessário nenhum conhecimento especial mnem mesmo nenhuma formação específica. Apenas precisamos de tempo, pelo que quem usa estas ferramentas não .é eficiente e portanto tendem a compensar com muitas horas de trabalho.\nVamos um nível mais abaixo e escrever um guião. Foi aqui que começamos. O nosso script nem é assim tão mau, fazia o que pretendíamos. Ao contrário do fluxo de trabalho baseado em cliques, pelo menos podemos relê-lo, outras pessoas podem lê-lo, e até seria possível executá-lo no futuro, ainda que com algum esforço e com um pouco de sorte. Para que este script possa se executado no futuro com êxito, não pode depender de pacotes com atualizações que ponham em causa o código (por exemplo, funções que são renomeadas). Além disso, se esse script depender de uma fonte de dados, os autores originais também terão de garantir que a mesma fonte de dados continua disponível. Outro problema são as colaborações ao escrever esse script. Sem nenhuma ferramenta de controle de versões nem plataforma de hospedagem de código, a colaboração neste script pode se transformar rapidamente em um pesadelo.\nÉ aqui que o Git e Github entram em acção, num nível mais profundo. Temos a vantagem que a colaboração foi simplificada. O histórico de alterações está disponível para todos os elementos da equipe e é possível reverter alterações, experimentar novos recursos usando ramificações e gerir o projeto de modo geral. Neste níveltambém usamos novos paradigmas de programação para tornar o código do projeto menos verboso, usando a programação funcional, com os benefícios adicionais de facilitar o teste, a documentação e a partilha (que discutiremos em sua plenitude nesta parte do livro). Usando a programação letrada, também é muito mais fácil chegar ao nosso resultado final (que geralmente é um relatório). Exploramos ao máximo as ideias DRY, para garantir que nosso código seja de alta qualidade.\nMas, se o orçamento e o tempo permitirem, ainda podemos ir mais fundo, e definitivamente devemos fazê-lo. No futuro podemos querer atualizar o nosso script para usarmos a funcionalidade mais recente do nosso pacote preferido mas com a gravação da versão do pacote, ficamos limitados a uma versão muito antiga e às suas dependências. Como podemos saber que ao atualizarmos aquele pacote não partimos o nosso fluxo de trabalho? Além disso, queremos tornar a execução do script o mais fácil possível e, idealmente, o nemos interactivo possível. Na verdade, qualquer interação humanacom na análise é uma fonte de erros. É por isso que precisamos de testar o nosso código de uma forma completa e sistemática. E esses testes também pecisam de ser executados duma forma não interactiva. Com as ferramentes que usaremos nesta segunda parte, podemos, de facto, configurar o nosso projecto, desde o início, de modo a que seja reprodutível naturalmente. Usando as ferramentas certas e configurando as coisas correctamente, não precisamos realmente de investir mais tempo para tornar o processo reprodutível. O projeto simplesmente será reprodutível porque foi desenhado dessa forma. Praticamente sem nenhum custo adicional!\nOutro problema de apenas registarmos a versão dos pacotes utilizados é que às vezes isso não é suficiente. Isto porque a instalação de pacotes antigos pode ser desafiante por dois motivos:\nPara resolvermos este problema precisamos congelar o próprio ambiente computacional, e para isto usaremos o Docker.\nPor fim, e este é o último nível do iceberg que não faz parte deste livro, é também necessário tornar a construção do ambiente computacional reprodutível. O Guix é a ferramenta que permite fazer exatamente isso. No entanto, esse é um tópico muito profundo por si só, e há soluções alternativas para conseguir isso usando o Docker, por isso não falaremos sobre o Guix.\nNos próximos capítulos, vamos percorrer o iceberg. Primeiro, vamos reescrever nosso projeto usando programação funcional e letrada. O nosso projecto não será composto por dois scripts mas sim por dois ficheiros de Rmarkdown que podemos executar para se sejam legíveis e partilháveis com quaisquer partes interessadas.\nEm seguida, transformaremos esses dois ficheiros .Rmd num pacote. Isso será feito usando o pacote de Sébastien Rochette {fusen}1. O {fusen} facilita muito a passagem dos nossos ficheiros .Rmd para um pacote, usando o que Sébastien chamou de método Rmarkdown first.\nUma vez que tenhamos o nosso pacote, podemos usar o {testthat} para fazermos testes unitários, e funções do R base para fazermos programação assertiva. Nesta fase o nosso código deverá estar bem documentado, fácil de partilhar, e amplamente testado.\nComeçámos com scripts muito simples, que é como a maioria das análises é feita. Em seguida, usando programação funcional e letrada, esses scripts foram transformados em ficheiros RMarkdown e, nesta parte do livro, esses ficheiros RMarkdown serão transformados em um pacote. É claro que, no futuro, podemos começar imediatamente a partir dos ficheiros RMarkdown ou do pacote. Devemos começar pelo pacote, pois, como veremos, começar pelo pacote é basicamente o mesmo esforço que começar por um arquivo RMarkdown simples, graças a {fusen}, mas agora temos os benefícios adicionais de usar os recursos de desenvolvimento de pacotes para melhorar a nossa análise.\nDepois de termos o pacote podemos criar um verdadeiro pipeline usando {targets}, um pacote incrivelmente útil para automatizar a compilação.\nChegados a este ponto, podemos finalmente falar em reproducibilidade mais concretamente. O motivo porque demoramos tanto tempo a chegar aqui é que para tornamos o nosso pipeline reprodutível precisamos de bases sólidas. Não faz sentido tornar reprodutível uma análise que é instável.",
    "crumbs": [
      "Parte 2: Write IT Down"
    ]
  },
  {
    "objectID": "part2_intro.html#o-iceberg-da-reproducibilidade",
    "href": "part2_intro.html#o-iceberg-da-reproducibilidade",
    "title": "Parte 2: Write IT Down",
    "section": "",
    "text": "Pacotes antigos podem precisar de versões antigas do R, e a isntalação de versões antigas do R pode ser complidaca (dependendo do sistema operacional que tivermos);\nOs pacotes mais antigos podem precisar de ser compilados e, consequentemente, dependem de versões mais antigas de bibliotecas de desenvolvimentonecessárias paar a compilação.",
    "crumbs": [
      "Parte 2: Write IT Down"
    ]
  },
  {
    "objectID": "part2_intro.html#footnotes",
    "href": "part2_intro.html#footnotes",
    "title": "Parte 2: Write IT Down",
    "section": "",
    "text": "https://thinkr-open.github.io/fusen/↩︎",
    "crumbs": [
      "Parte 2: Write IT Down"
    ]
  },
  {
    "objectID": "repro_intro.html",
    "href": "repro_intro.html",
    "title": "10  Reproducibilidade básica: congelar pacotes",
    "section": "",
    "text": "10.1 Gravar as versões dos pacotes com {renv}\nAgora que já usamos a programação funcional e letrada, temos de começar a pensar sobre a insfraestrutura que rodeia o nosso código. Ou seja:\nO {renv} permite-nos criar ambientes reprodutíveis, tratando do segundo ponto. Isto é, permite-nos registar os pacotes que foram usados num determinado projecto. Este registo é um ficheiro designado renv.lock que aparece na raíz do nosso projecto quando executamos o {renv}. Podemos usar o {renv} depois de fazermos a análise (como neste caso), ou imediatamente depois de iniciarmos o nosso projecto. Podemos atualizar o renv.lock à medida que formos adicionando ou removendo pacotes à nossa análise. O ficheiro renv.lock pode ser usado para restaurar exatamente a mesma livraria de pacotes usada na nossa análise, noutro computador, ou no nosso, num futuro.\nIsto funciona porque o {renv} faz mais do que manter uma lista dos pacotes usados e das suas versões, no ficheiro renv.lock. Na verdade é criada uma livraria por projecto que fica completamente isolada da livraria principal do R que temos na nossa máquina, mas também de outras livrarias que possamos ter criado com o {renv}. Para nos poupar tempo, ao criamos a livraria {renv}, os pacotes são copiados da livraria principal em vez de serem baixados e re-instalados (se o pacote usado já está instalado na nossa livraria principal).\nComeçamos então por instalar pacote {renv}:\ninstall.packages(\"renv\")\na partir da pasta onde temos os nossos ficheiros Rmd que fizemos no capítulo anterior:\ne depois de garantirmos que as últimas alterações já estão em Github (se não estiverem fazemos o commit e o push para o branch rmd), criamos um novo branch designado como renv porque vamos experimentar uma nova funcionalidade, ou seja:\ne agora trabalharemos neste novo branch como habitualmente, garantindo que antes de fazermos push estamos no branch renv:\nAgora apenas temos de reiniciar a sessão do R e executar o comando:\nrenv::init()\ndeveremos ver qualquer coisa como:\n* Initializing project ...\n* Discovering package dependencies ... Done!\n* Copying packages into the cache ... [76/76] Done!\nThe following package(s) will be updated in the lockfile:\n\n# CRAN ===============================\n***and then a long list of packages***\n\nThe version of R recorded in the lockfile will be updated:\n- R              [*] -&gt; [4.2.2]\n\n* Lockfile written to 'path/to/housing/renv.lock'.\n* Project 'path/to/housing' loaded. [renv 0.16.0]\n* renv activated -- please restart the R session.\nvamos espreitar os ficheiros que criamos:\nComo podemos ver, temos dois novos ficheiros e uma nova pasta. Os novos ficheiros são o já referido renv.lock e o .Rprofile. A nova pasta tem o nome renv. O renv.lock é o ficheiro que lista os pacotes usados na nossa análise. O ficheiro .Rprofile tem os ficheiros que são lidos pelo R automaticamente no arranque. Devemos ter um ficheiro de sistema que é lido pelo R no arranque, mas se o R detecta um ficheiro .Rprofile na directoria em que é iniciado, então é este ficheiro que é lido. Vejamos o conteúdo deste ficheiro.\nO ficheiro tem apenas uma linha:\nO script activate.R na pasta renv foi criado pelo renv::init(). Vejamos todo o conteúdo desta pasta:\nNa pasta renv, temos outra pasta library: esta é a pasta que tem a livraria isolada para o nosso projecto. Não queremos fazer backup a esta pasta, nem a queremos passar para o Github, pois tende a crescer muito rapidamente. Podemos ver também que temos o ficheiro .gitignore que contém caminhos para outros ficheiros e pastas às quais não queremos fazer backup e que devem ser ignorados pelo Git. Se abrirmos o .gitignore podemos ver que temos lá a pasta library/ listada para ser ignorada. Podemos ter tantos ficheiros .gitignore quantos quiseremos mas se tivermos um na raíz do projecto este é aplicado a todo o projecto.\nPor exemplo, se estivermos a trabalhar com dados sensíveis, podemos criar um ficheiro .gitignore no directório do nosso projecto e listar a pasta que tem os dados sensíveis. Podemos criar este ficheiro com um simples editor de texto e para este exemplo apenas escrevemos:\nAssim, garantimos que o conteúdo desta pasta será ignorado pelo Git e não passará para o Github.\nVamos reiniciar uma nova sessão do R, no directório do nosso projecto; deveremos ver a seguinte mensagem de início:\n* Project 'path/to/housing' loaded. [renv 0.16.0]\nIsto significa que esta sessão do R usará os pacotes instalados na livraria isolada que acabamos de criar. Vejamos o ficheiro renv.lock:\nO renv.lock é um ficheiro JSON que lista todos os pacotes, bem como as suas dependências, usados neste projecto. No início do ficheiro está a versão R usada para gerar este renv.lock. É importante relembrar que quando usamos o {renv} para recuperar uma livraria dum projecto numa nova máquina, a versão do R não será restaurada. Ou seja, no futuro poderemos restaurar este projecto com versões antigas de pacotes que usaremos numa versão mais nova do R, o que pode gerarn problemas.\nE é isto… Geramos um ficheiro renv.lock o que significa que nós no futuro ou outra pessoa poderá usar a livraria que usamos neste projecto. Para isto apenas é necessário que a outra pessoa (ou nós no futuro) instale o {renv} e use o ficheiro renv.lock que criamos, para restaurar a livraria. Vejamos como é que isto funciona, clonando o repositório Github deste link (que por sua vez é um fork daqui1))\nDeveremos ver a pasta tagets-minimal no nosso computador e depois de iniciarmos uma sessão do R nesta pasta, temos de executar o comando:\nrenv::restore()\nSomos então questionados sobre a activação deste projecto:\nThis project has not yet been activated.\nActivating this project will ensure the project library\nis used during restore.\nPlease see `?renv::activate` for more details.\n\nWould you like to activate this project before restore? [Y/n]:\nteclamos em Y e deveremos ver a lista de pacotes que precisam de ser instalados. Seremos questionados mais uma vez se queremos continuar e teclando y poderemos ver os pacotes a serem instalados. Se repararmos nos links, veremos que muitos são baixados do arquivo do CRAN, por exemplo:\nPodemos ver a palavra Archive no url. Isto acontece porque a versão usada do {vroom} neste projecto não é a mais recente.\nAinda assim, há a possibilidade de fazermos renv::restore() e a instalação dos pacotes falhar. Vejamos o exemplo em que tentamos restaurar uma livraria em dois computadores diferentes, um Windows e um Linux. No primeiro caso o renv::restore() falhou e no segundo foi bem sucedido. Isto aconteceu porque a compilação do pacote {dplyr} falhou no caso do Windows provavelmente por não ter a versão correcta do Rtools instalada. Ou seja, se no ficheiro renv.lock estiver registado a versão 4.1.0 do R mas depois quisermos restaurar a livraria num computador com a versão 4.2.2, os pacotes serão compilados com o Rtools 4.2 e não com o Rtools 4.0 (que também compila livrarias para o R 4.1). Assim, neste caso para não termos problemas em restaurar a livraria no novo Windows teriamos de instalar primeiro as versões correctas do R e do Rtools.\nIsto não quer dizer que o {renv} é inútil, pelo menos serve para garantir que atualizações de livrarias não interferem nos nossos projectos anteriores. Ou seja, podemos trabalhar em vários projectos simultaneamente e termos a certeza de que se atualizamos a nossa livraria (por exemplo para podermos usar uma nova função de um determinado pacote), isto só afecatará a livraria em que este pacote foi atualizado e não as outras livrarias dos outros projectos. Quando trabalhamos apenas com uma livraria para todo o sistema, a atualização de um pacote pode fazer falhas noutros projectos, quer seja porque atualizamos também outro pacote inadevertidamente ou porque uma qualquer função que também foi atualizada neste processo mudou de nome ou passou a usar parametros diferentes. Podemos evitar este tipo de problemas se usarmos sempre uma livraria por projecto com o {renv}.\nPara além desta característica, o ficheiro renv.lock também fornece uma impressão digital muito útil para o Docker e que abordaremos num próximo capítulo. Resumidamente, como o renv.lock regista a versão do R utilizado, podemos logo começar com uma imagem Docker que contenha a versão correcta do R e a partir daqui já não deveremos ter problemas ao fazermos o renv::restore().\nPodemos então usar o {renv} em duas situações:",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducibilidade básica: congelar pacotes</span>"
    ]
  },
  {
    "objectID": "repro_intro.html#gravar-as-versões-dos-pacotes-com-renv",
    "href": "repro_intro.html#gravar-as-versões-dos-pacotes-com-renv",
    "title": "10  Reproducibilidade básica: congelar pacotes",
    "section": "",
    "text": "a versão do R;\nos pacotes usados para a análise;\ne todo o ambiente computacional, incluindo o próprio hardware.\n\n\n\n\n\n\n\nsave_data.Rmd, o script que faz o download e prepara os dados;\nanalyse_data.Rmd, o script que faz a análise dos dados.\n\n\nowner@localhost ➤ git checkout -b renv\n\nowner@localhost ➤ git add .\nowner@localhost ➤ git commit -m \"some changes\"\nowner@localhost ➤ git push origin renv\n\n\n\n\n\nowner@localhost ➤ ls -la\ntotal 1070\ndrwxr-xr-x 1 owner Domain Users     0 Feb 27 12:44 .\ndrwxr-xr-x 1 owner Domain Users     0 Feb 27 12:35 ..\n-rw-r--r-- 1 owner Domain Users    27 Feb 27 12:44 .Rprofile\ndrwxr-xr-x 1 owner Domain Users     0 Feb 27 12:40 .git\n-rw-r--r-- 1 owner Domain Users   306 Feb 27 12:35 README.md\n-rw-r--r-- 1 owner Domain Users  2398 Feb 27 12:38 analyse_data.Rmd\ndrwxr-xr-x 1 owner Domain Users     0 Feb 27 12:44 renv\n-rw-r--r-- 1 owner Domain Users 20502 Feb 27 12:44 renv.lock\n-rw-r--r-- 1 owner Domain Users  6378 Feb 27 12:38 save_data.Rmd\n\nowner@localhost ➤ cat .Rprofile\n\nsource(\"renv/activate.R\")\n\nowner@localhost ➤ ls -la renv\ntotal 107\ndrwxr-xr-x 1 owner Domain Users     0 Feb 27 12:44 .\ndrwxr-xr-x 1 owner Domain Users     0 Feb 27 12:35 ..\n-rw-r--r-- 1 owner Domain Users    27 Feb 27 12:44 activate.R\ndrwxr-xr-x 1 owner Domain Users     0 Feb 27 12:40 .gitignore\ndrwxr-xr-x 1 owner Domain Users     0 Feb 27 12:40 library\n-rw-r--r-- 1 owner Domain Users  6378 Feb 27 12:38 settings.dcf\n\n\ndatasets/\n\n\n\n\nowner@localhost ➤ cat renv.lock\n{\n\"R\": {\n  \"Version\": \"4.2.2\",\n  \"Repositories\": [\n  {\n   \"Name\": \"CRAN\",\n   \"URL\": \"https://packagemanager.rstudio.com/ all/latest\"\n  }\n  ]\n},\n\"Packages\": {\n  \"MASS\": {\n    \"Package\": \"MASS\",\n    \"Version\": \"7.3-58.1\",\n    \"Source\": \"Repository\",\n    \"Repository\": \"CRAN\",\n    \"Hash\": \"762e1804143a332333c054759f89a706\",\n    \"Requirements\": []\n  },\n  \"Matrix\": {\n    \"Package\": \"Matrix\",\n    \"Version\": \"1.5-1\",\n    \"Source\": \"Repository\",\n    \"Repository\": \"CRAN\",\n    \"Hash\": \"539dc0c0c05636812f1080f473d2c177\",\n    \"Requirements\": [\n      \"lattice\"\n    ]\n\n    ***and many more packages***\n\n\nowner@localhost ➤ git clone git@github.com:b-rodrigues/targets-minimal.git\n\n\n\n\n\nRetrieving 'https://cloud.r-project.org/src/contrib/Archive /vroom/vroom_1.5.5.tar.gz'\n\n\n\n\n\n\nConcluimos o nosso projecto e apenas queremos registar os pacotes usados. Basta então executar renv::init() no final do projecto, fazemos commit e o push do ficheiro renv.lock para o Github.\nUsamos o {renv} logo ao iniciarmos o nosso projecto para assim isolarmos uma livraria para o projecto e evitarmos interferências de outras livrarias do R.\n\n\n10.1.1 Uso corrente do {renv}\nNo caso de querermos usar o {renv} logo quando iniciamos um novo projecto, podemos seguir este exemplo. Começamos numa nova directoria e adicionamos um template .Rmd como este:\n---\ntitle: \"My new project\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nlibrary(dplyr)\n``\n\n## Overview\n\n## Analysis\nAntes de continuarmos, verificamos que compila num ficheiro HTML, na nossa directoria, executando:\n\nrmarkdown::render(\"test.Rmd\")\n\nNo chunk de setup definimos os pacotes que vamos usar e depois de reiniciarmos a sessão do R na mesma directoria podemos executar renv::init(). Devemos a gora ver as questões que já referias em cima e por fim um ficheiro renv.lock (com o pacote {dplyr} e as suas dependências).\nAgora se adicionarmos a linha libraray(ggplot2) ao chunk de setup gravarmos o nosso .Rmd e voltarmos a compilar rmarkdown::render(\"test.Rmd) vamos ter a seguinte mensagem de erro:\nQuitting from lines 7-9 (my_new_project.Rmd)\nError in library(ggplot2) : there is no package called 'ggplot2'\nUma vez que temos o {renv} ativado e consequentemente temos uma livraria específica para este projecto temos de instalar o pacote {ggplot2} antes de o usarmos (ainda que já possa estar instalado na livraria do nosso sistema). Para o instalarmos apenas temos de reiniciar a sessão do R e executamos install.packages(\"ggplot2\"). Se a versão disponível na livraria do nosso sistema for a mais actual então apenas será copiada para a livraria do nosso projecto, caso contrário será instalada a versão mais actual que estiver no CRAN. Podemos agora atualizar o ficheiro renv.lock com o comando renv::snapshot(). Lista dos pacotes a serem registados no renv.lock:\n\n**list of many packages over here**\n\nDo you want to proceed? [y/N]: \n* Lockfile written to 'path/to/my_new_project/renv.lock'.\n\nSe abrirmos o ficheiro renv.lock e procurarmos por \"ggplot2\" veremos esta pacote listado junto com as suas dependências. Esta versão do {ggplot2} é única para o nosso projecto, podemos usar outras versões do {ggplot2} em outros projectos sem que tenham interferência neste. Se por qualuer motivo não quisermos usar a versão mais recente de um qualquer pacote no nosso projecto podemos instalar uma outra versão, ainda graças ao {renv}. Por exemplo, para instalarmos uma versão antiga do {AER}, apenas temos de executar:\n\nrenv::install(\"AER@1.0-0\") # this is a version from August 2008\n\nAinda assim a instalação de versões antigas de pacotes pode dar erros e veremos em próximos capítulos como contornar estes potenciais erros.\nSempre que instalamos mais pacotes necessários para o nosso projecto, temos de executar renv::snapshot(), para adicionarmos estes pacotes ao renv.lock. No final do nosso projecto devemos sempre executar renv::snapshot() para garantirmos que não nos esquecemos de nenhum pacote. Depois é só fazer o devido commit e o push para termos o backup do ficheiro renv.lock.\n\n\n10.1.2 Colaborar com o {renv}\nO {renv} tabém pode ser muito útil em trabalho colaborativo. Ao iniciarmos o projecto geramos o ficheiro renv.lock e quando o projecto for clonado por outros colaboradores já terá as versões correctas dos pacotes a serem usados. No entanto todos os colaboradores devem garantir que trabalham com a mesma versão do R para evitarem problemas de maior. Fica aqui uma vinheta que explica isto mesmo.\n\n\n10.1.3 Atalhos do {renv}\nPor muito útil que seja o {renv} há sempre alguns atalhos que importa sublinhar. Devemos perceber o que o {renv} faz e também o que não faz. E porque é que o {renv} apenas não é suficiente para garantirmos a reproducibilidade do nosso projecto.\nO primeiro problema (como já referimos) reside no facto do renv.lock apenas registar a versão do R que é usada e não a restaura quando usamos o renv::restore(). Temos que ser nós próprios a instalarmos a versão correcta do R. No Windows isto não é difícil, o problema é quando temos a necessidade de usar versões diferentes do R em projectos diferentes, o que pode ser confuso.\nO pacote {rig} pode facilitar a instalação e a troca entre versões do R tal como descrito aqui2. Mas a forma mais segura deverá ser o Docker que abordaremos mais à frente.\nOutro problema do {renv} é que a instalção de versões antigas de pacotes pode ser problemática. Por exemplo, se precisarmos de especificidades do sistema que nos estejam limitadas por falta de permissões de administrador, para instalarmos versões mais antigas, pomos em causa a reproducibilidade do nosso projecto. Mais uma vez o Docker pode resolver este problema. No futuro apenas precisariamos de correr um contentor Docker que é muito mais simples do que instalar algumas versões antigas de pacotes.\nRegistemos também que no desenvolvimento do nosso projecto podemos ter a necessidade de atualizarmos alguns pacotes para melhorarmos a qualidade dos nossos scripts. Podemos fazer isto com update.packages() e depois com renv::snapshot() para gerarmos um novo ficheiro renv.lock. Mas como podemos ter a certeza que estas atualizações não deitam a perder outras partes do nosso código? Transformar a nossa análise num pacote pode ajudar-nos a garantir que não temos quebras pois será mais fácil de definirmos testes para o nosso código. Se os testes indicarem que algo correu mal, apenas temos de recuperar o ficheiro renv.lock anterior com a ajuda do Git e voltamos a restaurar a livraria antiga.\nPodemos usar o {renv} junto com o Docker para garantirmos a reproducilbilidade do nosso projecto, sendo que o ficheiro renv.lock nos dá a impressão digital para construirmos a imagem Docker para uma reproducibilidade a longo prazo.\nMas e se não tivermos um ficheiro renv.lock, como podemos executar uma análise antiga?",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducibilidade básica: congelar pacotes</span>"
    ]
  },
  {
    "objectID": "repro_intro.html#o-arqueologista",
    "href": "repro_intro.html#o-arqueologista",
    "title": "10  Reproducibilidade básica: congelar pacotes",
    "section": "10.2 O aRqueologista",
    "text": "10.2 O aRqueologista\nImaginemos que temos de executar um script antigo para o qual não temos um ficheiro renv.lock que nos permita saber que versões de R e dos pacotes temos de instalar. Ainda assim pode haver uma solução (que não seja executar tudo e rezar para que não dê erros) mas temos de saber pelo menos em que altura é que o script foi escrito. Digamos que o script foi escrito em 2017, por volta de outubro. Com esta informação, podemos usar os pacotes {rang} ou {groundhog} para fazermos o download dos pacotes de outubro de 2017 para uma livraria em separado e depois correremos o script.\nA utilização do {rang} está documentada num prepint3 (Chan and Schoch 2023) e no seu repositório4 no Github.\nA outra opção é usar o {groundhog} (website5). Esta opção já não é nova e é de fácil utilização. Suponhamos que temos um script de outubro de 2017 como este:\n\nlibrary(purrr)\nlibrary(ggplot2)\n\ndata(mtcars)\n\nmyplot &lt;- ggplot(mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\nggsave(\"/home/project/output/myplot.pdf\", myplot)\n\nSe quisermos executar este script com versões do {purrr} e {ggplot2} que eram atuais em outubro de 2017, podemos fazê-lo apenas alterando library() por:\n\ngroundhog::groundhog.library(\"\n    library(purrr)\n    library(ggplot2)\",\n    \"2017-10-04\"\n    )\n\ndata(mtcars)\n\nmyplot &lt;- ggplot(mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\nggsave(\"/home/project/output/myplot.pdf\", myplot)\n\nmas vamos ter a seguinte mensagem:\n-----------------------------------------------\n|IMPORTANT.\n|    Groundhog says: you are using R-4.2.2, but the version of R current\n|    for the entered date, '2017-10-04', is R-3.4.x. It is recommended\n|    that you either keep this date and switch to that version of R, or\n|    you keep the version of R you are using but switch the date to\n|    between '2022-04-22' and '2023-01-08'.\n|\n|    You may bypass this R-version check by adding:\n|    `tolerate.R.version='4.2.2'`as an option in your groundhog.library()\n|    call. Please type 'OK' to confirm you have read this message.\n|   &gt;ok\nO {grounghog} sugere-nos que devemos mudar para a versão do R que era usada na altura que o script foi escrito. Se quisermos ignorar a mensagem de aviso, podemos adicionar o parâmetro tolerate.R.version = '4.2.2', e talvez consigamos que o script corra na mesma:\n\ngroundhog.library(\"\n    library(purrr)\n    library(ggplot2)\",\n    \"2017-10-04\",\n    tolerate.R.version = \"4.2.2\")\n\ndata(mtcars)\n\nmyplot &lt;- ggplot(mtcars) +\n  geom_line(aes(y = hp, x = mpg))\n\nggsave(\"/home/project/output/myplot.pdf\", myplot)\n\nMas tal como o {renv}, com o {rang} a instalação dos pacotes pode falhar e pelas mesmas razões já referidas.\nDe novo, a solução é tratarmos da peça no puzzle da reproducibilidade que é o próprio ambiente computacional.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducibilidade básica: congelar pacotes</span>"
    ]
  },
  {
    "objectID": "repro_intro.html#conclusão",
    "href": "repro_intro.html#conclusão",
    "title": "10  Reproducibilidade básica: congelar pacotes",
    "section": "10.3 Conclusão",
    "text": "10.3 Conclusão\nNeste capítulo tivemos a primeira amostra da reproducibilidade. As ferramentas aqui apresentadas, embora sejam muito úteis, não são suficientes se quisermos o nosso projecto verdadeiramente reprodutível, especialmente a longo prazo. Há muitas coisas que podem correr mal quando pretendemos instalar versões antigas de pacotes, pelo que temos de arranjar maneiras de não o fazermos. É aqui que o Docker pode ser útil. Mas antes disso temos de voltar ao nosso projecto e transformar a nossa análise num pacote. E como poderemos ver, isto será fácil, uma vez que já temos 95% do trabalho feito.\n\n\n\n\nChan, Chung-hong, and David Schoch. 2023. “RANG: Reconstructing Reproducible r Computational Environments.” arXiv.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducibilidade básica: congelar pacotes</span>"
    ]
  },
  {
    "objectID": "repro_intro.html#footnotes",
    "href": "repro_intro.html#footnotes",
    "title": "10  Reproducibilidade básica: congelar pacotes",
    "section": "",
    "text": "https://is.gd/AAnByB↩︎\nhttps://is.gd/dvH2Sj↩︎\nhttps://arxiv.org/abs/2303.04758↩︎\nhttps://is.gd/sQu7NV↩︎\nhttps://groundhogr.com↩︎",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reproducibilidade básica: congelar pacotes</span>"
    ]
  },
  {
    "objectID": "targets.html",
    "href": "targets.html",
    "title": "13  Automatização com {targets}",
    "section": "",
    "text": "13.1 Introdução\nOs workflows que têm por base vários scripts podem ser muito problemáticos. Primeiro os scripts podem e vão ser executados fora de ordem. Podemos mitigar alguns dos problemas que isto pode criar, usando funções puras, mas ainda assim temos de garantir que não executamos os nossos scripts fora de ordem. Mas o que é que isto significa? Bem, suponhamos que alteramos uma função e que agora só queremos voltar a executar as partes do workflow afetadas por esta mudança.Para isto temos de saber de memória quais são as partes dos scripts que são afetadas e quais não. O que pode ser bem complicado, principalmente com pipelines grandes. Pelo que iremos executar apenas algumas partes, esperando que não precisemos de voltar a executar tudo de novo.\nOutro problema é que os pipelines em scripts são difíceis de ler e de compreender. Para mitigarmos isto podemos escrever muitos comentários mas temos o problema de também fazer a manutenção desses comentários. Quando o código e os comentários deixam de estar sincronizados começam (ou melhor, continuam) os problemas.\nExecutar diferentes partes do pipeline em paralelo também é muito complicado se o nosso pipeline está definido em scripts. Teremos de partir o script em partes independentes (garantindo que de facto são independentes) e executá-las em paralelo, talvez usando diferentes sessões de R para cada novo script. As boas notícias é que temos usado progamação funcional pelo que o nosso pipeline é um conjunto de funções puras, o que simplifica a execução do pipeline em paralelo.\nMas já devemos suspeitar que os engenheiros de software também se depararam com problemas semelhantes no desenvolvimento de software, e também devemos saber que eles encontraram uma forma de contornar este problema. Que entrem as ferramentas de construção de automatização.\nQuando construímos uma ferramenta para construir automatização, estamos a escrever uma receita que define como o código fonte deve ser ‘cozinhado’ num software (ou, no nosso caso, num report, numa tabela de dados ou num qualquer produto de dados).\nUma ferramenta de automatização deve rastrear:\nTal como muitas outras ferramentas que já vimos neste livro, as ferramentas de construção de automatização permitem-nos não depender da nossa memória. Escrevemos a receita uma vez e podemos voltar a preocupar-nos apenas com o código do nosso projecto. Não nos devemos preocupar com o próprio pipeline, nem pensar na melhor maneira de o executar. Deixemos o computador preocupar-se com isso, de certeza que é melhor do que nós nessas tarefas.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#introdução",
    "href": "targets.html#introdução",
    "title": "13  Automatização com {targets}",
    "section": "",
    "text": "qualquer alteração em qualquer parte do código. Apenas os outputs que são afetados pelas alterações que fizemos devem ser re-executados (bem como as suas dependências);\nqualquer alteração em qualquer dos ficheiros rastreados. Por exemplo, se um ficheiro de dados é atualizado periodicamente, podemos rastrear este ficheiro e a ferramenta de construção de automatização apenas executa as partes do pipeline que são afetadas por esta atualização de dados;\nquais as partes que podem ser executadas em paralelo com segurança (com a opção de executar o pipeline em multiplos cores CPU).",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#inicialização-rápida-com-o-targets",
    "href": "targets.html#inicialização-rápida-com-o-targets",
    "title": "13  Automatização com {targets}",
    "section": "13.2 Inicialização rápida com o {targets}",
    "text": "13.2 Inicialização rápida com o {targets}\nAntes de mais: para sabermos tudo sobre o pacote {targets} devemos ler o excelente manual do {targets}1. Está tudo lá. O que iremos ver aqui é apenas uma rápida introdução a alguns dos pontos principais que precisamos conhecer para começar.\nVamos criar uma nova pasta chamada targets_intro/, e começar uma nova sessão do R. Para já podemos ignorar o {renv}. Mais à frente poderemos ver como o {renv} trabalha com o {targets} para produzir um pipeline que é quase completamente reprodutível. Na nova sessão e dentro da pasta targets_intro/ vamos executar o comando:\n\ntargets::tar_script()\n\nCom isto criaremos um ficheiro template _targets.R nesta directoria. É neste ficheiro que vamos definir o nosso pipeline. Podemos abri-lo e ver que é definido basicamente em três partes:\n\nprimeiro são definidos os pacotes a usar e as funções de ajuda;\ndepois vêm as opções específicas do pipeline;\ne por fim o próprio pipeline, definido como um conjunto de targets.\n\nVejamos cada uma destas parte individualmente.\n\n13.2.1 A anatomia do _targets.R\nNa primeira parte do pipeline definimos os pacotes que vamos usar, bem como as funções de ajuda que queremos carregar. No template, a primeira linha tem library(targets) seguido pela definição duma função. Há duas coisas que importa sublinhar.\nSe o nosso pipeline precisa do pacote {dplyr} (por exemplo), podemos escrever library(dplyr) logo depois de library(targets). No entanto, é melhor carregarmos os pacotes com a função tar_option_set(packages = \"dplyr\"). Isto porque se executamos o pipeline em paralelo, temos de garantir que os pacotes estão disponíveis para todos os workers (geralmente, um worker por cada core CPU). Se carregarmos os pacotes no início do ficheiro _targets.R, os pacotes apenas estarão disponíveis na sessão original que executa o comando library(...), mas não para as sessões dos workers chamados para a execução em paralelo.\nA ideia é que no início do nosso script apenas lemos o pacote {targets} e ventualmente outros que sejam necessários para executar o próprio pipeline (como veremos mais à frente). Mas os pacotes que são necessários para executarmos funções que são usadas dentro do nosso pipeline devem ser carregados com tar_option_set(packages = \"...\"). Dito de outra forma: no início do script devemos ter pacote para a infaestrutura do pipeline ({targets} e mais algum), mas em {tar_option_set()} pacotes para as funções executadas dentro do pipeline.\nNa segunda parte é onde definimos algumas das opções globais para o pipeline. Como já referimos, é aqui que definimos os pacotes usados dentro do pipeline. Não vamos listar aqui todas as opções porque apenas estariamos a repetir o que está na documentação2. Nesta segunda parte também podemos definir algumas funções que podem ser necessárias para executarmos o nosso pipeline. Por exemplo, podemos definir funções para ler e limpar dados. Se criarmos um package para o nosso projecto, podemos usá-lo e não precisariamos de outras funções. No entanto, às vezes não precisamos de criar um package e apenas fazemos algumas funções que nos ajudam na nossa análise, neste caso podemos defini-las directamente no início do nosso ficheiro _targets.R ou então num ficheiro em separado dentro duma pasta functions/ na mesma directoria do _targets.R. A escolha é de cada um, mas a segunda opção é mais recomendável. No script de exemplo, definimos a seguinte função:\n\nsummarize_data &lt;- function(dataset) {\n  colMeans(dataset)\n}\n\nFinalmente, temos o próprio pipeline:\n\nlist(\n  tar_target(data,\n             data.frame(x = sample.int(100),\n                        y = sample.int(100))),\n\n  tar_target(data_summary,\n             summarize_data(data)) # Call your custom functions.\n)\n\nO pipeline mais não é que uma lista de targets. Definimos um target com a função tar_target() que tem pelo menos dois inputs: o primeiro é o nome do target (sem aspas) e o segundo é a função que gera o target. Logo um target definido como tar_target(y, f(x)) pode ser lido como y &lt;- f(x). O target seguinte pode usar como input o output dum target anterior, pelo que podemos ter qualquer coisa como tar_target(z, f(y)) (tal como no template de exemplo).",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#um-pipeline-é-composto-por-funções-puras",
    "href": "targets.html#um-pipeline-é-composto-por-funções-puras",
    "title": "13  Automatização com {targets}",
    "section": "13.3 Um pipeline é composto por funções puras",
    "text": "13.3 Um pipeline é composto por funções puras\nPodemos executar este pipeline, com o comando:\n\ntargets::tar_make()\n\n• start target data\n• built target data [0.82 seconds]\n• start target data_summary\n• built target data_summary [0.02 seconds]\n• end pipeline [1.71 seconds]\nO pipeline finalizou e agora? Este pipeline fez algumas estatisticas sumárias mas para onde foram? Se fizermos data_summary para tentarmos ver algum resultado temos um erro:\n\ndata_summary\n\nError: object 'data_summary' not found\nO que aconteceu?\nTemos de nos lembrar que queremos que o nosso pipeline seja uma sequência de funções puras. Isto é, o sucesso da execução do nosso pipeline não deve depender de nada que esteja no ambiente global (para além de ler os pacotes da primeira parte do script e das opções definidas em tar_option_set()) e não deve alterar nada fora do seu próprio âmbito. Ou seja, o pipeline não deve alterar nada do ambiente global. É exactamente assim que um pipeline definido com {targets} funciona. Um pipeline definido usando o {targets} será puro e o seu output não garavará nada no ambiente global. Em termos estritos um pipeline não é exactamente puro. Vejamos a pasta que contém o script _targets.R. Temos agora a pasta _targets/. Se virmos dentro desta nova pasta e dentro da pasta objects/, temos dois objectos, data e data_summary. Estes são os outputs do pipeline.\nCada target definido dentro do nosso pipeline é gravado no formato .rds. Este é um formato específico do R que podemos usar para guardar qualquer tipo de objecto. Não importa o quê: um dataframe, um modelo, um ggplot, qualquer objecto que possamos ter em R pode ser gravado em disco com a função saveRDS() e posteriormente ser lido noutra sessão com readRDS(). O {targets} usa estas duas funções para guardar os resultados dos targets do nosso pipeline e também para aceder a eles a partir da pasta _targets/ em vez de os voltar a computar. Devemos ter sempre presente que se usarmos o Git para versionar o nosso código (tal como fazemos aqui) devemos adicionar a pasta _targets/ ao ficheiro .gitignore.\nComo o nosso pipeline é puro nenhum resultado dos targets é gravado no ambiente global, pelo que se quisermos aceder aos seus outputs temos de usar as funções tar_read() ou tar_load(). A diferença é que tar_read() apenas lê o target e imprime-o na consola, enquanto que tar_load() lê o target e carrega-o no ambiente global. No nosso exemplo, para extrairmos o object data_summary usamos tar_load(data_summary):\n\ntar_load(data_summary)\n\ne agora quando escrevemos data_summary, temos:\n\ndata_summary\n\n\n   x    y\n50.5 50.5\n\nPodemos também carregar todos os targets duma vez com tar_load_everything(), para não termos de carregar os targets um a um.\nAntes de continuarmos a ver mais características do {targets} devemos ter presente que um pipeline é composto por funções puras. Logo funções que tenham efeitos colaterais (como as que lêm dados ou imprimem alguma coisa no ecrã) são mais difíceis de manipular. Por exemplo, para fazermos um gráfico com o R base temos de fazer uma série de chamadas de funções com efeitos colaterais. Se abrirmos uma consola e escrevermos plot(mtcars), vamos ver um gráfico. Mas a função plot() não cria qualquer output. Apenas imprime uma imagem no nosso ecrã, o que é um efeito colateral. Podemos verificar isto se tentarmos guardar plot() numa variável:\n\na &lt;- plot(mtcars)\n\nao fazermos isto vemos o gráfico mas quando executamos a variável a, o gráfico não aparece:\n\na\n\n\nNULL\n\nE porque plot() não é uma função pura se a tentarmos usar num pipeline {targets} teremos como resultado NULL, quando tentarmos ler o target que deveria ter o gráfico:\n\nlist(\n  tar_target(data,\n             data.frame(x = sample.int(100),\n                        y = sample.int(100))),\n\n  tar_target(data_summary,\n             summarize_data(data)), # Call your custom functions.\n\n  tar_target(\n    data_plot,\n    plot(data)\n  )\n)\n\nApenas adicionamos um novo target com tar_target() para fazermos um gráfico. Se voltarmos a fazer tar_make() e depois tar_load(data_plot) para carregarmos o novo target, ao escrevermos data_plot apenas é impresso NULL e não vemos nenhum gráfico.\nPodemos dar a volta a este problema usando o ggplot(). Isto porque o output do ggplot() é um objecto do tipo ggplot. Podemos fazer qualquer coisa como a &lt;- ggplot() + ect.. e depois escrevemos a para ver o gráfico. Se fizermos str(a) podemos ver a lista que tem a estrutura do gráfico.\nOutra forma de contornar o problema é gravar o gráfico em disco. Para isto temos de escrever uma nova função, por exemplo:\n\nsave_plot &lt;- function(filename, ...){\n\n  png(filename = filename)\n  plot(...)\n  dev.off()\n\n}\n\nSe definirmos esta função no inicio do script do ficheiro _targets.R, podemos usar esta função em vez de plot() no último target:\n\nsummarize_data &lt;- function(dataset) {\n  colMeans(dataset)\n}\n\nsave_plot &lt;- function(filename, ...){\n  png(filename = filename)\n  plot(...)\n  dev.off()\n\n  filename\n}\n\n# Set target-specific options such as packages.\ntar_option_set(packages = \"dplyr\")\n\n# End this file with a list of target objects.\nlist(\n  tar_target(data,\n             data.frame(x = sample.int(100),\n                        y = sample.int(100))),\n\n  tar_target(data_summary,\n             summarize_data(data)), # Call your custom functions.\n\n  tar_target(\n    data_plot,\n    save_plot(\n      filename = \"my_plot.png\",\n      data),\n    format = \"file\")\n)\n\nDepois de executarmos este pipeline veremos um ficheiro chamado my_plot.png na pasta do pipeline. Se escrevermos tar_load(data_plot) e depois data_plot veremos o argumento filename de save_plot(). Isto porque o target tem de devolver alguma coisa e no caso de funções que gravam ficheiros em disco é recomendável devolver o caminho onde o ficheiro foi gravado. Assim se quisermos usar o ficheiro noutro target, podemos escrever tar_target(x, f(data_plot)). Assim, uma vez que o target data_plot devolve um caminho, podemos escrever f() de forma a qua saiba como usar o caminho. Se escrevessemos tar_target(x, f(\"path/to/my_plot.png\")), então o {targets} não tinha como saber que o target x depende do target data_plot. Não existiria uma dependência entre targets.\nPor fim, no último target tivemos de usar a opção format = file que será aprofundada mais à frente.\nVale a pena realçar que o pacote {ggplot2} tem uma função que permite gravar objectos ggplot no disco ggplot2::ggsave(). Assim poderiamos ter definido dois targets, um para fazer o objecto ggplot e outro para gerar a imagem .png desse objecto.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#tratar-ficheiros",
    "href": "targets.html#tratar-ficheiros",
    "title": "13  Automatização com {targets}",
    "section": "13.4 Tratar ficheiros",
    "text": "13.4 Tratar ficheiros\nNesta secção, vamos ver como o {targets} lida com ficheiros. Primeiro vamos executar as seguintes linhas de código na pasta que tem o script _targets.R que temos estado a usar:\n\ndata(mtcars)\n\nwrite.csv(mtcars,\n          \"mtcars.csv\",\n          row.names = F)\n\nAcabamos de criar o ficheiro mtcars.csv na nossa pasta. Vamos agora usá-lo no nosso pipeline.\nEscrevemos o seguinte pipeline:\n\nlist(\n  tar_target(\n    data_mtcars,\n    read.csv(\"mtcars.csv\")\n  ),\n\n  tar_target(\n    summary_mtcars,\n    summary(data_mtcars)\n  ),\n\n  tar_target(\n    plot_mtcars,\n    save_plot(\n      filename = \"mtcars_plot.png\",\n      data_mtcars),\n    format = \"file\")\n)\n\nPodemos correr o pipeline e no fim teremos um gráfico. O problema é que o ficheiro mtcars.csv não está a ter registo de alterações. Se fizermos uma alteração ao ficheiro, como por exemplo:\n\nwrite.csv(head(mtcars), \"mtcars.csv\", row.names = F)\n\nAo voltarmos a executar o pipeline, as nossas alterações aos dados foram ignoradas:\n✔ skip target data_mtcars\n✔ skip target plot_mtcars\n✔ skip target summary_mtcars\n✔ skip pipeline [0.1 seconds]\nComo podemos ver, o {targets} não está a registar as alerações no ficheiro mtcars.csv. Assim, o pipeline está up-to-date para o {targets} pois para ele não houve alterações.\nVamos voltar a alterar o nosso ficheiro:\n\nwrite.csv(mtcars, \"mtcars.csv\", row.names = F)\n\nAgora alteramos o primeiro target para que as alterações do ficheiro sejam rastreadas. Lembremo-nos que os targets têm de ser funções puras que devolvem alguma coisa. Logo vamos alterar o primeiro target para que devolva o caminho para o ficheiro, usado a opção format = \"file\" em tar_target():\n\npath_data &lt;- function(path){\n  path\n}\n\nlist(\n  tar_target(\n    path_data_mtcars,\n    path_data(\"mtcars.csv\"),\n    format = \"file\"\n  ),\n  tar_target(\n    data_mtcars,\n    read.csv(path_data_mtcars)\n  ),\n  tar_target(\n    summary_mtcars,\n    summary(data_mtcars)\n  ),\n  tar_target(\n    plot_mtcars,\n    save_plot(filename = \"mtcars_plot.png\",\n              data_mtcars),\n    format = \"file\")\n)\n\nO primeiro target poderá ser simplesmente escrito como:\n\ntar_target(\n  path_data_mtcars,\n  \"mtcars.csv\",\n  format = \"file\"\n)\n\nAgora temos o target chamado path_data_mtcars que apenas devolve o caminho para os dados. Mas como usámos a opção format = \"file\", o {targets} já sabe que deve rastrear esse ficheiro. Ou seja, qualquer alteração ao ficheiro será reconhecida e qualquer target que dependa desse input será marcado como estando out-of-date. Os restantes targets permanecem iguais.\nExecutemos o pipeline com tar_make(). Agora, vamos alterar o ficheiro outra vez:\n\nwrite.csv(head(mtcars),\n          \"mtcars.csv\",\n          row.names = F)\n\nSe voltarmos a executar o pipeline com tar_make() podemos ve que o {targets} reconhece que há alterações e executa o pipeline tendo em conta essas alterações.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#o-gráfico-de-dependências",
    "href": "targets.html#o-gráfico-de-dependências",
    "title": "13  Automatização com {targets}",
    "section": "13.5 O gráfico de dependências",
    "text": "13.5 O gráfico de dependências\nComo podemos ver o {targets} faz o rastreamento das alterações aos ficheiros e também às funções que usamos. Qualquer alteração no código de alguma dessas funções fará com que o {targets} identifique quais os targets que estão out-of-date e quais os que devem ser recalculados junto com os que dependam destes. Podemos visualizar isto com a função tar_visnetwork(). Com isto abrimos um gráfico de rede interactivo no nosso browser:\n\n\n\n\n\nEsta imagem é aberta no nosso web-browser.\n\n\n\n\nAqui podemos ver que todos os targets estão up-to-date. Se fizermos uma alteração aos dados de input, é este o resultado:\n\n\n\n\n\nComo os dados de input mudaram, temos de voltar a executar o pipeline.\n\n\n\n\nComo todos os target dependem dos dados de input, temos de volatr a executar todo o pipeline. Vamos voltar a executar tudo com tar_make(), antes de continuarmos.\nPodemos agora adicionar outro target ao nosso pipeline, um que não dependa dos dados de input. O script deverá ficar assim:\n\nlist(\n  tar_target(\n    path_data_mtcars,\n    \"mtcars.csv\",\n    format = \"file\"\n  ),\n  tar_target(\n    data_iris,\n    data(\"iris\")\n  ),\n  tar_target(\n    summary_iris,\n    summary(data_iris)\n  ),\n  tar_target(\n    data_mtcars,\n    read.csv(path_data_mtcars)\n  ),\n  tar_target(\n    summary_mtcars,\n    summary(data_mtcars)\n  ),\n  tar_target(\n    plot_mtcars,\n    save_plot(\n      filename = \"mtcars_plot.png\",\n      data_mtcars),\n    format = \"file\")\n)\n\nAntes de voltarmos a executar o pipeline, vejamos o workflow com tar_visnetwork():\n\n\n\n\n\nVemos claramente que o pipeline tem duas partes completamente independentes.\n\n\n\n\nPodemos ver que há duas partes independentes, bem como duas funções que não estão a ser usadas, path_data() e summ() que poderíamos remover. Depois podemos alterar de novo os dados de input e se corrermos o pipeline com tar_make() seremos bem sucedidos.\nVamos adicionar o último target:\n\ntar_target(\n  list_summaries,\n  list(\n    \"summary_iris\" = summary_iris,\n    \"summary_mtcars\" = summary_mtcars\n  )\n),\n\neste target cria uma lista com dois sumários que calculamos, e ao chamarmos tar_visnetwork():\n\n\n\n\n\nOs dois workflows separados acabam num único output.\n\n\n\n\nFinalmente, voltamos a executar o pipeline uma última vez para obtermos o output final.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#executar-o-pipeline-em-paralelo",
    "href": "targets.html#executar-o-pipeline-em-paralelo",
    "title": "13  Automatização com {targets}",
    "section": "13.6 Executar o pipeline em paralelo",
    "text": "13.6 Executar o pipeline em paralelo\nCom o {targets} podemos executar partes independentes do noso pipeline em paralelo. No exemplo anterior é óbvio quais as partes que são independentes mas quando o pipeline cresce em complexidade pode ser bem difícil encontrar as partes que são independentes.\nVamos então executar o exemplo anterior em paralelo. Mas primeiro vamos criar uma função que demora algum tempo a executar. A função summary() é tão rápida que não vale a pena executarmos duas chamadas desta função em paralelo (na verdade até demoraria mais, como veremos no fim). Vamos então definir a nova função slow_summary():\n\nslow_summary &lt;- function(...){\n  Sys.sleep(30)\n  summary(...)\n}\n\ne vamos substituir summary() por slow_summary(), no nosso pipeline:\n\nlist(\n  tar_target(\n    path_data_mtcars,\n    \"mtcars.csv\",\n    format = \"file\"\n  ),\n  tar_target(\n    data_iris,\n    data(\"iris\")\n  ),\n  tar_target(\n    summary_iris,\n    slow_summary(data_iris)\n  ),\n  tar_target(\n    data_mtcars,\n    read.csv(path_data_mtcars)\n  ),\n  tar_target(\n    summary_mtcars,\n    slow_summary(data_mtcars)\n  ),\n  tar_target(\n    list_summaries,\n    list(\n      \"summary_iris\" = summary_iris,\n      \"summary_mtcars\" = summary_mtcars\n    )\n  ),\n  tar_target(\n    plot_mtcars,\n    save_plot(filename = \"mtcars_plot.png\",\n              data_mtcars),\n    format = \"file\")\n)\n\neste é o aspecto do pipeline, antes de ser executado:\n\n\n\n\n\nÉ usada slow_summary() em vez de summary().\n\n\n\n\npodemos também reparar que removemos as funções não utilizadas path_data() e summ()\nA execução deste pipeline sequencialmente demorará cerca de 1 minuto, uma vez que cada chamada de slow_summary() demora 30 segundos. Para voltarmos a executar o pipeline, fazemos tar_destroy() (e apagamos todos os targets criados) e depois tar_make():\n\ntargets::tar_make()\n\n• start target path_data_mtcars\n• built target path_data_mtcars [0.18 seconds]\n• start target data_iris\n• built target data_iris [0 seconds]\n• start target data_mtcars\n• built target data_mtcars [0 seconds]\n• start target summary_iris\n• built target summary_iris [30.26 seconds]\n• start target plot_mtcars\n• built target plot_mtcars [0.16 seconds]\n• start target summary_mtcars\n• built target summary_mtcars [30.29 seconds]\n• start target list_summaries\n• built target list_summaries [0 seconds]\n• end pipeline [1.019 minutes]\nUma vez que a computação de summary_iris é completamente independente de summary_mtcars, estas computações podem ser feitas em paralelo por dois cores CPU em paralelo. Para isto precisamos de dois novos pacotes, {future} e {future.callr} no início do nosso script. Vamos també precisar de chamar plan(callr) antes do início do pipeline. Vejamos o _targets.R completo:\n\nlibrary(targets)\nlibrary(future)\nlibrary(future.callr)\nplan(callr)\n\n# Sometimes you gotta take your time\nslow_summary &lt;- function(...) {\n  Sys.sleep(30)\n  summary(...)\n}\n\n# Save plot to disk\nsave_plot &lt;- function(filename, ...){\n  png(filename = filename)\n  plot(...)\n  dev.off()\n\n  filename\n}\n\n# Set target-specific options such as packages.\ntar_option_set(packages = \"dplyr\")\n\nlist(\n  tar_target(\n    path_data_mtcars,\n    \"mtcars.csv\",\n    format = \"file\"\n  ),\n  tar_target(\n    data_iris,\n    data(\"iris\")\n  ),\n  tar_target(\n    summary_iris,\n    slow_summary(data_iris)\n  ),\n  tar_target(\n    data_mtcars,\n    read.csv(path_data_mtcars)\n  ),\n  tar_target(\n    summary_mtcars,\n    slow_summary(data_mtcars)\n  ),\n  tar_target(\n    list_summaries,\n    list(\n      \"summary_iris\" = summary_iris,\n      \"summary_mtcars\" = summary_mtcars\n    )\n  ),\n  tar_target(\n    plot_mtcars,\n    save_plot(\n      filename = \"mtcars_plot.png\",\n      data_mtcars),\n    format = \"file\")\n)\n\nPOdemos então executar este pipeline em paralelo usando tar_make_future() (ou sequencialmente com tar_make()). Para construirmos o pipeline do zero, fazemos tar_destroy() e depois tar_make_future():\n# Set workers = 2 to use 2 cpu cores\ntargets::tar_make_future(workers = 2)\n• start target path_data_mtcars\n• start target data_iris\n• built target path_data_mtcars [0.2 seconds]\n• start target data_mtcars\n• built target data_iris [0.22 seconds]\n• start target summary_iris\n• built target data_mtcars [0.2 seconds]\n• start target plot_mtcars\n• built target plot_mtcars [0.35 seconds]\n• start target summary_mtcars\n• built target summary_iris [30.5 seconds]\n• built target summary_mtcars [30.52 seconds]\n• start target list_summaries\n• built target list_summaries [0.21 seconds]\n• end pipeline [38.72 seconds]\nComo podemos ver, foi mais rápido mas não demorou metado do tempo. A razão de ser mais rápido mas não 2x mais rápido deve-se ao facto de haver alguma sobrecarga quando se corre código em paralelo. Novas sessões do R têm de ser espalhadas pelos targets, os dados precisam de ser transferidos e os pacotes precisão de ser carregados nessas novas sessões. Por isso é que só vale a pena paralelizar código que demora algum tempo a ser executado. Se reduzirmos o número de segundos da função slow_summary(...) (por exemplo para 10), executar o código em paralelo pode ser mais lento do que executar o código sequencialmente, devido a essa sobrecarga. Mas se tivermos várias computações que demoram algum tempo, vale a pena defenirmos o setup inicial para a computação paralela. Ou seja, para executarmos o nosso pipeline em paralelo, as sessões extra de workers que são geradas pelo {targets} precisam de saber que pacotes devem usar e é também por isto que é importante carregarmos os pacotes que o pipeline precisa de usar:\n\ntar_option_set(packages = \"dplyr\")",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#o-targets-e-o-rmarkdown-ou-o-quarto",
    "href": "targets.html#o-targets-e-o-rmarkdown-ou-o-quarto",
    "title": "13  Automatização com {targets}",
    "section": "13.7 O {targets} e o RMarkdown (ou o Quarto)",
    "text": "13.7 O {targets} e o RMarkdown (ou o Quarto)\nTambém é possível compilar documentos usando o RMarkdown (ou Quarto) com o {targets}. Para isto definimos um pipeline com os outputs que precisamos no documento e depois definimos o documento como um target a ser também computado. Por exemplo se queremos mostrar uma tabela no documento, definimos um pipeline que tem um target que constrói os dados para a tabela. Fazemos o mesmo para um gráfico ou um modelo estatístico. No ficheiro .Rmd (ou .Qmd), usamos targets::tar_read() para carregarmos os vários objectos que precisamos.\nVejamos o seguinte ficheiro _targets.R:\n\nlibrary(targets)\n\ntar_option_set(packages = c(\"dplyr\", \"ggplot2\"))\n\nlist(\n  tar_target(\n    path_data_mtcars,\n    \"mtcars.csv\",\n    format = \"file\"\n  ),\n  tar_target(\n    data_mtcars,\n    read.csv(path_data_mtcars)\n  ),\n  tar_target(\n    summary_mtcars,\n    summary(data_mtcars)\n  ),\n  tar_target(\n    clean_mtcars,\n    mutate(data_mtcars,\n           am = as.character(am))\n  ),\n  tar_target(\n    plot_mtcars,\n    {ggplot(clean_mtcars) +\n       geom_point(aes(y = mpg,\n                      x = hp,\n                      shape = am))}\n  )\n)\n\nEste pipeline lê o ficheiro .csv anterior e cria um resumo dos dados e um gráfico. Mas não queremos apenas que esses objectos sejam guardados como ficheiros .rds, queremos usá-los para fazer o nosso documento (seja em formato .Rmd ou .Qmd). Para isto precisamos de outro pacote, {tarchtypes}. Este pacote têm muitas funções que nos permitem definir novos tipos de targets (a essas funções chamamos de fábrica de targets). A nova fábrica de targets que precisamos é tarchetypes::tar_render(). Como o nome indica, esta função faz a renderização dum ficheiro .Rmd. Podemos então escrever um ficheiro .Rmd (como em baixo) e guardá-lo junto ao nosso pipeline:\n---\ntitle: \"mtcars is the best data set\"\nauthor: \"mtcars enjoyer\"\ndate: today\n---\n\n## Load the summary\n\n```{r}\ntar_read(summary_mtcars)\n```\nNO nosso ficheiro _targets.R, carregamos o {tarchetypes} no início do ficheiro e adicionamos um novo target no final:\n\nlibrary(targets)\nlibrary(tarchetypes)\n\ntar_option_set(packages = c(\"dplyr\", \"ggplot2\"))\n\nlist(\n  tar_target(\n    path_data_mtcars,\n    \"mtcars.csv\",\n    format = \"file\"\n  ),\n  tar_target(\n    data_mtcars,\n    read.csv(path_data_mtcars)\n  ),\n  tar_target(\n    summary_mtcars,\n    summary(data_mtcars)\n  ),\n  tar_target(\n    clean_mtcars,\n    mutate(data_mtcars,\n           am = as.character(am))\n  ),\n  tar_target(\n    plot_mtcars,\n    {ggplot(clean_mtcars) +\n       geom_point(aes(y = mpg,\n                      x = hp,\n                      shape = am))}\n  ),\n  tar_render(\n    my_doc,\n    \"my_document.Rmd\"\n  )\n)\n\nAo executarmos o nosso pipeline com tar_make() estamos a compilar o ficheiro .Rmd num ficheiro .html que podemos abrir no nosso browser. Mesmo que a nossa intenção seja compilar o documento noutro formado, é avisado fazer a compilação para .html, no desenvolvimento. Assim podemos abrir o ficheiro .html no browser e continuar a trabalhar no ficheiro fonte, sempre que executamos o pipeline com alterações ao documento, apenas temos de refrescar o browser para vermos essas alterações. Se a compilação for, por exemplo, para um documento Word, temos de fechar e voltar a abrir o documento se quisermos ver as alterações, o que pode ser irritante. A outra razão prende-se com o facto do output de ficheiro .html ser um formato de apenas-texto e como tal pode ser rastreado por um sistema de controlo de versões. No entanto, devemos ter presente que os ficheiros .html podem ser demasiado grandes e nesse caso não nos interessa rastreá-los, apenas queremos rastrear a sua fonte (os ficheiros .Rmd).\nSe abrirmos o ficheiro de output, devemos ver algo como isto:\n\n\n\n\n\nEste é o output do nosso pipeline. O nosso primeiro data product!\n\n\n\n\nNo final terá um aspecto mais bonito. Não vale a pena perdermos tempo em fazer coisas bonitas, logo no início. Idealmente, tentamos executar o nosso pipeline com exemplos simples e vamos adicionando funcionalidades. Devemos também procurar receber feddback do conteúdo o mais cedo possível. Seria um desperdício gastarmos tempo em dar bom aspecto a uma coisa que não é o esperado. Vamos adicionar também o ggplot dos dados ao documento.\nBasta acrescentarmos:\n```{r}\ntar_read(plot_mtcars)\n```\nno final do ficheiro .Rmd. Ao voltarmos a executar o pipeline, vamos adicionar o gráfico ao documento. Antes de continuarmos, vamos só relembrar, de novo, a utilidade do {targets} alterando os dados subjacentes. Executemos o código:\n\nwrite.csv(head(mtcars),\n          \"mtcars.csv\",\n          row.names = F)\n\ne voltemos a executar o pipeline. Como alteramos os dados e todos os targets dependem dos dados, o documento é reconstruido por inteiro. Ou seja, no caso de precisarmos de construir um reporte semanal, diário, ou até hora a hora, usando o {targets} o reporte atualizado pode ser construido automaticamente, e os targets que não são impactados pelo update não precisam de ser refeitos. Vamos voltar aos dados iniciais, executando:\n\nwrite.csv(mtcars,\n          \"mtcars.csv\",\n          row.names = F)\n\ne voltamos a refazer o documento, executando o pipeline.\nAgora que verificamos que o pipeline está a funcionar bem, podemos trabalhar no próprio documento, transformando o output numa tabela com bom aspecto com {flextable}. Mas temos um problema: o output de summary() não é um data.frame mas sim um table e o flextable::flextable() espera como input um data.frame. Assim, se chamarmos flextable::flextable() com o output de summary() vamos ter uma mensagem de erro. No entanto, podemos substituir summary() por uma que devolva um data.frame, como é o caso de skimr::skim(); voltemos ao nosso pipeline e toquemos summary() por skim() (depois de adicionarmos os pacotes {skimr} e {flextable}):\nlibrary(targets)\nlibrary(tarchetypes)\n\ntar_option_set(packages = c(\n                 \"dplyr\",\n                 \"flextable\",\n                 \"ggplot2\",\n                 \"skimr\"\n                 )\n               )\n\nlist(\n  tar_target(\n    path_data_mtcars,\n    \"mtcars.csv\",\n    format = \"file\"\n  ),\n  tar_target(\n    data_mtcars,\n    read.csv(path_data_mtcars)\n  ),\n  tar_target(\n    summary_mtcars,\n    skim(data_mtcars)\n  ),\n  tar_target(\n    clean_mtcars,\n    mutate(data_mtcars,\n           am = as.character(am))\n  ),\n  tar_target(\n    plot_mtcars,\n    {ggplot(clean_mtcars) +\n       geom_point(aes(y = mpg,\n                      x = hp,\n                      shape = am))}\n  ),\n  tar_render(\n    my_doc,\n    \"my_document.Rmd\"\n  )\n)\nNo ficheiro .Rmd podemos então usar o output de tar_read(summary_mtcars) com flextable()\n## Load the summary\n\n```{r}\ntar_read(summary_mtcars) %&gt;%\n  flextable()\n```\nSe executarmos o pipeline e voltarmos a ver o output, veremos uma bela tabela com muitas estatísticas sumárias. Uma vez que o output de skim() é um data.frame, podemos manter apenas as estatísticas que nos interessam com dplyr::select() para as colunas que precisamos:\n## Load the summary\n\n``{r}\ntar_read(summary_mtcars) %&gt;%\n  select(Variable = skim_variable,\n         Mean = numeric.mean,\n         SD = numeric.sd,\n         Histogram = numeric.hist) %&gt;%\n  flextable()\n``\nse quisermos esconder o código R no output do documento, apenas temos de usar knitr::opts_chunk$set(echo = F) no ficheiro .Rmd, ou se quisermos esconder o código para chunks individualmente, usamos echo = FALSE no cabeçalho dos chunks. O ficheiro final .Rmd será:\n---\ntitle: \"mtcars is the best data set\"\nauthor: \"mtcars enjoyer\"\ndate: today\n---\n\n```{r, include = FALSE}\n# Hides all source code\nknitr::opts_chunk$set(echo = F)\n```\n\n## Load the summary statistics\n\nI really like to see the distribution of the\nvariables as a cell of a table:\n\n```{r}\ntar_read(summary_mtcars) %&gt;%\n  select(Variable = skim_variable,\n         Mean = numeric.mean,\n         SD = numeric.sd,\n         Histogram = numeric.hist) %&gt;%\n  flextable() %&gt;%\n  set_caption(\"Summary statistics for mtcars\")\n```\n\n## Graphics\n\nThe plot below is really nice, just look at it:\n\n```{r, fig.cap = \"Scatterplot of `mpg` and `hp` by type of transmission\"}\ntar_read(plot_mtcars) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\nComo podemos ver, ao usarmos tar_read(), temos o objecto como se tivesse sido gerado no próprio ficheiro .Rmd, e podemos ir adicionando-lhe coisas (como mudar o tema do ggplot). Quando estivermos satisfeitos com o conteúdo, podemos adicionar output: word_document ao cabeçalho yaml para gerarmos um documento Word.\nPor usarmos o {targets} para compilar documentos RMarkdown e porque a computação dos objectos é tratada pelo {targets}, a compilação do documento é muito rápida. Apenas precisa de carregar targets que já foram computados. Isto também significa que beneficiamos doutra vantagem do {targets}, apenas os {targets} out-of-date são re-computados e essa computação pode ser feita em paralelo. Sem o {targets} na compilação dum RMarkdown serão sempre recomputados todos os objectos e sempre sequencialmente.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#rescrever-o-nosso-projecto-como-um-pipeline",
    "href": "targets.html#rescrever-o-nosso-projecto-como-um-pipeline",
    "title": "13  Automatização com {targets}",
    "section": "13.8 Rescrever o nosso projecto como um pipeline",
    "text": "13.8 Rescrever o nosso projecto como um pipeline\nPodemos voltar agora tornar o nosso pequeno projecto num pipeline reprodutível. Voltemos para a pasta do nosso projecto e em particular para o branch fusen. Foi neste branch que usamos o {fusen} para transformarmos o nosso .Rmd num pacote. Este pacote tem funções para atualizarmos os dados. Mas lembremo-nos que escrevemos a análise noutro .Rmd que não inflacionámos, analyse_data.Rmd. Vamos então escrever o pipeline {targets} que irá usar o pacote inflacionado e computar todos os targets para a análise. O primeiro passo é criar um novo branch a partir do branch rmd, pois isto facilitar-nos-á o trabalho:\n#switch to the rmd branch\nowner@localhost ➤ git checkout rmd\n\n#create and switch to the new branch called pipeline\nowner@localhost ➤ git checkout -b pipeline \nComeçemos por apagar o ficheiro save_data.Rmd que já não precisamos pois já temos tudo disponível a partir do pacote que desenvolvemos.\nowner@localhost ➤ rm save_data.Rmd\nVamos iniciar uma nova sessão do R e instalar o nosso pacote {housing}. Para garantirmos que todos temos acesso à mesma versão, podemos executar:\n\nremotes::install_github(\"rap4all/housing@fusen\",\n                        ref = \"1c86095\")\n\nAssim podemos instalar o pacote a partir do repositório Github, e mais especificamente do branch {fusen} no commit 1c86095 (podemos ter de instalar o pacote {remotes}, primeiro). Com o pacote instalado podemos começar a construir o pipeline. Na mesma sessão do R, chamamos tar_script() para termos um template no ficheiro _targets.R:\n\ntargets::tar_script()\n\nDevemos ter três ficheiros: README.md, _targets.R e analyse_data.Rmd. Vamos alterar o ficheiro analyse_data.Rmd para que leia targets que foram pré-computados em vez de fazer a computação dentro do ficheiro analyse_data.Rmd quando é compilado.\nPrimeiro, precisamos de carregar os dados. Os nossos conjuntos de dados fazem parte do nosso pacote pelo que apenas precisamos de usar data(commune_level_data) e data(country_level_data). Mas, como já referimos, o {targets} apenas gosta de funções puras e data() não é pura! Vejamos o que acontece quando fazemos data('mtcars'). No RStudio isto é mais fácil de ver: iniciamos uma nova sessão, chamamos data(mtcars) e podemos ver no painel Enviroment:\n\n\n\n\n\nO que está descrito como ‘mtcars’ ainda não é um ‘data.frame’\n\n\n\n\nNeste ponto, mtcars é apenas uma Promise. Só se interagirmos com ele é que a Promise passa a ser um data.frame. Ou seja, data() devolve uma promessa, mas podemos guardá-la numa variável. Experimentemos:\n\nx &lt;- data(mtcars)\n\nVejamos que x contém a string “mtcars” da classe character. Então, data() devolve uma promessa que guarda no ambiente global (isto é um efeito-lateral) mas devolve uma string. Como o {targets} precisa de funções puras, se escrevermos:\n\ntar_target(\n  target_mtcars,\n  data(mtcars)\n)\n\no target target_mtcars será igaul à string \"mtcars\". Relembremos que um target tem de devolver alguma coisa e funções com efeitos-laterais nem sempre devolvem alguma coisa, ou pelo menos não a coisa que queremos. Como vimos com plot() que não devolve um objecto, também aqui temos o mesmo problema.\nPara resolvermos este problema, precisamos duma função pura que devolve um data.frame. Isto significa que precisamos de carregar os dados, que resultam ser uma promessa (carregados num ambiente directamente), e depois temos e evaluar essa promessa. A função que faz isto:\n\nread_data &lt;- function(data_name, package_name){\n\n  temp &lt;- new.env(parent = emptyenv())\n\n  data(list = data_name,\n       package = package_name,\n       envir = temp)\n\n  get(data_name, envir = temp)\n}\n\nEsta função tem como argumentos data_name e package_name, ambos strings.\nUsamos data() com três argumentos: list =, package = e envir =. Precisamos do primeiro argumento (list =) porque queremos passar data_name como uma string. Se fizéssemos qualquer coisa como data(data_name), esperando que data_name fosse substituído pelo valor do seu input teríamos um erro. Isto porque data() iria procurar por um conjunto de dados que se chamasse literalmente data_name em vez de substituir pelo seu input. O segundo argumento, package = serve para definirmos em que pacote é que devemos procurar o conjunto de dados, e neste caso daremos como input o nosso pacote{housing}. Por fim temos o argumento envir =. Este argumento serve para dizer a data() onde deve carregar o conjunto de dados. Por defeito, data() carrega os dados no ambiente global. Mas nós queremos uma função pura que só deve devolver um objecto de dados e não pode carregar nada no ambiente global. É aqui que entra o ambiente temporário criado na primeira linha do corpo da função. Então a função carrega o objecto de dados no ambiente temporário (que é diferente do ambiente global) e quando terminamos podemos livrar-nos desse ambiente deixando limpo o ambiente global.\nPor fim usamos get(), para tornarmos a promessa num conjunto de dados. Se apenas fizéssemos data(list = data_name...) o noso target teria simplesmente uma string do tipo character. A função get() procura um objecto pelo nome e devolve-o. Ou seja, na linha get(data_name), o nome do conjunto de dados que dermos a data_name como input e que está no ambiente temporário que definimos é devolvido como um conjunto de dados. Desta forma não há qualquer interação com o ambiente global, pelo que a função é pura: devolve sempre o mesmo output para o mesmo input, e não polui, de qualquer maneira, o ambiente global. Depois desta função ser executada o ambiente temporário é descartado.\nIsto parece complicado mas é apenas uma consequência do {targets} precisar de funções puras que devolvem alguma coisa para funcionar bem. Infelizmente nem todas as funçõs do R são puras pelo que precisamos deste tipo de artimanhas. No entanto, este trabalho não é em vão. Ao nos forçar a usar funções puras, o {targets} contribui para a qualidade geral e para a segurança do nosso pipeline. Quando o nosso pipeline acabar de executar o ambinte global permanecerá limpo. Ter objectos a poluir o nosso ambiente global pode causar interações em próximas execuções do pipeline:\nVamos então continuar o nosso pipeline:\n\nlibrary(targets)\nlibrary(tarchetypes)\n\ntar_option_set(packages = \"housing\")\n\nsource(\"functions/read_data.R\")\n\nlist(\n  tar_target(\n    commune_level_data,\n    read_data(\"commune_level_data\",\n              \"housing\")\n  ),\n\n  tar_target(\n    country_level_data,\n    read_data(\"country_level_data\",\n              \"housing\")\n  ),\n\n  tar_target(\n    commune_data,\n    get_laspeyeres(commune_level_data)\n  ),\n\n  tar_target(\n    country_data,\n    get_laspeyeres(country_level_data)\n  ),\n\n  tar_target(\n    communes,\n    c(\"Luxembourg\",\n      \"Esch-sur-Alzette\",\n      \"Mamer\",\n      \"Schengen\",\n      \"Wincrange\")\n  ),\n\n  tar_render(\n    analyse_data,\n    \"analyse_data.Rmd\"\n  )\n\n)\n\nE vejamos agora como está o ficheiro analyse_data.Rmd:\n---\ntitle: \"Nominal house prices data in Luxembourg\"\nauthor: \"Bruno Rodrigues\"\ndate: \"`r Sys.Date()`\"\n---\n\nLet’s load the datasets (the Laspeyeres price index is already computed):\n\n```{r}\ntar_load(commune_data)\ntar_load(country_data)\n```\n\nWe are going to create a plot for 5 communes and compare the\nprice evolution in the communes to the national price evolution. \nLet’s first load the communes:\n\n```{r}\ntar_load(communes)\n```\n\n```{r, results = \"asis\"}\nres &lt;- lapply(communes, function(x){\n\n  knitr::knit_child(text = c(\n\n    '\\n',\n    '## Plot for commune: `r x`',\n    '\\n',\n    '```{r, echo = F}',\n    'print(make_plot(country_data, commune_data, x))',\n    '```'\n\n     ),\n     envir = environment(),\n     quiet = TRUE)\n\n})\n\ncat(unlist(res), sep = \"\\n\")\n\n```\nComo podemos ver os dados são carregados com tar_load() que carrega os dois conjuntos de dados previamente computados. A parte final do documento é semelhante ao que tinhamos antes de tornarmos a nossa análise num pacote e depois num pipeline. Usamos um documento ‘filho’ para gerarmos tantas secções quantas as necessárias (Don’t Repeat Yourself!). Podemos alterar o noso pipeline, removendo elementos do objecto communes e voltando a executar todo o pipeline com tar_make().\nFinalizada esta introdução ao {targets}: transformando a nossa análise num pipeline; precisamos agora garantir que os seus outputs são reprodutíveis. O primeiro passo é usarmos o {renv}, mas como já discutimos, isto não é suficiente, mas é essencial que o façamos. Vamos então inicializar o {renv}:\n\nrenv::init()\n\nEste comando cria um ficheiro renv.lock com a listagem de todas as dependências do pipeline. Também o nosso pacote do Github fica listado:\n\"housing\": {\n  \"Package\": \"housing\",\n  \"Version\": \"0.1\",\n  \"Source\": \"GitHub\",\n  \"RemoteType\": \"github\",\n  \"RemoteHost\": \"api.github.com\",\n  \"RemoteRepo\": \"housing\",\n  \"RemoteUsername\": \"rap4all\",\n  \"RemoteRef\": \"fusen\",\n  \"RemoteSha\": \"1c860959310b80e67c41f7bbdc3e84cef00df18e\",\n  \"Hash\": \"859672476501daeea9b719b9218165f1\",\n  \"Requirements\": [\n    \"dplyr\",\n    \"ggplot2\",\n    \"janitor\",\n    \"purrr\",\n    \"readxl\",\n    \"rlang\",\n    \"rvest\",\n    \"stringr\",\n    \"tidyr\"\n  ]\n},\nSe repararmos nos campos RemoteSha e RemoteRef reconhecemos o hash do commit e o repositório que usamos para instalar o pacote:\n\"RemoteRef\": \"fusen\",\n\"RemoteSha\": \"1c860959310b80e67c41f7bbdc3e84cef00df18e\",\nIsto significa que se mais alguém quiser re-executar o nosso projecto, usando renv::restore(), instala a versão correcta do pacote. Para concluirmos, devemos editar o ficheiro Readme.md e adicionar instruções sobre como re-executar o projecto. O ficheiro Readme.md poderá ficar assim:\n# How to run\n\n- Clone the repository: `git clone git@github.com:rap4all/housing.git`\n- Switch to the `pipeline` branch: `git checkout pipeline`\n- Start an R session in the folder and run `renv::restore()` \n   to install the project’s dependencies.\n- Run the pipeline with `targets::tar_make()`.\n- Checkout `analyse_data.html` for the output.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#mais-algumas-pequenas-dicas",
    "href": "targets.html#mais-algumas-pequenas-dicas",
    "title": "13  Automatização com {targets}",
    "section": "13.9 Mais algumas pequenas dicas",
    "text": "13.9 Mais algumas pequenas dicas\nNesta secção poderemos ver algumas funções muito úteis que estão incluídas no pacote {targets} e que devemos conhecer.\n\n13.9.1 Carregar todos os targets duma vez\nÉ possível carregar todos os targets em cache com a função tar_load_everything(). Mas se o nosso pipeline tiver muitos targets, isto pode ser demorado.\n\n\n13.9.2 Adicionar meta informação ao pipeline\nA função tar_meta() devolve um dataframe com alguma informação sobre o pipeline. Isto pode se bem útil, depois de executarmos o pipeline, para identificação de alguns avisos e erros. Vejamos o aspecto deste dataframe.\n\ntargets::tar_meta()\n\n# A tibble: 11 × 18\n   name       type  data  command depend [...]\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  [...]\n 1 analyse_d… stem  c251… 995812… 3233e… [...]\n 2 commune_d… stem  024d… 85c2ab… ec7f2… [...]\n 3 commune_l… stem  fb07… f48470… ce0d8… [...]\n 4 commune_l… stem  fb07… 2549df… 15e48… [...]\n 5 communes   stem  b097… be3c56… a3dad… [...]\n 6 country_d… stem  ae21… 9dc7a6… 1d321… [...]\nHá meas colunas além destas que estamos a ver, as de maior interesse são as colunas warnings e error. No exemplo seguinte temos um aviso depois de alterarmos o código para read_data():\n\ntargets::tar_make()\n\n• start target commune_level_data\n• built target commune_level_data [0.61 seconds]\n• start target country_level_data\n• built target country_level_data [0.02 seconds]\n✔ skip target communes\n✔ skip target commune_data\n✔ skip target country_data\n✔ skip target analyse_data\n• end pipeline [0.75 seconds]\nWarning messages:\n1: this is a warning \n2: this is a warning \n3: 2 targets produced warnings. Run tar_meta(fields = warnings, \n  complete_only = TRUE) for the messages. \n&gt; \nPorque temos avisos, o pipeline dá a mensagem para executarmos tar_meta(fields = warnings, complete_only = TRUE), assim:\n\ntar_meta(fields = warnings, complete_only = TRUE)\n\nCom este código temos um dataframe com o nome do target que produz o aviso e o próprio aviso.\n# A tibble: 2 × 2\n  name               warnings\n  &lt;chr&gt;              &lt;chr&gt;\n1 commune_level_data this is a warning\n2 country_level_data this is a warning\nE assim podemos ver melhor o que se passa.\n\n\n13.9.3 Tornar um target ou todo o pipeline outdated\nCom tar_invalidate() podemos tornar um target outdated, para que quando voltarmos a executar o pipeline este volta a ser computado (juntamente com todos os targets que dele dependam). Isto pode ser útil para garantirmos que tudo está a correr correctamente. Também é possível dinamitar todo o pipeline e voltar a executar tudo do zero com a função tar_destroy().\n\n\n13.9.4 Visualizações costumizadas\nCom o comando visNetwork::visNetworkEditor(tar_visnetwork()), é iniciada uma aplicação Shiny que nos permite costumizar a rede do nosso pipeline. Podemos brincar com as opções e ver que efeito têm na nossa rede. Também é possível gerar código R que pode ser copiado para um script garantindo que geramos sempre o mesmo aspecto.\n\n\n13.9.5 Usar targets dum pipeline noutro projecto\nSe precisarmos de carregar algum target noutro projecto (como por exemplo para referenciar um estudo antigo), podemos usar o pacote {withr}:\n\nwithr::with_dir(\n  \"path/to/root/of/project\",\n  targets::tar_load(target_name))\n\n\n\n13.9.6 Perceber uma mensagem de erro críptica\nÁs vezes ao executarmos um pipeline temos a mensagem de erro:\nError:\n! Error running targets::tar_make()\n  Target errors: targets::tar_meta(fields = error, complete_only = TRUE)\n  Tips: https://books.ropensci.org/targets/ debugging.html\n  Last error: argument 9 is empty\nO importante é a última linha: “Last error: argument 9 is empty”. Não é claro qual o target que dá o erro: isto porque o erro não é devido a um target mas sim ao próprio pipeline. Lembremo-nos que o pipeline é apenas uma lista de targets. Se o último target acaba com uma vírgula (,) a list() está à espera de mais um elemento. Como não há mais nenhum, então dá erro. É como se escrevessemos list(1, 2, ).",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#conclusão",
    "href": "targets.html#conclusão",
    "title": "13  Automatização com {targets}",
    "section": "13.10 Conclusão",
    "text": "13.10 Conclusão\nAqui podemos ver porque precisamos de adicionar a automatização à nossa caixa de ferramentas. O pacote {targets} é fantástico porque trata de coisas muito enfadonhas por nós. Com o {tatgets} não nos precisamos de preocupar em saber quais são os objectos que precisam de ser re-computados quando precisamos de alterar o nosso código. Não precisamos de reescrever o nosso código para o executarmos em paralelo. E com o {renv}, outros utilizadores podem correr o nosso pipeline e reproduzir os nosso resultados.\nNo próximo capítulo mergulhamos mais fundo no iceberg da reprodutibilidade.\n\n\n\n\nLandau, William Michael. 2021. “The Targets r Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 6 (57): 2959.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "targets.html#footnotes",
    "href": "targets.html#footnotes",
    "title": "13  Automatização com {targets}",
    "section": "",
    "text": "https://is.gd/VS6vSs↩︎\nhttps://is.gd/lm4QoO↩︎",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Automatização com `{targets}`</span>"
    ]
  },
  {
    "objectID": "repro_cont.html",
    "href": "repro_cont.html",
    "title": "14  Pipelines de análise reprodutíveis com o Docker",
    "section": "",
    "text": "14.1 O que é o Docker?\nEm termos simples (e tecnicamente errados), o Docker facilita a execução duma máquina virtual (VM) Linux no nosso computador. Uma VM é um computador dentro du computador: a ideia é ligarmos o computador, iniciarmos o Windows (ou o sistema operativo que utilizamos todos os dias), mas depois iniciarmos o Ubuntu (uma distribuição Linux muito popular) como se fosse qualquer outra aplicação instalada no nosso computador e utilizá-lo (quase) como fariamos normalmente. É isto que uma solução VM clássica como o Virtualbox nos oferece. Podemos iniciar e utilizar o Ubuntu de forma interactiva a partir do Windows. O que pode ser bastante útil para testes, por exemplo.\nA diferença entre o Docker e o Virtualbox (ou VMware) é que o primeiro reduz a VM ao seu essencial. Não há interface gráfica de utilizador, por exemplo, e não vamos (normalmente) usar uma VM Docker interativamente. Em vez disso, escrevemos num ficheiro de texto as especificações da VM que pretendemos. A este ficheiro de texto designamos de Dockerfile. Por exemplo, se queremos a VM tenha por base o Ubuntu, então essa seria a primeira linha do Dockerfile. Depois queremos que a VM tenha o R instalado. Então essa seria a segunda linha. Precisamos de instalar pacotes R, então adicionamos essas linhas também. Talvez seja necessário adicionar algumas dependências do sistema? Adicione-mo-las. Por fim, adicionamos o código do pipeline que pretende tornar reproduzível.\nQuando terminarmos, teremos este ficheiro de texto, o Dockerfile, com uma receita completa para gerar uma VM Docker. Essa VM é designada como imagem (como dissemos anteriormente, tecnicamente não é uma VM verdadeira, mas não vamos discutir isso). Então temos um ficheiro de texto que nos ajuda a definir e a gerar uma imagem. Aqui, já podemos ver uma primeira vantagem de usarmos o Docker em relação a uma solução de VM mais tradicional como o Virtualbox: podemos escrever facilmente estes Dockerfiles e versioná-los. Podemos facilmente começar a partir de outro Dockerfile de outro projeto e adaptá-lo ao nosso pipeline atual. E o mais importante, porque tudo está escrito, é reprodutível (mas mais sobre isso no final deste capítulo…).\nOk, temos a imagem. Esta imagem terá por base em alguma distribuição Linux, geralmente o Ubuntu. Tem uma versão específica do Ubuntu, enós podemos adicionar uma versão específica do R. Podemos também baixar versões específicas de todos os pacotes necessários para o nosso pipeline. O resultado é um ambiente feito sob medida para o nosso pipeline. Podemos agora executar o pipeline com esta imagem Docker e obter sempre exatamente os mesmos resultados, sempre. Isso acontece porque, independentemente de como, onde ou quando executarmos o pipeline dockerizado, a mesma versão do R, com a mesma versão dos pacotes R, na mesma distribuição Linux, será usada para reproduzir os resultados do nosso pipeline. A propósito, quando executamos uma imagem do Docker, ou seja, quando executamos o nosso pipeline com a imagem definida, estamos a referirnos a um container do Docker.\nOu seja, um ficheiro Docker definie uma imagem Docker, a partir da qual podemos executar containers. As imagens seguintes são ilustrativas. Na primeira vemos o que acontece quando o mesmo pipeline é executado em duas versões diferentes do R e em dois sistemas operativos diferentes:\nSe repararmos nos outputs, veremos que são diferentes.\nAgora, se executarmos o mesmo pipeline com o Docker:\nOutra forma de vermos uma imagem Docker: é uma sandbox imutável, onde as regras do jogo são sempre as mesmas. Não importa onde ou quando executamos essa sandbox, o pipeline sempre será executado nesse mesmo espaço bem definido. Como o pipeline é executado nas mesmas versões do R (e pacotes) e no mesmo sistema operativo definido na imagem do Docker, o nosso pipeline é agora verdadeiramente reprodutível.\nMas porquê o Linux; porque é que não é possível criar imagens Docker com base em Windows ou macOS? Lembremo-nos da introdução, onde explicámos o que é reprodutibilidade:\nO código aberto não é apenas um requisito para a linguagem de programação que utilizamos para construirmos o pipeline, também se estende ao sistema operativo em que o pipeline é executado. Assim, a razão pela qual o Docker utiliza o Linux é porque pode utilizar distribuições Linux como o Ubuntu gratuitamente e sem restrições. Não há licenças que precisem ser compradas ou ativadas, e as distribuições Linux podem ser personalizadas para qualquer caso de uso imaginável. Assim, as distribuições Linux são a única opção disponível para o Docker para esta tarefa.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pipelines de análise reprodutíveis com o Docker</span>"
    ]
  },
  {
    "objectID": "repro_cont.html#o-que-é-o-docker",
    "href": "repro_cont.html#o-que-é-o-docker",
    "title": "14  Pipelines de análise reprodutíveis com o Docker",
    "section": "",
    "text": "Executar um pipeline sem o Docker pode resultar (potencialmente) em outputs diferentes.\n\n\n\n\n\n\n\nExecutar um pipeline com o Docker tem sempre o mesmo output.\n\n\n\n\n\nO código aberto é um requisito fundamental para a reprodutibilidade.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pipelines de análise reprodutíveis com o Docker</span>"
    ]
  },
  {
    "objectID": "repro_cont.html#uma-iniciação-ao-linux",
    "href": "repro_cont.html#uma-iniciação-ao-linux",
    "title": "14  Pipelines de análise reprodutíveis com o Docker",
    "section": "14.2 Uma iniciação ao Linux",
    "text": "14.2 Uma iniciação ao Linux\nAté aqui, podiamos ter seguido o livro usando qualquer sistema operativo. A maior parte do código deste livro é código R, por isso não importa em que sistema operativo o estamos a executar. Mas já houve algum código que executámos na consola do Linux (por exemplo, usámos ls para listar ficheiros). Esses comandos também devem funcionar no macOS, mas no Windows, eu usámos o terminal do Git Bash. Isso porque ls (e outros comandos semelhantes) não funcionam no prompt de comando padrão do Windows (mas devem funcionar no Powershell). Em vez de usarmos o terminal (ou o Git Bash) para navegar no sistema de arquivos do nosso computador, podiamos continuar usando a interface de usuário do nosso sistema operacional também. Por exemplo, no ?sec-packages, listamos o conteúdo dpasta dev/ com o comando:\nowner@localhost ➤ ls dev/\nmas podiamos apenas ter aberto a pasta dev/ no explorador de ficheiros do nosso sistema operativo. Mas para usarmos o Docker, precisamos de conhecer o Linux e um pouco do ecosistema e de conceitos Linux. Mas não é tão difícil como pode parecer.\nLinux não é o nome de um sistema operativo específico, mas sim do kernel de um sistema operativo. Um kernel é um componente importante de um sistema operativo. O Linux é gratuito e de código aberto, e está entre os projectos gratuitos e de código aberto mais bem sucedidos de sempre. Como a sua licença permite (e encoraja) a reutilização, qualquer pessoa pode usar este kernel e adicionar todos os outros componentes necessários para construir um sistema operativo completo e disponibilizar o produto final. É por isto que existem muitas distribuições Linux: uma distribuição Linux é um sistema operativo completo que usa o Linux como kernel. A distribuição Linux mais popular chama-se Ubuntu, e procurarmos no Google qualquer coisa como “easy linux os for beginners” (sistema operativo Linux fácil para principiantes), a resposta que aparecerá no topo será provavelmente o Ubuntu, ou uma das outras variantes do Ubuntu (sim, porque o Ubuntu em si também é open-source e software livre, e é possível construir variantes usando o Ubuntu como base, como o Linux Mint).\nPara definirmos as nossas imagens Docker, usaremos o Ubuntu como base. O sistema operativo Ubuntu tem duas versões por ano, uma em abril e outra em outubro. Nos anos pares, a versão de abril é uma versão de suporte a longo prazo (LTS). As versões LTS recebem actualizações de segurança durante 5 anos, e as imagens Docker utilizam geralmente uma versão LTS como base. Neste momento (maio de 2023), a atual LTS é o Ubuntu 22.04 Jammy Jellyfish (os lançamentos do Ubuntu são nomeados com um número da forma YY.MM e depois um nome de código baseado em algum animal).\nPodemos instalar o Ubuntu no nosso computador. Mas não é necessário, uma vez que podemos usar o Docker para embarcar os nossos projectos!\nUma grande diferença entre o Ubuntu (e outras distribuições Linux) e o macOS e o Windows é a forma como instala o software. Resumindo, o software para distribuições Linux é distribuído como pacotes. Se quisermos instalar, por exemplo, o editor de texto Emacs, podemos executar o seguinte comando no terminal:\nsudo apt-get install emacs-gtk\nVamos por partes: sudo faz com que os próximos comandos sejam executados como root. root é o jargão do Linux para a conta de administrador. Portanto, se digitarmos sudo xyz, o comando xyz será executado com privilégios de administrador. Depois temos o apt-get install. apt-get é o gestor de pacotes do Ubuntu, e install é o comando que instala o emacs-gtk. emacs-gtk é o nome do pacote Emacs. Como somos utilizadores do R, isto é-nos familiar: afinal, as extensões para o R também são instaladas usando um gestor de pacotes e um comando: install.packages(“nome_do_pacote”). Tal como no R, onde os pacotes são descarregados do CRAN, o Ubuntu descarrega pacotes de um repositório que podemos consultar aqui1. Claro que, como usar a linha de comando pode ser intimidante para iniciantes, também é possível instalar pacotes usando uma loja de software, assim como no macOS ou no Windows. Mas recordemos, o Docker só usa o que é absolutamente necessário para funcionar, por isso não existe uma interface de utilizador interactiva. Isto não se deve ao facto de os programadores do Docker não gostarem de interfaces de utilizador, mas sim porque o objetivo do Docker não é utilizar imagens Docker de forma interactiva, pelo que não há necessidade de uma interface de utilizador. Portanto, é necessário sabermos como instalar os pacotes do Ubuntu com a linha de comando.\nTal como para o R, é possível instalarmos software de diferentes fontes. podemos adicionar diferentes repositórios e instalar software a partir daí. Não vamos fazê-lo aqui, mas só como um aparte, se estivermos a usar o Ubuntu no nosso computador como nosso sistema operativo diário, devemos realmente verificar o r2u2, um repositório do Ubuntu que vem com pacotes R pré-compilados que podem ser instalados, muito, muito rapidamente. Apesar de não o utilizarmos aqui (e veremos o porquê mais tarde neste capítulo), devemos definitivamente considerar o r2u como fornecedor de pacotes R binários se utilizarmos o Ubuntu como o nosso sistema operativo diário.\nVamos supor que estamo a usar o Ubuntu na nossa máquina, e que usamos o R. Se quisermos instalar o pacote R {dplyr}, acontece algo interessante quando escrevemos:\n\ninstall.packages(\"dplyr\")\n\nNo Windows e no macOS, um binário compilado é baixado do CRAN e instalado no nosso computador. Um “binário” é o código fonte compilado dum pacote. Muitos pacotesR vêm com código o C++ ou Fortran e este código não pode ser usado como tal pelo R. Logo esses bits de código C++ e Fortran precisam de ser compilados para serem usados. Pensemos da seguinte forma: se o código fonte são os ingredientes, o binário compilado é refeição cozinhada. Imaginemos que cada vez que queremos comer Bouillabaisse, ou a cozinhamos nós mesmos… ou a mandamos vir entregar em nossa casa. Provavelmente, optaremos pela entrega a domicílio (especialmente se for gratuita). Mas isto significa que houve alguém que teve de confeccionar a Bouillabaisse para nós. O CRAN basicamente cozinha pacotes de código fonte em binários para o Windows e macOS, como podemos ver:\n\n\n\nLinks de Download links para binários pré-compilados do tidyverse.\n\n\nNesta imagem, podemos ver links para binários compilados do pacote {tidyverse} para Windows e macOS, mas nenhum para qualquer distribuição Linux. Isto porque, como referimos na introdução, existem muitas, muitas, muitas distribuições Linux. Então, na melhor das hipóteses, o CRAN poderia oferecer binários para o Ubuntu, mas o Ubuntu não é a única distribuição Linux, e o Ubuntu tem dois lançamentos por ano, o que significa que cada pacote CRAN (que precisa de compilação) precisaria ser compilado duas vezes por ano. Isto é uma tarefa enorme, a menos que o CRAN decida oferecer apenas binários para as versões LTS. Mas isso ainda seria a cada dois anos.\nEntão, o que acontece é que o fardo da compilação é empurrado para o utilizador. Sempre que escrevemos install.packages(“nome_do_pacote”), e se este pacote requer compilação, o pacote é compilado na nossa máquina, o que não só leva algum tempo, mas também pode falhar. Isto porque, muitas vezes, os pacotes R que requerem compilação precisam de algumas dependências adicionais ao nível do sistema que precisam ser instaladas. Por exemplo, estas estão as dependências do Ubuntu que precisam ser instaladas para que a instalação do pacote {tidyverse} seja bem-sucedida:\nlibicu-dev\nzlib1g-dev\nmake\nlibcurl4-openssl-dev\nlibssl-dev\nlibfontconfig1-dev\nlibfreetype6-dev\nlibfribidi-dev\nlibharfbuzz-dev\nlibjpeg-dev\nlibpng-dev\nlibtiff-dev\npandoc\nlibxml2-dev\nÉ por isso que o r2u é tão útil: ao adicionarmos este repositório, o que estmos essencialmente a fazer é dizer ao R para não irmos buscar os pacotes ao CRAN, mas sim ao repositório r2u. E este repositório contém pacotes R compilados para o Ubuntu. Assim, as dependências necessárias ao nível do sistema são instaladas automaticamente e o pacote R não precisa de compilação. Logo, a instalação do pacote {tidyverse} demora menos de meio minuto numa máquina moderna.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pipelines de análise reprodutíveis com o Docker</span>"
    ]
  },
  {
    "objectID": "repro_cont.html#primeiros-passos-com-o-docker",
    "href": "repro_cont.html#primeiros-passos-com-o-docker",
    "title": "14  Pipelines de análise reprodutíveis com o Docker",
    "section": "14.3 Primeiros passos com o Docker",
    "text": "14.3 Primeiros passos com o Docker\nVamos começar por criar uma imagem Docker “Hello World”. Como referimos no início, para definirmos uma imagem Docker, precisamos de criar um Dockerfile com algumas instruções. Mas antes, é claro, precisamos instalar o Docker. Para instalarmos o Docker em qualquer sistema operativo (Windows, macOS ou Ubuntu ou outros Linuxes), podemos instalar o Docker Desktop3. Se estivermos a correr o Ubuntu (ou outra distribuição Linux) e não quisermos a GUI, podemos instalar o Docker engine4 e depois seguirmos os passos para Linux5 de pós instalação.\nEm qualquer caso, independentemente do nosso sistema operativo, vamos utilizar a linha de comandos para interagir com o Docker. Quando terminarmos de instalar o Docker, criamos uma pasta em algum lugar do seu computador e dentro dessa pasta criamos um arquivo de texto vazio com o nome “Dockerfile”. Isso pode ser complicado no Windows, pois é preciso remover a extensão .txt que é adicionada por padrão. Poderá ser necessário ativar a opção “Extensões de nomes de ficheiros” no painel Ver do explorador de ficheiros do Windows para facilitar este processo. Em seguida, abrimos este ficheiro com o nosso editor de texto preferido e adicionamos as seguintes linhas:\nFROM ubuntu:jammy\n\nRUN uname -a\nEste Dockerfile muito simples faz duas coisas: começa por afirmar que é baseado no sistema operacional Ubuntu Jammy (então versão 22.04), e então executa o uname -a. Este comando, que é executado dentro da linha de comando do Ubunu, imprime a versão do kernel Linux daquela versão particular do Ubuntu. FROM e RUN são comandos do Docker; existem alguns outros que vamos descobrir um pouco mais tarde. Agora, o que fazemos com este Dockerfile? Lembremo-nos, um Dockerfile define uma imagem. Então, agora, precisamos construir essa imagem para executar um container. Abrimos um terminal/ prompt de comando na pasta onde está o Dockerfile e digitamos:\nowner@localhost ➤ docker build -t raps_hello .\nO comando docker build cria uma imagem a partir do Dockerfileque está no caminho . (um simples . significa “na directoria de trabalho atual”). A opção -t atribui à imagem o nome raps_hello. Se tudo correr, bem deveremos ver o output:\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM ubuntu:jammy\n ---&gt; 08d22c0ceb15\nStep 2/2 : RUN uname -a\n ---&gt; Running in 697194b9a519\nLinux 697194b9a519 6.2.6-1-default #1 SMP PREEMPT_DYNAMIC \n     Mon Mar 13 18:57:27 UTC 2023 (fa1a4c6) x86_64 x86_64 x86_64 GNU/Linux\nRemoving intermediate container 697194b9a519\n ---&gt; a0ea59f23d01\nSuccessfully built a0ea59f23d01\nSuccessfully tagged raps_hello:latest\nEm Step 2/2 veremos o output do comando unamec -a:\nLinux 697194b9a519 6.2.6-1-default #1 SMP PREEMPT_DYNAMIC\n     Mon Mar 13 18:57:27 UTC 2023 (fa1a4c6) x86_64 x86_64 x86_64 GNU/Linux\nCada expressão RUN no Dockerfile é executada no momento de construção e é o que usaremos para instalar o R e os pacotes necessários. Assim, quando é construída, ficamos com uma imagem que contém todo o software que precisamos.\nAgora, gostaríamos de poder usar essa imagem. Com uma imagem construída, podemos iniciar um ou vários containers que podemos utilizar para o que quisermos. Vamos criar um exemplo mais realista. Construimos uma imagem Docker que vem com o R pré-instalado. Para isto, precisamos voltar ao nosso Dockerfile e alterá-lo um pouco:\nFROM ubuntu:jammy\n\nENV TZ=Europe/Luxembourg\n\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ &gt; /etc/timezone\n\nRUN apt-get update && apt-get install -y r-base\n\nCMD [\"R\"]\nPrimeiro com o ENV, criamos uma variável chamada TZ e definimo-la para o fuso horário da Europe/Luxembourg (podemos alterar para outro fuso horário qualquer). De seguida, executamos um comando de aspecto bastante complexo que define o fuso horário definido para todo o sistema. Tivemos de fazer tudo isto porque, quando instalarmos o R, será instalada uma dependência a nível do sistema chamada tzdata. Esta ferramenta pede então ao utilizador que introduza o seu fuso horário de forma interactiva. Mas como não podemos interagir com a imagem enquanto está a ser construída, o processo de construção ficaria preso neste prompt. Usando estes dois comandos, podemos definir o fuso horário correto e assim que o tzdata for instalado, essa ferramenta já não pede o fuso horário, e o processo de construção pode continuar. Este é um problema bastante comum quando se constroem imagens Docker baseadas no Ubuntu, por isso a correcção é facilmente encontrada com uma pesquisa no Google (mas fica já aqui, de graça).\nDepois temos comandos RUN. O primeiro usa o gestor de pacotes do Ubuntu para refrescar os repositórios (isto garante que os repositórios da nossa instalação local do Ubuntu estão sincronizados com os updates de software mais recentes dos repositórios centrais do Ubuntu). Depois usamos o gestor de pacotes do Ubuntu para instalar o r-base. O r-base é o pacote que instala o R. Finalizamos este ficheiro Docker com a execução de CMD [\"R\"]. Este é o comando que será executado quando iniciarmos o container. Devemos ter em conta que os comandos RUN são executados no momento da construção enquanto que CMD é executado no momento do arranque. Esta distinção será importante mais tarde.\nVamos construir a imagem (vai demorar algum tempo porque há muito software a ser instalado):\nowner@localhost ➤ docker build -t raps_ubuntu_r .\nEsta ordem constrói a imagem designada raps_ubuntu_r. esta imagem tem por base o Ubunto 22.04 Jammy Jellyfish e vem com o R pré-instalado. Mas a versão o R inatalada é q disponibilizada pelos repositórios Ubuntu, que neste moemnto é a versão 4.1.2, enquanto que a última versão do R é 4.2.3. OU seja, a versão disponível a partir dos repositórios do Ubuntu é anterior à versão mais actual. Mas não importa, trataremos disso mais tarde.\nPodemos agora iniciar o container com o comando:\nowner@localhost ➤ docker run raps_ubuntu_r\nE este é o output que temos:\nFatal error: you must specify '--save', '--no-save' or '--vanilla'\nQual é o problema? Quando iniciamos o container, o comando especificado pelo CMD é executado e, em seguida, o container é encerrado. Assim, aqui, o container executou o comando R, que iniciou o interpretador R, mas depois saiu imediatamente. Ao sair do R, os utilizadores devem especificar se querem ou não guardar o espaço de trabalho. É isto que a mensagem acima nos está a dizer. Então, como é que podemos usar isto? Existe alguma forma de utilizarmos esta versão do R de forma interactiva?\nSim, existe uma maneira de usar essa versão do R dentro da nossa imagem do Docker de forma interativa, mesmo que não seja isso o que queremos. O que queremos, em vez disso, é que o nosso pipeline seja executado quando executamos o container. Não queremos mexer no container de forma interactiva. Mas vejamos como podemos interagir com esta versão dockerizada do R. Primeiro, é preciso deixar o container rodar em segundo plano. para isso executamos o comando:\nowner@localhost ➤ docker run -d -it --name ubuntu_r_1 raps_ubuntu_r\nIsto inicia o container a que chamamos “ubuntu_r_1” a partir da imagem “raps_ubuntu_r” (recordemos que podemos executar muitos containers a partir de uma única definição de imagem). Graças à opção -d, o container corre em segundo plano, e a opção -it indica que queremos que uma shell interactiva esteja à nossa espera. Assim, o contentor corre em segundo plano, com uma shell interactiva à nossa espera, em vez de lançar (e depois parar imediatamente) o comando R. Podemos agora “ligar-nos” à shell interactiva e iniciar o R nela usando:\nowner@localhost ➤ docker exec -it ubuntu_r_1 R\nDeveremos estar a ver um prompt familiar:\nR version 4.1.2 (2021-11-01) -- \"Bird Hippie\"\nCopyright (C) 2021 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; \nSejamos bem vindos a uma versão dockerizada do R. Tudo isto pode parecer muito complicado, principalmente se é a primeira vez que brincamos com o Docker. No entanto, não nos devemos preocupar muito, uma vez que:\n\nnão vamos usar containers Docker de forma interactiva, não é esse o nosso objectivo, mas pode ser util ligarmo-nos a um container em execução para verificarmos se tudo está a correr como esprado;\nvamos construir as nossas imagens a partir de imagens pré-construidas do Rocker project6 e estas imagens vêm já com muito software pré-instalado e com as configurações feitas.\n\nO que devemos retirar desta secção é que precisamos escrever um ficheiro Docker que nos permita construir uma imagem. Essa imagem pode então ser usada para executar um (ou vários) containers. Esses containers, depois de iniciados, executarão nosso pipeline em um ambiente que está congelado, de modo que o output dessa execução permanecerá constante, para sempre.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pipelines de análise reprodutíveis com o Docker</span>"
    ]
  },
  {
    "objectID": "repro_cont.html#o-rocker-project",
    "href": "repro_cont.html#o-rocker-project",
    "title": "14  Pipelines de análise reprodutíveis com o Docker",
    "section": "14.4 O Rocker project",
    "text": "14.4 O Rocker project\nO projeto Rocker oferece uma coleção muito grande de imagens Docker “R-ready” que podemos usar como ponto de partida para construir as nossas próprias imagens Docker. Antes de usarmos essas imagens, porém, ainda precisamos compreender um conceito muito importante do Docker. Vamos voltar à nossa imagem Docker “Hello World”:\nFROM ubuntu:jammy\n\nRUN uname -a\nA primeira linha, FROM ubuntu:jammy baixa uma imagem do Ubuntu Jammy, mas de onde? Todas estas imagens são descarregadas do Docker Hub, que podemos visitar aqui7. Se criarmos uma conta, podemos até enviar as nossas próprias imagens para lá. Por exemplo, poderíamos colocar a imagem que construímos antes, que chamamos de raps_ubuntu_r, no Docker Hub. Então, se quiséssemos criar uma nova imagem Docker que se baseia em raps_ubuntu_r, poderíamos simplesmente digitar FROM username:raps_ubuntu_r (ou algo similar).\nTambém é possível não usarmos o Docker Hub e partilharmos a imagem que construímos como um ficheiro, como veremos mais à frente.\nO projeto Rocker oferece muitas imagens diferentes, que estão descritas aqui8. Vamos usar as imagens versionadas. Estas são imagens que carregam versões específicas do R. Desta forma, não importa quando a imagem é construída, a mesma versão do R será instalada ao ser construída a partir do código fonte. Vejamos por que é importante construir o R a partir do código. Quando construímos a imagem com o Dockerfile que escrevemos antes, o R é instalado a partir dos repositórios do Ubuntu. Para isso, usamos o gerenciador de pacotes do Ubuntu e o seguinte comando: apt-get install -y r-base. Neste momento, a versão do R que é instalada é a versão 4.1.3. Há dois problemas com a instalação do R a partir dos repositórios do Ubuntu. Primeiro, temos que usar o que for instalado, o que pode ser um problema de reprodutibilidade. Se executámos a nossa análise usando a versão 4.2.1 do R, então gostaríamos de instalar essa versão do R. O segundo problema é que quando construímos a imagem hoje obtemos a versão 4.1.3. Mas não é impossível que, se construirmos a imagem daqui a 6 meses, obtenhamos a versão 4.2.0 do R, pois é provável que a versão que vem nos repositórios do Ubuntu seja actualizada.\nIsto significa que, dependendo de quando construímos a imagem do Docker, podemos obter uma versão diferente do R. Existem apenas duas maneiras de evitar esse problema: ou construímos a imagem uma vez e a arquivamos e nos certificamos de manter sempre uma cópia e embarcamos essa cópia para sempre (ou pelo tempo que quisermos garantir que o pipeline seja reprodutível), assim como embarcaríamos dados, código e qualquer documentação necessária para tornar o pipeline reprodutível. Ou escrevemos o Dockerfile de tal forma que ele sempre produz a mesma imagem, independentemente de quando ele é construído. Aconselha-se vivamente optarmos pela segunda opção, mas também arquivarmos a imagem. Mas, claro, isso também depende de quão crítico é o nosso projeto. Talvez não precisemos de começar a arquivar imagens, ou talvez nem precisemos de nos certificar de que o Dockerfile produz sempre a mesma imagem. Ainda assim é recomendável que escrevamos os nossos Dockerfiles de tal forma que eles sempre produzam a mesma imagem. É mais seguro, e não significa trabalho extra, graças ao projeto Rocker.\nVamos então de novo ao Rocker project e especificamente às suas imagens versionadas que podemos encontrar aqui9. Quando usamos imagens versionadas como base dos nossos projectos estamos a garantir que:\n\numa versão fixa do R que é construída de raíz. Não interessa quando construímos a imagem, sempre embarcará com a mesma versão do R;\nO sistema operativo será o lançamento LTS correspondente à versão do R;\nos repositórios R serão definidos Posit Public Package Manager (PPPM) a uma determinada data. Isto garante que os pacotes R não precisam de ser compilados uma vez que o PPPM entrega pacotes binários para a arquitectura amd64 (a arquitectuta que todos os computadores que não são Apple geralmente usam).\n\nEste último ponto requer mais algumas explicações. Devemos ter em conta que as imagens versionadas do Rocker utilizam o PPPM definido numa determinada data. Esta é uma excelente caraterística do PPPM. Por exemplo, a imagem Rocker versionada que vem com o R 4.2.2 tem os repositórios definidos para 14 de março de 2023, como podemos aqui10. Isto significa que se usarmos install.packages(“dplyr”) dentro de um container executado a partir daquela imagem, então a versão do {dplyr} instalada será a que estava disponível no dia 14 de março.\nIsto pode ser conveniente em certas situações, e podemos querer, dependendo das nossas necessidades, utilizar o PPPM numa data específica para definir imagens Docker, como faz o projeto Rocker. Podemos inclusivé definir o PPPM numa data específica para a nossa máquina de desenvolvimento principal (basta seguir as instruções aqui11). Mas temos de ter presente que não vamos receber nenhuma atualização de pacotes, portanto, se quisermos instalar uma nova versão de um pacote que possa introduzir alguns novos recursos interessantes, precisamos alterar os repositórios novamente. É por isso que é aconselhável manter os repositórios por defeito (ou usar o r2u se estiver no Ubuntu) e gerir as bibliotecas de pacotes dos nossos projectos com o {renv}. Desta forma, não tem de mexer em nada e tem a flexibilidade de ter uma biblioteca de pacotes separada por projeto. O outro benefício adicional é que podemos então usar o ficheiro renv.lock do projeto para instalarmos exatamente a mesma biblioteca de pacotes dentro da imagem do Docker.\nComo uma rápida introdução ao uso de imagens Rocker, vamos usar o ficheiro renv.lock do nosso pipeline, que pode ser baixado de aqui12. Este é o último ficheiro renv.lock que geramos do nosso pipeline, contém todos os pacotes necessários para executarmos o pipeline, incluindo as versões corretas dos pacote {targets} e do pacote {housing} que desenvolvemos. Uma observação importante: não importa se o ficheiro renv.lock contém pacotes que foram lançados após o dia 14 de março. Mesmo que os repositórios dentro da imagem do Rocker que vamos usar estejam definidos para essa data, o ficheiro lock também especifica o URL do repositório correto para descarregar os pacotes. Portanto, esse URL será usado em vez do definido para a imagem do Rocker.\nOutro aspeto útil do ficheiro renv.lock é que também registra a versão do R que foi usada para desenvolver originalmente o pipeline, neste caso, R versão 4.2.2. Portanto, essa é a versão que usaremos em nosso Dockerfile. Precisamos verificar também a versão do {renv} que usamos para construir o ficheiro renv.lock. Não temos necessariamente de instalar a mesma versão, mas é recomendável que o façamos. Por exemplo, agora está disponível o {renv} versão 0.17.1, mas o ficheiro renv.lock foi escrito com {renv} versão 0.16.0. Assim, para evitarmos um eventual problema de compatibilidade, vamos instalar exatamente a mesma versão. Felizmente, isso é muito fácil de fazer (para verificar a versão de {renv} que foi usada para escrever o ficheiro de lock, basta procurar a palavra “renv” no ficheiro lock).\nEnquanto o {renv} se encarrega de instalar os pacotes R corretos, não se encarrega de instalar as dependências corretas ao nível do sistema. É por isso que temos de ser nós a instalar estas dependências ao nível do sistema. Vejamos uma lista de dependências ao nível do sistema que podemos instalar para evitar quaisquer problemas abaixo, vamos também ver como conseguimos chegar a essa lista. É bastante fácil graças ao Posit e ao seu PPPM. Por exemplo, aqui13 está a página de resumo do pacote {tidyverse}. Se selecionar “Ubuntu 22.04 (Jammy)” no canto superior direito, e depois descermos, veremos uma lista de dependências que podemos simplesmente copiar e colar no nosso Dockerfile:\n\n\n\nDependência a nivel do sistema para o pacote {tidyverse} no Ubuntu.\n\n\nUsaremos esta lista para instalarmos as dependências requeridas pelo nosso pipeline.\nCriamos uma nova pasta, não importa o nome, e guardamos o ficheiro renv.lock do link em cima, dentro dessa pasta. Depois criamos um ficheiro de texto vazio que chamamos Dockerfile. Adicionamos as seguintes linhas:\nFROM rocker/r-ver:4.2.2\n\nRUN apt-get update && apt-get install -y \\\n    libglpk-dev \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libgit2-dev \\\n    default-libmysqlclient-dev \\\n    libpq-dev \\\n    libsasl2-dev \\\n    libsqlite3-dev \\\n    libssh2-1-dev \\\n    libxtst6 \\\n    libcurl4-openssl-dev \\\n    libharfbuzz-dev \\\n    libfribidi-dev \\\n    libfreetype6-dev \\\n    libpng-dev \\\n    libtiff5-dev \\\n    libjpeg-dev \\\n    libxt-dev \\\n    unixodbc-dev \\\n    wget \\\n    pandoc\n\nRUN R -e \"install.packages('remotes')\"\n\nRUN R -e \"remotes::install_github('rstudio/renv@0.16.0')\"\n\nRUN mkdir /home/housing\n\nCOPY renv.lock /home/housing/renv.lock\n\nRUN R -e \"setwd('/home/housing');renv::init();renv::restore()\"\nA primeira linha indica que vamos basear a nossa imagem na imagem do projeto Rocker que vem com a versão 4.2.2 do R, que é a versão correta de que precisamos. Depois, instalamos as dependências necessárias ao nível do sistema com o gestor de pacotes do Ubuntu, como explicado anteriormente. Depois vem o pacote {remotes}. Este permite-nos baixar uma versão específica do {renv} do Github, que é o que fazemos na próxima linha. Convém sublinhar que fazemos isto simplesmente porque o ficheiro renv.lock original foi gerado usando a versão 0.16.0 do {renv} e, portanto, para evitar quaisquer possíveis problemas de compatibilidade, também usamos este para restaurar os pacotes necessários para o pipeline. Mas é muito provável que pudéssemos instalar a versão atual do {renv} para restaurar os pacotes, e que isso teria funcionado sem problemas. De notar que para versões posteriores do {renv}, pode ser necessário inserir um ‘v’ antes do número da versão: renv@v1.0.2, por exemplo. Mas só para estarmos salvaguardados, instalamos a versão correta de {renv}. A propósito, todos estes passos estão explicados nesta vinheta14 (mantivemos apenas as linhas de código absolutamente essenciais para que funcionasse). A seguir vem a linha RUN mkdir /home/housing, que cria uma pasta (mkdir significa make diretory), dentro da imagem Docker, em /home/housing. Nas distribuições Linux, /home/ é o diretório que os usuários usam para armazenar os seus arquivos, criamos então a pasta /home/ e, dentro dela, criamos uma nova pasta, housing, que conterá os arquivos do nosso projeto. Não importa se mantém esta estrutura ou não, podemos ignorar a pasta /home/. O que importa é que coloquemos os ficheiros onde os possamos encontrar.\nDe seguida, vem COPY renv.lock /home/housing/renv.lock. Com isto copiamos o ficheiro renv.lock do nosso computador (recordemos que devemos salvar esse ficheiro ao lado do Dockerfile) para /home/housing/renv.lock. Assim, incluímos o ficheiro renv.lock dentro da imagem do Docker, o que será crucial para a próxima e última etapa: RUN R -e “setwd(‘/home/housing’);renv::init();renv::restore()”.\nIsto executa o programa R a partir da linha de comandos do Linux com a opção -e. Esta opção permite passar uma expressão R para a linha de comandos, que tem de ser escrita entre “”. A utilização do R -e tornar-se-á rapidamente um hábito, porque é assim que podemos executar o R de forma não interactiva, a partir da linha de comandos. A expressão que passamos define a pasta de trabalho para /home/housing, e então usamos renv::init() e renv::restore() para restaurarmos os pacotes do ficheiro renv.lock que copiamos antes. Usando esse Dockerfile, podemos agora construir uma imagem que virá com a versão 4.2.2 do R pré-instalada, bem como todos os exactos pacotes que usamos para desenvolver o pipeline do housing.\nConstruímos a imagem com docker build -t housing_image . (sem esquecer o . no final).\nO processo de construção vai demorar algum tempo, pelo que podemos ir buscar uma bebida quente entretanto. Agora, já fizemos metade do trabalho: temos um ambiente que contém o software necessário para nosso pipeline, mas os ficheiros do pipeline em si ainda estão em falta. Mas antes de adicionarmos o próprio pipeline, vamos ver se a imagem do Docker que construímos funciona. Para isso, fazemos login numa linha de comando dentro de um container Docker em execução iniciado a partir da nossa imagem com este único comando:\nowner@localhost ➤ docker run --rm -it --name housing_container housing_image bash\nCom isto iniciamos o bash (a linha de comandos do Ubuntu) dentro do housing_container iniciado a partir da imagem housing_image. Ao adicionarmos o marcador --rm ao docker run, o Docker container é parado quando fizermos o log out (se não o fizermos o Docker container continua a correr em bakground). Uma vez autenticados, podemos mover-nos para a pasta do projecto com:\nuser@docker ➤ cd home/housing\ne então iniciarmos o interpretador R:\nuser@docker ➤ R\nse tudo correr bem, deveremos ver uma prompt R familiar, com a message do {renv} no fim:\nR version 4.2.2 (2022-10-31) -- \"Innocent and Trusting\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n* Project '/home/housing' loaded. [renv 0.16.0]\nTentemos carregar o pacote {housing} com library(\"housing\"), para garantirmos que tudo funciona.\n\n\n\n\nBhandari Neupane, Jayanti, Ram P. Neupane, Yuheng Luo, Wesley Y. Yoshida, Rui Sun, and Philip G. Williams. 2019. “Characterization of Leptazolines a–d, Polar Oxazolines from the Cyanobacterium Leptolyngbya Sp., Reveals a Glitch with the ‘Willoughby–Hoye’ Scripts for Calculating NMR Chemical Shifts.” Organic Letters 21 (20): 8449–53.",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pipelines de análise reprodutíveis com o Docker</span>"
    ]
  },
  {
    "objectID": "repro_cont.html#footnotes",
    "href": "repro_cont.html#footnotes",
    "title": "14  Pipelines de análise reprodutíveis com o Docker",
    "section": "",
    "text": "https://packages.ubuntu.com/jammy/↩︎\nhttps://github.com/eddelbuettel/r2u↩︎\nhttps://docs.docker.com/desktop/↩︎\nhttps://docs.docker.com/engine/install/ubuntu/#install-using-the-repository↩︎\nhttps://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user↩︎\nhttps://rocker-project.org/↩︎\nhttps://hub.docker.com/↩︎\nhttps://rocker-project.org/images/↩︎\nhttps://rocker-project.org/images/versioned/r-ver.html↩︎\nhttps://is.gd/fdrq4p↩︎\nhttps://is.gd/jbdTKC↩︎\nhttps://is.gd/5UcuxW↩︎\nhttps://is.gd/ZaXHwa↩︎\nhttps://rstudio.github.io/renv/articles/docker.html↩︎",
    "crumbs": [
      "Parte 2: Write IT Down",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pipelines de análise reprodutíveis com o Docker</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bhandari Neupane, Jayanti, Ram P. Neupane, Yuheng Luo, Wesley Y.\nYoshida, Rui Sun, and Philip G. Williams. 2019. “Characterization\nof Leptazolines a–d, Polar Oxazolines from the Cyanobacterium\nLeptolyngbya Sp., Reveals a Glitch with the\n‘Willoughby–Hoye’ Scripts for Calculating NMR Chemical\nShifts.” Organic Letters 21 (20): 8449–53.\n\n\nChan, Chung-hong, and David Schoch. 2023. “RANG: Reconstructing\nReproducible r Computational Environments.” arXiv.\n\n\nLandau, William Michael. 2021. “The Targets r Package: A Dynamic\nMake-Like Function-Oriented Pipeline Toolkit for Reproducibility and\nHigh-Performance Computing.” Journal of Open Source\nSoftware 6 (57): 2959.\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "References"
    ]
  }
]